\chapter{Detailed Scope Analysis}\label{app-scope}

As outlined in \Cref{scope}, the scope of our research is limited to testing
applied to code itself. Throughout our research, we identify many
approaches that are out of scope based on this criteria.

\section{Hardware Testing}
\label{hard-test}
While testing the software run \emph{on} or in control \emph{of} hardware is in
scope, testing performed on the hardware \emph{itself} is out of scope. The
following are some examples of hardware testing approaches we exclude from our
research:

\begin{itemize}
    \item Ergonomics testing and proximity-based testing \citepISTQB{}
          are out of scope, since they are used for testing hardware.
    \item \acf{emsec} testing \ifnotpaper
              (\citealp{ISO2021}; \citealp[p.~95]{ZhouEtAl2012})\else
              \cite{ISO2021}, \cite[p.~95]{ZhouEtAl2012}\fi, which deals with
          the ``security risk'' of ``information leakage via electromagnetic
          emanation'' \citep[p.~95]{ZhouEtAl2012}, is also out of scope.
          \ifnotpaper
    \item All the examples of domain-specific testing given by
          \citet[p.~26]{Firesmith2015} are focused on hardware, so these
          examples are out of scope. However, this might not be representative
          of \emph{all} kinds of domain-specific testing (e.g., \acf{ml} model
          testing seems domain-specific), so some subset of this approach may
          be in scope.
    \item Similarly, the examples of environmental tolerance testing given by
          \citet[p.~56]{Firesmith2015} do not seem to apply to software.
          For example, radiation tolerance testing seems to focus on hardware,
          such as motors \citep{MukhinEtAl2022}, robots \citep{ZhangEtAl2020},
          or ``nanolayered carbide and nitride materials''
          \citep[p.~1]{TunesEtAl2022}. Acceleration tolerance testing seems to
          focus on \accelTolTest{} and acoustic tolerance testing on rats
          \citep{HolleyEtAl1996}, which are even less related! Since these all
          focus on environment-specific factors that would not impact the code,
          these examples are out of scope. As with domain-specific testing, a
          subset of environmental tolerance testing may be in scope, but since
          no candidates have been found, this approach is out of scope for now.
    \item \citet{SPICE2022} uses the terms ``software
          qualification testing'' and ``system qualification testing'' in the
          context of the automotive industry. While these may be in scope, the
          more general idea of ``qualification testing'' seems to refer to the
          process of making a hardware component, such as an electronic
          component \citep{AhsanEtAl2020}, gas generator \citep{ParateEtAl2021}
          or photovoltaic device, ``into a reliable and marketable product''
          \citep[p.~1]{SuhirEtAl2013}. Therefore, it is currently unclear if
          this is in scope\todo{Investigate further}.
          \fi
    \item \acf{orthat} can be used when testing software \citep{Mandl1985} (in
          scope) but can also be used when testing hardware
          \citep[pp.~471--472]{Valcheva2013}, such as ``processors \dots{} made
          from pre-built and pre-tested hardware components'' \citetext{p.~471}
          (out of scope). A subset of \acs{orthat} called ``\acf{toat}'' is
          used for ``experimental design problems in manufacturing''
          \citep[p.~1573]{YuEtAl2011} or ``product and manufacturing process
          design'' \citep[p.~44]{Tsui2007} and is thus also out of scope.
          \ifnotpaper
    \item Since control systems often have a software \emph{and} hardware
          component \citep{ISO2015,PreußeEtAl2012,ForsythEtAl2004},
          only the software component is in scope. In some cases, it is
          unclear whether the ``loops''\footnote{Humorously, the testing of
              loops in chemical systems \citep{Dominguez-PumarEtAl2020} and
              copper loops \citep{Goralski1999} are out of scope.} being
          tested are implemented by software or hardware, such as those in
          wide-area damping controllers \citep{PierreEtAl2017,TrudnowskiEtAl2017}.
          \begin{itemize}
              \item A related note: ``path coverage'' or ``path testing''
                    seems to be able to refer to either paths through code
                    (as a subset of control-flow testing)
                    \citep[p.~5-13]{SWEBOK2024} or through a model, such as
                    a finite-state machine (as a subset of model-based
                    testing) \citep[p.~184]{DoğanEtAl2014}.
          \end{itemize}
          \fi
\end{itemize}

\section[V\&V of Other Artifacts]{\acs{vnv} of Other Artifacts}
\label{other-vnv}
While many artifacts produced by the software life cycle can be tested, only
testing performed on the code \emph{itself} is in scope. Therefore, we exclude
the following test approaches either in full or in part:

\begin{itemize}
    \item Design reviews and documentation reviews are out of scope, as they
          focus on the \acs{vnv} of design \citep[pp.~132]{IEEE2017} and
          documentation \ifnotpaper \citetext{p.~144}\else
              \cite[pp.~144]{IEEE2017}\fi, respectively.
          \ifnotpaper
    \item Security audits can focus on ``an organization's \dots{} processes
          and infrastructure'' \citepISTQB{} (out of scope) or
          ``aim to ensure that all of the products installed on a site are
          secure when checked against the known vulnerabilities for those
          products'' \citep[p.~28]{Gerrard2000b} (in scope).
          \fi
    \item Error seeding is the ``process of intentionally adding
          known faults\footnote{
          \label{add-fault}While error seeding and fault injection testing
          both introduce faults as part of testing, they do so with different
          goals: to ``estimat[e] the number of faults remaining''
          \citep[p.~165]{IEEE2017} and ``test the robustness of the system''
          \citeyearpar[p.~42]{IEEE2022}, respectively. Therefore, these
          approaches are not considered synonyms, and the lack of this relation
          in the literature is not included in \Cref{syns} as a synonym
          flaw.} to those already in a computer program'',
          done to both ``monitor[] the rate of detection and removal'',
          which is a part of \acs{vnv} of the \acs{vnv} itself (out of scope),
          ``and estimat[e] the number of faults remaining''
          \citep[p.~165]{IEEE2017}, which helps verify the actual code (in scope).
    \item Fault injection testing, where ``faults are artificially
          introduced\textsuperscript{\ref{add-fault}} into the \acs{sut}
          [System Under Test]'', can be used to evaluate the effectiveness of a
          test suite \citep[p.~5-18]{SWEBOK2024}, which is a part of \acs{vnv}
          of the \acs{vnv} itself (out of scope), or ``to test
          the robustness of the system in the event of internal and
          external failures'' \citep[p.~42]{IEEE2022}, which helps verify
          the actual code (in scope).
    \item ``Mutation [t]esting was originally conceived as a
          technique to evaluate test suites in which a mutant is a slightly
          modified version of the \acs{sut}'' \citep[p.~5-15]{SWEBOK2024},
          which is in the realm of \acs{vnv} of the \acs{vnv} itself (out of
          scope). However, it ``can also be categorized as a structure-based
          technique'' and can be used to assist fuzz and metamorphic testing
          \citep[p.~5-15]{SWEBOK2024} (in scope).
\end{itemize}

\section{Static Testing}
\label{static-test}
Throughout the literature, static testing is more ambiguous than dynamic
testing, with more ad hoc processes and inconsistent in/exclusion from the
scope of software testing in general (see \flawref{static-test-flaw}).
Furthermore, it seems less relevant to our
original goal (the automatic generation of tests). In particular, many
techniques require human intervention, either by design (such as code
inspections) or to identify and resolve false positives (such as
intentional exceptions to linting rules). Nevertheless, understanding
the breadth of testing approaches requires a ``complete'' picture of how
software can be tested and how the various approaches relate to one another.
Parts of these static approaches may even be generated in the
future! For these reasons, we keep static testing in scope for this stage
of our work, even though static testing will likely be removed at a later
stage of analysis\thesisissueref{41,44} based on our original motivation.

% \ifnotpaper

% TODO: can this be formalized at all and included in a glossary or section?
% \citeauthor{Patton2006} introduces ``specification testing''
% \citeyearpar[pp.~56-62]{Patton2006}, which is static black-box testing.
% Most of this section is irrelevant to generating test cases, due to
% requiring human involvement and verifying the specification, not the code.
% However, it provides a ``Specification Terminology Checklist''
% \citep[p.~61]{Patton2006} that includes some keywords that, if found, could
% trigger an applicable warning to the user (similar to the idea behind the
% correctness/consistency checks project), indicating that a requirement is
% ambiguous or incomplete \citep[see][p.~1-8]{SWEBOK2024}:

% \begin{itemize}
%     \item \textbf{Potentially unrealistic:} always, every, all, none, every,
%           certainly, therefore, clearly, obviously, evidently
%     \item \textbf{Potentially vague:} some, sometimes, often, usually,
%           ordinarily, customarily, most, mostly, good, high-quality, fast,
%           quickly, cheap, inexpensive, efficient, small, stable
%     \item \textbf{Potentially incomplete:} etc., and so forth, and so on,
%           such as, handled, processed, rejected, skipped, eliminated,
%           if \dots{} then \dots{} (without ``else'' or ``otherwise''),
%           to be determined \citep[p.~408]{vanVliet2000}
% \end{itemize}

% \citeauthor{Patton2006} also provides a ``Generic Code Review Checklist''
% \citeyearpar[pp.~99-103]{Patton2006} in the context of static testing.
% Some of the following issues \emph{may} be able to be detected
% automatically (e.g., by linters), but this checklist is primarily used
% by human reviewers:

% \begin{itemize}
%     \item \phantomsection
%           \label{data-ref-errors}
%           Data reference errors: ``bugs caused by using a variable,
%           constant, \dots{} [etc.] that hasn't been properly declared
%           or initialized'' for its context \citetext{p.~99}
%     \item Data declaration errors: bugs ``caused by improperly
%           declaring or using variables or constants'' \citetext{p.~100}
%     \item Computation errors: ``essentially bad math''; e.g., type
%           mismatches, over/underflow, zero division, out of meaningful
%           range \citetext{p.~101}
%           \label{comp-errors}
%     \item Comparison errors: ``very susceptible to boundary condition
%           problems''; e.g., correct inclusion, floating point
%           comparisons \citetext{p.~101}
%     \item Control flow errors: bugs caused by ``loops and other control
%           constructs in the language not behaving as expected''
%           \citetext{p.~102}
%     \item Subroutine parameter errors: bugs ``due to incorrect passing
%           of data to and from software subroutines'' \citetext{p.~102}
%           (could also be called ``interface errors''
%           \citep[p.~416]{vanVliet2000})
%     \item Input/output errors: e.g., how are errors handled?
%           \citetext{pp.~102-103}
%     \item ASCII character handling, portability, compilation warnings
%           \citetext{p.~103}
% \end{itemize}
