\section{Preliminary Recommendations}\label{app-recs}

As shown in \Cref{flaws}, the software testing literature is quite flawed. Due
to the sheer number of flaws and the size of the domain itself, it will take a
lot of time, effort and expertise to organize these terms (and their relations)
logically. However, the hardest step is often the first one, so we give some
examples of how these flaws can be resolved. These changes arise when we notice
an issue with the current state of the terminology and think about what
\emph{we} would do to make it better. We do not claim that these are correct,
unbiased, or exclusive, just that they can be used as an inspiration for those
wanting to pick up where we leave off.

When redefining test approaches, we seek to make them:
\begin{enumerate}
    \item \textbf{Atomic:} Each term should only define one thing.
          %   (e.g., disaster/recovery testing seems to have two disjoint definitions)
    \item \textbf{Distinct:} Each term should be meaningfully separate from
          others and not overlap with them.
          %   (e.g., failover/recovery testing, failover and recovery
          %   testing, and failover testing are all given separately)
    \item \textbf{Consistent:} Each piece of data associated with a term should
          be cohesive and not contradict others.
          %   (e.g., backup/recovery testing and failover/recovery
          %   testing explicitly exclude an aspect included in its parent
          %   disaster/recovery testing)
    \item \textbf{Intuitive:} Each term's definition and relations should
          follow logically from, or at least be consistent with, the term's
          name.
          %   (e.g., backup and recovery testing's definition
          %   implies the idea of performance, but its name does not)
\end{enumerate}
Likewise, we seek to eliminate classes of flaws that can be detected
automatically, such as test approaches that are given as \ifnotpaper synonyms to
    multiple distinct approaches (\Cref{multiSyns}) or as \fi parents of
themselves (\Cref{selfPars})\ifnotpaper,\fi\ or pairs of approaches
with both a parent-child \emph{and} synonym relation (\Cref{parSyns,parSyns-full}).

We give recommendations for each ``subset'' of testing that we describe in
\Cref{flawSubsets}---\ifnotpaper operational (acceptance)
    testing (\Cref{oat-test-rec}), \fi recovery testing (\Cref{rec-test-rec}),
scalability testing (\Cref{scal-test-rec}), and compatibility testing
(\Cref{compat-test-rec})---as well as the ``superset'' of
performance\-/related testing (\Cref{perf-test-rec}). We provide graphical
representations (see \Cref{\ifnotpaper app-rel-vis\else tools\fi})
of these subsets when helpful in \recFigs{}, in which arrows representing
relations between approaches are coloured based on the source tier (see
\Cref{source-tiers}) that defines them. We colour all proposed approaches and
relations orange. \ifnotpaper We also include inferred approaches and relations
    in grey for completeness, although they are not explicitly given in the
    literature (see \Cref{infers}).

    \subsection{Operational (Acceptance) Testing}\label{oat-test-rec}
    Since this terminology is not standardized (see \Cref{oat-flaw}), we
    propose that ``\acf{operat}'' and ``\acf{ot}'' are treated as
    synonyms for a type of acceptance testing (\citealp[p.~22]{IEEE2022};
    \citealpISTQB{}) that focuses on ``non-functional'' attributes of the system
    \citep{LambdaTest2024}\todo{find more academic sources}. Indeed, this is how we
    track this approach in \ourApproachGlossary{}! We define it as ``test[ing] to
    determine the correct installation, configuration and operation of a module and
    that it operates securely in the operational environment'' \citep{ISO_IEC2018}
    or to ``evaluate a system or component in its operational environment''
    \citep[p.~303]{IEEE2017}, particularly ``to determine if operations and/or
    systems administration staff can accept [it]'' \citepISTQB{}.
\fi

\subsection{Recovery Testing}\label{rec-test-rec}
% The following terms should be used in place of the current terminology to
% more clearly distinguish between different recovery-related test approaches.
% The result of the proposed terminology, along with their relations, is
% demonstrated in \Cref{fig:rec-graph-proposed}.

To remedy the flaws we describe in \Cref{recov-flaw}, we recommend that the
literature uses these terms more consistently, resulting in the improved graph
in \Cref{fig:rec-graph-proposed}. The following proposals ``recapture''
information from the literature more consistently:

\begin{enumerate}
    \item Prefer the term ``recoverability testing'' over ``recovery testing''
          to indicate its focus on a system's \emph{ability} to recover, not its
          \emph{performance} of recovering \citep[p.~47]{Kam2008}. ``Recovery
          testing'' may be an acceptable synonym, since it seems to be more
          prevelant in the literature.
    \item Introduce the term \emph{recovery performance testing} when evaluating
          performance metrics of a system's recovery as a subapproach of
          recoverability testing and performance-related testing\footnote{
              See \Cref{perf-test-rec}.}
          (\citealp[Fig.~2]{IEEE2022}; \citeyear[p.~2]{IEEE2013}).
    \item Introduce separate terms for the different methods of recovery which
          are all subapproaches of recoverability testing:
          \begin{enumerate}
              \item from backup memory (\emph{backup recovery testing})
                    (\citealp[p.~37]{IEEE2021c}; \citeyear[p.~2]{IEEE2013}),
              \item from a back-up system (\emph{failover testing})
                    (\citealp[p.~5\=/9]{SWEBOK2024}; \citealpISTQB{}), or
              \item by transferring operations elsewhere (\emph{transfer
                        recovery testing}) \citep[p.~37]{IEEE2021c}.
          \end{enumerate}
\end{enumerate}

\recoveryGraphs{}

% \begin{itemize}
%     \item \textbf{Recoverability Testing:} ``Testing \dots\ aimed at
%           verifying software restart capabilities after a system crash or
%           other disaster'' \citep[p.~5-9]{SWEBOK2024} including ``recover[ing]
%           the data directly affected and re-establish[ing] the desired state
%           of the system''
%           \ifnotpaper
%               (\citealp{ISO_IEC2023a}; similar in \citealp[p.~7-10]{SWEBOK2024})
%           \else
%               \cite{ISO_IEC2023a} (similar in \cite[p.~7-10]{SWEBOK2024})
%           \fi so that the system ``can perform
%           required functions'' \citep[p.~370]{IEEE2017}. ``Recovery testing''
%           will be a synonym, as in \citep[p.~47]{Kam2008}, since it is the
%           more prevalent term throughout various sources, although
%           ``recoverability testing'' is preferred to indicate that this
%           explicitly focuses on the \emph{ability} to
%           recover, not the \emph{performance} of recovering.
%     \item \textbf{Failover Testing:} Testing that ``validates the SUT's
%           ability to manage heavy loads or unexpected failure to continue
%           typical operations'' \cite[p.~5-9]{SWEBOK2024} by entering a
%           ``backup operational mode in which [these responsibilities] \dots\
%           are assumed by a secondary system'' \citepISTQB{}. This will
%           replace ``failover/recovery testing'', since it is more clear, and
%           since this is one way that a system can recover from failure, it
%           will be a subset of ``recovery testing''.
%     \item \textbf{Transfer Recovery Testing:} Testing to evaluate if,
%           in the case of a failure, ``operation of the test item can be
%           transferred to a different operating site and \dots\ be transferred
%           back again once the failure has been resolved''
%           \citeyearpar[p.~37]{IEEE2021c}. This replaces the second definition
%           of ``disaster/recovery testing'', since the first is just a
%           description of ``recovery testing'', and could potentially be
%           considered as a kind of failover testing. This may not be
%           intrinsic to the hardware/software (e.g., may be the responsibility
%           of humans/processes).
%     \item \textbf{Backup Recovery Testing:} Testing that determines the
%           ability ``to restor[e] from back-up memory in the event of failure''
%           \citep[p.~37]{IEEE2021c}. The qualification that this occurs
%           ``without transfer[ing] to a different operating site or back-up
%           system'' \citetext{p.~37} \emph{could} be made explicit, but this is
%           implied since it is separate from transfer recovery testing and
%           failover testing, respectively.
%     \item \textbf{Recovery Performance Testing:} Testing ``how well a system or
%           software can recover \dots\ [from] an interruption or failure''
%           \ifnotpaper
%               (\citealp[p.~7-10]{SWEBOK2024}; similar in \citealp{ISO_IEC2023a})
%           \else
%               \cite[p.~7-10]{SWEBOK2024} (similar in \cite{ISO_IEC2023a})
%           \fi ``within specified parameters of time, cost, completeness, and
%           accuracy'' \citep[p.~2]{IEEE2013}. The distinction between the
%           performance-related elements of recovery testing seemed to be
%           meaningful\thesisissuetodo{40}, but was not captured consistently
%           by the literature. This will be a subset of ``performance-related
%           testing'' \ifnotpaper (see \Cref{perf-test-rec}) \fi
%           as ``recovery testing'' is in \citep[p.~22]{IEEE2022}. This could
%           also be extended into testing the performance of specific elements
%           of recovery (e.g., failover performance testing), but this be too
%           fine-grained and may better be captured as an
%           \hyperref[orth-test]{orthogonally derived test approach}.
% \end{itemize}

\subsection{Scalability Testing}\label{scal-test-rec}

The issues with scalability testing terminology we describe in \Cref{scal-flaw}
% and show the relations between related approaches (as described by the
% literature) in \Cref{fig:scal-graph-current}. These flaws
are resolved and/or explained by other sources! Taking this extra information
into account % results in \Cref{fig:scal-graph-proposed} and
provides a more accurate description of scalability testing.

\paragraph{\texttt{(CONTRA, SYNS)}}
\citeauthor{IEEE2021c} \citeyearpar[p.~39]{IEEE2021c} define ``scalability
testing'' as the testing of a system's ability to ``perform under conditions
that may need to be supported in the future''.
%, which ``may include assessing what level of additional resources (e.g.
% memory, disk capacity, network bandwidth) will be required to support
% anticipated future loads''.
This focus on ``the future''
is supported by \citetISTQB{}, \ifnotpaper who define \else which defines \fi
``scalability'' as ``the degree to which a component or system can be adjusted
for changing capacity''\ifnotpaper; the original source they reference agrees,
defining it as ``the measure of a system's ability to be upgraded to
accommodate increased loads'' \citep[p.~381]{GerrardAndThompson2002}\fi. In
contrast, capacity testing focuses on the system's present state, evaluating
the ``capability of a product to meet requirements for the maximum limits of a
product parameter''
%, such as the number of concurrent users, transaction
% throughput, or database size 
\citep{ISO_IEC2023a}. Therefore, these terms should \emph{not} be synonyms,
as done by \ifnotpaper
    \citet[p.~53]{Firesmith2015} and \citet[pp.~22\==23]{Bas2024}\else
    \cite[p.~53]{Firesmith2015} and \cite[pp.~22\==23]{Bas2024}\fi.

\paragraph{\texttt{(CONTRA, DEFS)}}

The underlying reason that sources disagree on whether external modification of
the system is part of scalability testing is its confusion with elasticity
testing. \citeauthor{BertolinoEtAl2019}\ say the two approaches are ``closely
related'' \citeyearpar[p.~93:28]{BertolinoEtAl2019}, even claiming that one
objective of elasticity testing is ``to evaluate scalability''
\citetext{p.~93:14}! However, \citet{SWEBOK2025} (\ifnotpaper who \else which \fi
cites \citet{BertolinoEtAl2019}) distinguishes between these approaches:
\begin{itemize}
    \item \textbf{Scalability Testing:} testing that evaluates ``the software's
          ability to scale up non-functional requirements such as load, number
          of transactions, and volume of data'' \citep[p.~5\=/9; similar on
              p.~5\=/5]{SWEBOK2025}.
          %       Based on this definition, scalability
          %   testing is a subtype of load testing, volume testing, and potentially
          %   transaction flow testing.
    \item \textbf{Elasticity Testing:} testing that evaluates the ability of a
          system to ``dynamically scal[e] up and down \dots{} resources as
          needed'' \citep[p.~93:18; similar on p.~93:13]{BertolinoEtAl2019}
          ``without compromising the capacity to meet peak utilization''
          \citep[p.~5\=/9]{SWEBOK2025}.
          %   Based on this definition, elasticity
          %   testing is then a subtype of memory management testing (with both
          %   being a subtype of resource utilization testing) and stress testing.
\end{itemize}
This distinction is consistent with how the terms are used in industry:
\citet{Pandey2023} says that scalability is the ability to % \thesisissuetodo{35}
``increase \dots\ performance or efficiency as demand increases over time'',
while elasticity allows a system to ``tackle changes in the workload [that]
occur for a short period''. Therefore, external modification of a system is
part of scalability testing but \emph{not} elasticity testing. This also
implies that \citepos{ISO_IEC2023a} notion of ``scalability'' actually refers
to ``elasticity''.

% \scalGraphs{}

\subsection{Performance(-related) Testing}\label{perf-test-rec}

``Performance testing'' is defined as testing ``conducted to evaluate the
degree to which a test item \dots{} accomplishes its designated functions''
\ifnotpaper
    (\citealp[p.~7]{IEEE2022}; \citeyear[p.~2]{IEEE2021a};
    \citeyear[p.~320]{IEEE2017}; similar in
    \citeyear[pp.~38\=/39]{IEEE2021c}; \citealp[p.~1187]{Moghadam2019})%
\else
    \cite[p.~320]{IEEE2017}, \cite[p.~7]{IEEE2022}, \cite[p.~2]{IEEE2021a}
    (similar in \cite[pp.~38\=/39]{IEEE2021c}, \cite[p.~1187]{Moghadam2019})%
\fi. It does this
by ``measuring the performance metrics''
\ifnotpaper
    (\citealp[p.~1187]{Moghadam2019}; similar in \citealpISTQB{})
\else
    \cite[p.~1187]{Moghadam2019} (similar in \cite{ISTQB})
\fi (such as the ``system's capacity for growth''
\citep[p.~23]{Gerrard2000b}), ``detecting the functional problems appearing
under certain execution conditions'' \citep[p.~1187]{Moghadam2019}, and
``detecting violations of non-functional requirements under expected and
stress conditions'' \ifnotpaper
    (\citealp[p.~1187]{Moghadam2019}; similar in \citealp[p.~5-9]{SWEBOK2024})%
\else
    \cite[p.~1187]{Moghadam2019} (similar in \cite[p.~5-9]{SWEBOK2024})%
\fi. It is performed either \dots\
\begin{enumerate}
    \item ``within given constraints of time and other resources''
          \ifnotpaper
              (\citealp[p.~7]{IEEE2022}; \citeyear[p.~320]{IEEE2017};
              similar in \citealp[p.~1187]{Moghadam2019})%
          \else
              \cite[p.~320]{IEEE2017}, \cite[p.~7]{IEEE2022} (similar
              in \cite[p.~1187]{Moghadam2019})%
          \fi, or
    \item ``under a `typical' load'' \citep[p.~39]{IEEE2021c}.
\end{enumerate}

It is listed as a subset of performance-related testing, which is defined as
testing ``to determine whether a test item performs as required when it is
placed under various types and sizes of `load'\,'' \citeyearpar[p.~38]{IEEE2021c},
along with other approaches like load and capacity testing
\citep[p.~22]{IEEE2022}. Note that ``performance, load and stress testing might
considerably overlap in many areas'' \citep[p.~1187]{Moghadam2019}.
In contrast, \citet[p.~5\=/9]{SWEBOK2024}
gives ``capacity and response time'' as examples of ``performance
characteristics'' that performance testing would seek to ``assess'', which
seems to imply that these are subapproaches to performance testing instead.
This is consistent with how some sources treat ``performance testing'' and
``performance-related testing'' as synonyms \ifnotpaper
    (\citealp[p.~5\=/9]{SWEBOK2024}; \citealp[p.~1187]{Moghadam2019})%
\else \cite[p.~5\=/9]{SWEBOK2024}, \cite[p.~1187]{Moghadam2019}%
\fi, as noted in \Cref{syns}. This makes sense because of how general the
concept of ``performance'' is; most definitions of ``performance testing'' seem
to treat it as a category of tests.

However, it seems more consistent to infer
that the definition of ``performance-related testing'' is the more general one
often assigned to ``performance testing'' performed ``within given constraints
of time and other resources'' \ifnotpaper (\citealp[p.~7]{IEEE2022};
    \citeyear[p.~2]{IEEE2021a}; \citeyear[p.~320]{IEEE2017}; similar in
    \citealp[p.~1187]{Moghadam2019})%
\else \cite[p.~320]{IEEE2017}, \cite[p.~7]{IEEE2022}, \cite[p.~2]{IEEE2021a}
    (similar in \cite[p.~1187]{Moghadam2019})\fi, and
``performance testing'' is a subapproach of this performed ``under a `typical'
load'' \citep[p.~39]{IEEE2021c}. This has other implications for relations
between these types of testing; for example, ``load testing'' usually occurs
``between anticipated conditions of low, typical, and peak usage''
\ifnotpaper (\citealp[p.~5]{IEEE2022}; \citeyear[p.~39]{IEEE2021c};
    \citeyear[p.~253]{IEEE2017}\todo{OG IEEE 2013}; \citealpISTQB{})%
\else \cite[p.~253]{IEEE2017}, \cite{ISTQB}, \cite[p.~5]{IEEE2022},
    \cite[p.~39]{IEEE2021c}\fi, so it is a child of ``performance-related
testing'' and a parent of ``performance testing''.

After these changes, some finishing touches remain. The reflexive parent
relations are incorrect (as described in \Cref{selfPars}) and can be removed.
Similarly, since ``soak testing'' is given as a synonym to both ``endurance
testing'' \emph{and} ``reliability testing'' (see \Cref{multiSyns}), it makes
sense to just use these terms instead of one that is potentially ambiguous.
These changes (along with those from \Cref{rec-test-rec,scal-test-rec}) result
in the proposed relations shown in \Cref{fig:perf-graph}.

\performanceGraph{}

\subsection{Compatibility Testing}\label{compat-test-rec}
``Co-existence'' and ``interoperability'' are often defined separately
\ifnotpaper (\citealp[pp.~73, 237]{IEEE2017}; \citealpISTQB{})%
\else \cite[pp.~73, 237]{IEEE2017}, \cite{ISTQB}\fi, sometimes explicitly as a
decomposition of ``compatibility'' \citep{ISO_IEC2023a}! Following this
precedent, ``co-existence testing'' and ``interoperability testing'' should be
defined as their own test approaches to make their definitions atomic;
\citet{IEEE2017} \multiAuthHelper{define} ``interoperability testing''
\citetext{p.~238} but not ``co-existence testing''. The term ``compatibility
testing'' may still be a useful test approach to define, but it should be
defined independently of its children: ``co-existence testing'' and
``interoperability testing''.
