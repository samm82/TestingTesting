\section{Terminology}
\label{terminology}

Our research aims to describe the current state of software testing literature,
including its flaws. Since we critique the lack of clarity, consistency, and
robustness in the literature, we do our best to hold ourselves to a higher
standard by defining and using terms consistently. For example, before we can
constructively describe the flaws in the literature, we need to define what
we mean by ``flaw'' and its related terms (\Cref{flaw-def}). We also track if
information found in our sources is more implicit (and therefore requires some
degree of judgement\thesisissueref{176}) by defining the notion of
``explicitness'' (\Cref{explicitness}) and similarly track the ``credibility''
of the sources themselves (\Cref{cred}). We make these distinctions to reduce
how much our preconceptions affect our analysis (or at least make it more
obvious to future researchers). To further prevent bias, we do not invent
or add our own classifications or kinds of relations. Instead, the notions of
test approach categories (\Cref{cats-def}), synonyms (\Cref{syn-rels}), and
parent-child relations (\Cref{par-chd-rels}) we present here
follow logically from the literature. We define them here for clarity
since we use them throughout this \docType{}, even though they are
``results'' of our research.

\subsection{Test Approaches}\label{approach-def}

Software testing is ``the process of executing a program with the intent of
finding errors'' \citep[p.~438]{PetersAndPedrycz2000}\todo{OG Myers 1976} where
the main steps are to:
\begin{enumerate}
    \item Identify the goal(s) of the test
    \item Decide on an approach
    \item Develop the tests
    \item Determine the expected results\todo{They introduce the idea of an
              oracle here, but we may not want to for simplicity.}
    \item Run the tests
    \item Compare the expected results to the actual results
          \ifnotpaper \citep[p.~443; similar in][p.~11]{Firesmith2015}
          \else \citetext{p.~443} (similar in \cite[p.~11]{Firesmith2015}) \fi
\end{enumerate}
The end goal is to evaluate ``some aspect of the system or component'' based on
these results \ifnotpaper (\citealp[p.~10]{IEEE2022}; \citeyear[p.~6]{IEEE2021};
    \citeyear[p.~465]{IEEE2017}\todo{OG ISO/IEC 2014})\else
    \cite[p.~465]{IEEE2017}, \cite[p.~10]{IEEE2022}, \cite[p.~6]{IEEE2021}\fi.
When this evaluation reveals errors, ``the faults causing them are what can and
must be removed'' \citep[p.~5\=/3]{SWEBOK2024}.

Of course, the chosen approaches influence what kinds of test cases are
developed and how, so it is important that they are defined correctly,
consistently, and unambiguously. We therefore focus on tracking flaws with how
the literature describes ``test approaches'': ``high-level test implementation
choice[s]'' \citep[p.~10]{IEEE2022} used to ``pick the particular test case
values'' \citeyearpar[p.~465]{IEEE2017}. Since the only approach that can
``fully'' test a system (exhaustive testing) is infeasible in most non-trivial
situations \ifnotpaper (\citeyear[p.~4]{IEEE2022}; \citealp[p.~5\=/5]{SWEBOK2024};
    \citealp[pp.~439, 461]{PetersAndPedrycz2000}; \citealp[p.~421]{vanVliet2000})\else
    \cite[pp.~439, 461]{PetersAndPedrycz2000}, \cite[p.~421]{vanVliet2000},
    \cite[p.~5\=/5]{SWEBOK2024}, \cite[p.~4]{IEEE2022}\fi, it is wise to select
multiple approaches \ifnotpaper \citep[p.~18]{IEEE2022} \else
    \citetext{p.~18} \fi to ``suitably cover any system''
\citetext{p.~33}. We record \approachCount{} test approaches mentioned in
the literature in \ourApproachGlossary{}, along with their properties and
relations. This includes any categories (\Cref{cats-def}), synonyms
(\Cref{syn-rels}), and parent-child relations (\Cref{par-chd-rels}) described
by the literature. These concepts follow logically from the literature so are
technically ``results'' of our research, but we define them here for clarity
since we use them throughout this \docType{}.

% that includes ``test level, test type, test technique, test practice and
% \dots{} static testing'' \citep[p.~10]{IEEE2022} and is used to  & black or white box, minimum and maximum
% boundary value testing \citep[p.~465]{IEEE2017}                                                  \\

\subsubsection{Approach Categories}\label{cats-def}

Since there are so many test approaches, it is helpful to categorize them.
The literature provides many ways to do so, but perhaps the most widely used
is the one given by \ifnotpaper\else \citeauthor{IEEE2022} \fi \citet{IEEE2022}.
This schema divides test approaches into levels, types, techniques, and practices
\citeyearpar[Fig.~2; see \Cref{tab:ieeeCats}]{IEEE2022}. These categories seem
to be pervasive throughout the literature, particularly ``level'' and ``type''.
\phantomsection{}\label{nonIEEE-sources}%
For example, six non-IEEE sources also give unit testing, integration testing,
system testing, and acceptance testing as examples of test levels \ifnotpaper
    (\citealp[pp.~5\=/6 to 5\=/7]{SWEBOK2024}; \citealpISTQB{};
    \citealp[pp.~807\==808]{Perry2006}; \citealp[pp.~443\==445]{PetersAndPedrycz2000};
    \citealp[p.~218]{KuļešovsEtAl2013}\todo{OG Black, 2009};
    \citealp[pp.~9, 13]{Gerrard2000a})\else
    \cite[pp.~443\==445]{PetersAndPedrycz2000},
    \cite[pp.~5\=/6 to 5\=/7]{SWEBOK2024}, \cite{ISTQB},
    \cite[pp.~807\==808]{Perry2006}, \cite[pp.~9, 13]{Gerrard2000a},
    \cite[p.~218]{KuļešovsEtAl2013}\fi,
although they may use a different term for ``test
level'' (see \Cref{tab:ieeeCats}). Because of their widespread use and
their usefulness in dividing the domain of software testing into more
manageable subsets, we use these categories for now.

\ifnotpaper
    % \afterpage{
    \begin{landscape}
        \begin{table*}[hbtp!]
            % Moved earlier to display nicely in paper
            \ieeeCatsTable{}
        \end{table*}
    \end{landscape}
    % }
\fi

These four subcategories
of test approaches can be loosely described by what they specify as
follows\thesisissueref{21}:
\begin{itemize}
    \item \textbf{Level}: What code is tested
    \item \textbf{Practice}: How the test is structured and executed
    \item \textbf{Technique}: How inputs and/or outputs are derived
    \item \textbf{Type}: Which software quality is evaluated
\end{itemize}
For example, boundary value analysis is a test technique since its inputs are
``the boundaries of equivalence partitions'' \ifnotpaper
    (\citealp[p.~2]{IEEE2022}; \citeyear[p.~1]{IEEE2021}; similar on p.~12 and
    in \citealpISTQB{})%
\else
    \cite[p.~2]{IEEE2022}, \cite[p.~1]{IEEE2021}%
\fi. Similarly, acceptance testing is a test level since its goal is to
``enable a user, customer, or other authorized entity to determine whether to
accept a system or component'' \ifnotpaper (\citealp[p.~5]{IEEE2017}; similar
    in \citeyear[p.~6]{IEEE2021}; \citealp[p.~344]{SakamotoEtAl2013})\else
    \cite[p.~5]{IEEE2017}\fi, which requires the system or component to be
developed and ready for testing.

Based on how we observe test approaches being categorized in the literature, we
make the following changes to the schema from \citet[Fig.~2]{IEEE2022}:

\begin{enumerate}
    \item We omit ``static testing'' as a test approach category, since it
          seems to be non-orthogonal to the others and thus less helpful for
          grouping test approaches. \ifnotpaper Other categorization schemas
              (see \Cref{alt-cats}) may consider static testing orthogonal, and
              some may consider it out-of-scope entirely (see \Cref{static-test})! \fi
    \item We introduce a non-orthogonal category of ``artifact''\thesisissueref{44,119,39}
          since some terms can refer to the application of a test approach
          \emph{as well as} the resulting document(s). Therefore, we do
          \emph{not} consider approaches categorized as an artifact \emph{and}
          another category as flaws\thesisissueref{119}, and as such omit them
          from \Cref{multiCats}.
\end{enumerate}

% For example, static assertion checking (mentioned by \ifnotpaper
%     \citealp[p.~345]{LahiriEtAl2013}; \citealp[p.~343]{ChalinEtAl2006}\else
%     \citealp[p.~343]{ChalinEtAl2006}, \citealp[p.~345]{LahiriEtAl2013}\fi) is a
% subapproach of assertion checking, which can also be performed dynamically.
% This parent-child relation (defined in \Cref{par-chd-rels}) means that static
% assertion checking may inherit assertion checking's inferred category of
% ``practice''. Based on observations such as this, we categorize testing
% approaches, \emph{including} static ones, based on the remaining categories
% from \citet{IEEE2022}.
% \ifnotpaper However, since there are many ways to categorize test approaches
%     (see \Cref{tab:otherCats,tab:otherCategorizations}), considering static
%     testing as an orthogonal distinction could make sense in specific contexts
%     (see \Cref{static-test}).

% \fi
% While we can categorize the vast majority of identified test approaches
% based on \citet{IEEE2022}'s categories, there are some outliers. For example,
% we categorize some test approaches as ``artifacts'',
% since some terms can refer to the application of a
% test approach \emph{as well as} the resulting document(s). Therefore,
% \ifnotpaper
%     Additionally, we identify ``test metrics''\thesisissueref{21,22} that
%     describe methods for \emph{evaluating} testing as opposed to methods for
%     \emph{performing} it and are therefore out-of-scope. Instead,
%     we capture the related test approaches that seek to maximize these
%     metrics as subsets of coverage-driven testing (see \Cref{cov-test}) and
%     experience-based testing \citep[p.~34]{IEEE2022}.
% \fi

\subsubsection{Synonym Relations}\label{syn-rels}

The same approach often has many names. For example,
``specification-based testing'' is also called\todo{more in Umar2000}:
\begin{enumerate}
    \item ``black-box testing''
          \ifnotpaper
              (\citealp[p.~9]{IEEE2022}; \citeyear[p.~8]{IEEE2021};
              \citeyear[p.~431]{IEEE2017}; \citealp[p.~5\=/10]{SWEBOK2024};
              \citealpISTQB{}; \citealp[p.~46 (without hyphen)]{Firesmith2015};
              \citealp[p.~344]{SakamotoEtAl2013}; \citealp[p.~399]{vanVliet2000})
          \else
              \cite[p.~9]{IEEE2022}, \cite{ISTQB}, \cite[p.~431]{IEEE2017},
              \cite[p.~5\=/10]{SWEBOK2024}, \cite[p.~8]{IEEE2021},
              % \cite[p.~46 (without hyphen)]{Firesmith2015},
              \cite[p.~399]{vanVliet2000},
              \cite[p.~344]{SakamotoEtAl2013}
          \fi
    \item ``closed-box testing''
          \ifnotpaper
              (\citealp[p.~9]{IEEE2022}; \citeyear[p.~431]{IEEE2017})
          \else
              \cite[p.~9]{IEEE2022}, \cite[p.~431]{IEEE2017}
          \fi
    \item ``functional testing''\footnote{``Functional testing'' may not
              \emph{actually} be a synonym for ``specification-based testing'';
              see \Cref{spec-func-test}.}
          \ifnotpaper
              (\citealp[p.~196]{IEEE2017}; \citealp[p.~44]{Kam2008};
              \citealp[p.~399]{vanVliet2000}; implied by
              \citealp[p.~129]{IEEE2021}; \citeyear[p.~431]{IEEE2017})
          \else
              \cite[p.~196]{IEEE2017}, \cite[p.~399]{vanVliet2000},
              \cite[p.~44]{Kam2008} (implied by \cite[p.~431]{IEEE2017},
              \cite[p.~129]{IEEE2021})
          \fi
    \item ``domain testing'' \citep[p.~5\=/10]{SWEBOK2024}
    \item ``specification-oriented testing'' \citep[p.~440, Fig.~12.2]{PetersAndPedrycz2000}
    \item ``input domain-based testing'' (implied by \citealp[pp.~4\=/7 to
              4\=/8]{SWEBOK2014})
\end{enumerate}
Throughout our work, we use the terms
``specification-based testing'' and ``structure-based testing'' to articulate
the source of the information for designing test cases, but a team or project
also using grey-box testing may prefer the terms ``black-box'' and ``white-box
testing'' for consistency. Thus, synonyms are not inherently problematic,
although they can be (see \Cref{syns}).

\defRel{synonym}{S}{how synonyms are used in natural language}
$S$ is symmetric and transitive, and although pairs of
synonyms in natural language are implied to be distinct, a relation that is
symmetric and transitive is provably reflexive; this implies that all terms are
trivially synonyms of themselves. Since $S$ is symmetric, transitive, and
reflexive, it is therefore an equivalence relation, reflecting the role of
synonyms in natural language where they can be used interchangeably. While
synonyms may emphasize different aspects or express mild variations, their core
meaning is nevertheless the same.

As an example of synonyms in natural language, ``windy'' is a synonym of
``gusty'' \citep{WindySyns}; since this is a symmetric relation, the inverse is
also true \citeyearpar{GustySyns}. ``Windy'' is also a synonym of ``blustery''
\citeyearpar{WindySyns}, so since this relation is transitive and ``gusty'' is
a synonym of ``windy'', ``gusty'' is also a synonym of ``blustery''
\citeyearpar{GustySyns}. Although these terms are distinct
\citeyearpar{GustySyns,WindySyns} and have nuance---``gust'' is defined as ``a
sudden brief rush of wind'' \citeyearpar{GustDef}---they still reflect the
same core concept and could be considered synonyms of themselves.

Synonym relations are often given explicitly in the literature. For example,
\citet[p.~9]{IEEE2022} \multiAuthHelper{list} ``black-box testing'' and
``closed box testing'' beneath the glossary entry for ``specification-based
testing'', meaning they are synonyms. ``Black-box testing'' is likewise given
under ``functional testing'' in \citeyearpar[p.~196]{IEEE2017}, meaning it is
also a synonym for ``specification-based testing'' through transitivity%
\qtodo{Is this clear/correct? Should I explain this more?}.
However, these relations can also be implicit (see \Cref{explicitness});
``functional testing'' is listed in a \emph{cf.} footnote to the glossary entry
for ``specification-based testing'' \citeyearpar[p.~431]{IEEE2017}, which
supports the previous claim but would not necessarily indicate a synonym
relation on its own.

Similarly, \citet[p.~5-10]{SWEBOK2024} says ``\emph{specification-based
    techniques} \dots{} [are] sometimes also called domain
testing techniques'' in the \acs{swebok} V4, from which the synonym of
``domain testing'' follows logically. However, its predecessor V3 only
\emph{implies} the more specific ``input domain-based testing'' as a synonym.
The section on test techniques says ``the classification of testing techniques
presented here is based on how tests are generated: from the software
engineer's intuition and experience, the specifications, the code structure
\dots'' \citep[p.~4\=/7]{SWEBOK2014}, and the first three subsections on the
following page are ``Based on the Software Engineer's Intuition and
Experience'', ``Input Domain-Based Techniques'', and ``Code-Based Techniques''
\citetext{p.~4\=/8}. The order of the introductory list lines up with these
sections, implying that ``input domain-based techniques'' are ``generated[]
from \dots{} the specifications'' (i.e., that input domain-based testing is the
same as specification-based testing). Furthermore, the examples of input
domain-based techniques given---equivalence partitioning, pairwise testing,
boundary-value analysis, and random testing---are all given as children%
\footnote{
    Pairwise testing is given as a child of combinatorial testing, which is
    itself a child of specification-based testing, by \ifnotpaper
        \citep[Fig.~2]{IEEE2021} and \citep[pp.~5\=/11 to 5\=/12]{SWEBOK2024}%
    \else
        \cite[pp.~5\=/11 to 5\=/12]{SWEBOK2024} and \cite[Fig.~2]{IEEE2021}%
    \fi, making it a ``grandchild'' of specification-based testing according to
    these sources.
} of specification-based testing \ifnotpaper
    (\citealp{IEEE2022}; \citeyear[Fig.~2]{IEEE2021}; \citealpISTQB{})\else
    \cite{IEEE2022,ISTQB}, \cite[Fig.~2]{IEEE2021}\fi; even V4 agrees with
this \citep[pp.~5\=/11 to 5\=/12]{SWEBOK2024}!

\subsubsection{Parent-Child Relations}\label{par-chd-rels}
Many test approaches are multi-faceted and can be ``specialized'' into others;
for example, there are many subtypes of performance-related testing,
such as load testing and stress testing (see \Cref{perf-test-rec}). These
``specializations'' will be referred to as ``children'' or ``subapproaches''
of the multi-faceted ``parent''. This nomenclature also extends to other
categories (such as ``subtype''; see \Cref{cats-def,tab:ieeeCats})
and software qualities (``subquality''\ifnotpaper; see \Cref{qual-test}\fi).

\defRel{parent-child}{P}{directed relations between approach pairs} This
relation should be irreflexive, asymmetric and transitive, making it a strict
partial order, and a given given ``child'' approach $c$ may have more than one
``parent'' approach $p$. This often manifests when a ``well-understood'' test
approach $p$ is decomposed into smaller, independently performable approaches
$c_1, \dots, c_n$, each with its own focus or nuance. This is often the case
for hierarchies of approaches given in the literature \ifnotpaper
    (\citealp[Fig.~2]{IEEE2022}; \citeyear[Fig.~2]{IEEE2021};
    \citealp{Firesmith2015})\else \cite{Firesmith2015},
    \cite[Fig.~2]{IEEE2022}, \cite[Fig.~2]{IEEE2021}\fi; see the graph of
specification-based test approaches in \Cref{fig:specBasedGraph}.

Another possible way for a parent-child relation to occur is when $c$'s
adequacy criteria is already satisfied by $p$. In other words, the completion
of $p$ indicates that ``sufficient testing has been done'' in regards to $c$
\citep[p.~402]{vanVliet2000}. While this metric only ``compares the
thoroughness of test techniques, not their ability to detect faults''
\citetext{p.~434}, it is sufficient to justify a parent-child relation
between the two approaches. These relations may also be represented as
hierarchies \ifnotpaper (\citealp[Fig.~F.1]{IEEE2021};
    \citealp[Fig.~13.17]{vanVliet2000})\else
    \citetext{Fig.~13.17}, \cite[Fig.~F.1]{IEEE2021}\fi; see the graph of
data flow test approaches in \Cref{fig:subsumesGraph}.

\parChdGraphs{}

% Based on what we observe in the literature,
% $P(c, p) = c \subset p \textrm{ or } c \Leftarrow p$. In other words,
% one of the following must be true:
% % There are many reasons two approaches may have a parent-child relation, such as:

% \begin{enumerate}
%     \item Everything described by $c$ is also described by $p$; $c \subset p$.
%           This is often the case when one ``well-understood'' subset of testing
%           can be decomposed into smaller, independently performable approaches,
%           each with its own focus or nuance. This is often the case for
%           hierarchies of approaches given in the literature \ifnotpaper
%               (\citealp[Fig.~2]{IEEE2022}; \citeyear[Fig.~2]{IEEE2021};
%               \citealp{Firesmith2015})\else \cite{Firesmith2015},
%               \cite[Fig.~2]{IEEE2022}, \cite[Fig.~2]{IEEE2021}\fi; we graph
%           the parent-child relations in the subdomain of specification-based
%           testing in \Cref{fig:specBasedGraph} as an example.
%     \item Completing $p$ implies that $c$ has also been completed; $c \Leftarrow p$.
%           This is really just a more specific case of the previous condition that
%           occurs when $c$'s adequacy criteria is already met by $p$. While
%           this relation only ``compares the thoroughness of test techniques, not
%           their ability to detect faults'' \citep[p.~434]{vanVliet2000}, it is
%           sufficient to constitute a parent-child relation. These may also be
%           represented as hierarchies \ifnotpaper (\citealp[Fig.~F.1]{IEEE2021};
%               \citealp[Fig.~13.17]{vanVliet2000})\else
%               \citetext{Fig.~13.17}, \cite[Fig.~F.1]{IEEE2021}\fi; we graph the
%           parent-child relations in the subdomain of data flow testing in
%           \Cref{fig:subsumesGraph} as an example.
% \end{enumerate}

% \begin{enumerate}
%     \item \textbf{One is a superset of the other.} In other words, for one
%           (parent) test approach to be performed in its entirety, the other
%           (child) approach will necessarily be performed as well. This is often
%           the case when one ``well-understood'' subset of testing can be
%           decomposed into smaller, independently performable approaches.
%           When all of these have been completed, we can logically conclude that
%           the parent approach has also been performed! In practice, this is
%           much harder to prove; although many hierarchies exist \ifnotpaper
%               (\citealp[Fig.~2]{IEEE2022}; \citeyear[Fig.~2]{IEEE2021};
%               \citealp{Firesmith2015})\else \cite{Firesmith2015},
%               \cite[Fig.~2]{IEEE2022}, \cite[Fig.~2]{IEEE2021}\fi, these are
%           likely incomplete. As an example, we graph the parent-child relations
%           from \ifnotpaper (\citealp[Fig.~2]{IEEE2022}; \citeyear[Fig.~2]{IEEE2021})
%           \else \cite[Fig.~2]{IEEE2022}, \cite[Fig.~2]{IEEE2021} \fi in the
%           subdomain of specification-based testing in \Cref{fig:specBasedGraph}
%           (along with relevant data from other sources).
%     \item \textbf{One is ``stronger than'' or ``subsumes'' the other.} When
%           comparing adequacy criteria that ``specif[y] requirements for
%           testing'' \citep[p.~402]{vanVliet2000}, ``criterion X is stronger
%           than criterion Y if, for all programs P and all test sets T,
%           X-adequacy implies Y-adequacy'' \citetext{p.~432}. While this
%           relation only ``compares the thoroughness of test techniques, not
%           their ability to detect faults'' \citetext{p.~434}, it is sufficient
%           to consider one a child of the other in a sense. We capture this
%           nuance by considering these parent-child relations implicit
%           (see \Cref{explicitness}). As an example, we graph the parent-child
%           relations from \ifnotpaper (\citealp[Fig.~F.1]{IEEE2021};
%               \citealp[Fig.~13.17]{vanVliet2000}) \else
%               \citetext{Fig.~13.17}, \cite[Fig.~F.1]{IEEE2021}
%           \fi in the subdomain of data flow testing can be found in
%           \Cref{fig:subsumesGraph} (along with relevant data from other sources).
%           \item \textbf{The parent approach is part of an orthogonal set.}
%                 When presented with a set of generic test approaches that are
%                 orthogonal to each other, it is often trivial to classify a given
%                 test approach as a child of just one them. For example,
%                 \citeauthor{IEEE2022} say that ``testing can take two forms: static
%                 and dynamic'' \citeyearpar[p.~17]{IEEE2022} and provide examples of
%                 subapproaches of static and dynamic testing \citetext{Fig.~1}.
%                 Likewise, \citeauthor{Gerrard2000a} says ``tests can be automated or
%                 manual'' \citeyearpar[p.~13]{Gerrard2000a} and gives subapproaches of
%                 automated and manual testing \ifnotpaper
%                     \citeyearpar[Tab.~2; ][Tab.~1]{Gerrard2000b}\else
%                     \citetext{Tab.~2}, \cite[Tab.~1]{Gerrard2000b}\fi. However, the
%                 orthogonality of these subsets does not mean they are mutually
%                 exclusive; in these same tables, \citeauthor{Gerrard2000a} labels
%                 usability testing as both static \emph{and} dynamic and 12 approaches
%                 as able to ``be done manually \emph{or} using a tool''
%                 \citeyearpar[p.~13\ifnotpaper, emphasis added\fi]{Gerrard2000a}.
% \end{enumerate}

\subsection{Flaws/Inconsistencies}\label{flaw-def}

Before we can start tracking and discussing flaws, we need to be clear about
what we mean by the term. Our research shows that the literature is full of
incorrect, missing, contradictory, unclear, nonatomic, and redundant
information. This should not be the case for a field as rigorous as software
engineering; we should be correct, complete, consistent, and clear, keeping
separate ideas separate and only including what is necessary! We refer to
any instance where one of these ideals is violated as a ``flaw'', as it
implies that something is wrong with the literature and an opportunity to
improve it, while avoiding words that are ``overloaded with too many meanings''
like ``error'' and ``fault'' \citep[p.~12\=/3; see
    \Cref{error-fault-failure}]{SWEBOK2024}. Terms such as these are primarily
used to describe software itself, while we want a term to describe its
supporting documents\footnote{A small literature review reveals that
    established standards (see \Cref{stds}) only use ``flaw'' to refer to
    requirements \citep[p.~38]{IEEE2022}, design \citetext{p.~43}, ``system
    security procedures \dots{} and internal controls''
    % under the term ``vulnerability''
    \citep[p.~194]{IEEE2012}, or code itself \citetext{p.~92}.}. \refHelper
\citet[p.~7\=/9]{SWEBOK2024} mentions that ``techniques and indicators can help
engineers measure technical debt, including \dots{} the number of engineering
flaws and violations'', which aligns with our goal of analyzing what is wrong
with software engineering's testing literature.

\subsubsection{Flaw Manifestations}\label{mnfst-def}

Perhaps the most obvious example of something being ``wrong'' with the
literature is that a piece of information it presents is incorrect---``wrong''
in the literal sense. However, if our standards for correctness require
clarity, consistency, and robustness, then there are many ways for a flaw to
manifest. This is one view we take when observing, recording, and analyzing
flaws: \emph{how}\thesisissueref{155} information is ``wrong''. We observe the
``manifestations'' described in \Cref{tab:flawMnfstDefs} throughout the
literature, and give each a unique key for later analysis and discussion. We
list them in descending order of severity, although this is partially
subjective: while some may disagree with our ranking, it is clear that
information being incorrect is worse than it being repeated. This ordering has
the benefit of serving as a ``flowchart'' for classifying flaws. For example,
if a piece of information is not intrinsically incorrect, then there are five
remaining manifestation types the flaw can be!

\input{assets/tables/flawMnfstDefs}

While we use the general term ``flaw'' to encompass all of these
manifestations, it has its limitations. In particular, when referring to a flaw
with more than one source (such as a contradiction or an overlap), it is
awkward to call it a ``flaw between two sources''\thesisissueref{140}. A more
intuitive way to describe this situation is that there is an ``inconsistency''
between the sources. This clearly indicates that there is disagreement between
the sources, but also does not imply that either one is correct---the
inconsistency could be with some ground truth\qtodo{Is this term well understood?}
if \emph{neither} source is correct\thesisissueref{140,151}! Note that these
cases are not categorized as ``mistakes'' if finding this ground truth requires
analysis that has not been performed yet.

\ifnotpaper \newpage \fi

\subsubsection{Flaw Domains}\label{dmn-def}

Another way to categorize flaws is by \emph{what} information is wrong, which
we call the flaw's ``domain''. We describe those we observe in
\Cref{tab:flawDmnDefs}, and tracking these uncovers which knowledge domains
are less standardized (and should therefore be approached with more rigour)
than others. Note that this is an orthogonal categorization to that of a flaw's
manifestation: each flaw \emph{manifests} in a particular \emph{domain}. This
means that we give each flaw two keys (one for each classification) and
present our observations according to these two views in \Cref{tab:flawMnfsts,%
    tab:flawDmns}. We explicitly define some of these domains in future
chapters and thus present them in that same order. Despite their nuance, the
remaining domains are relatively straightforward, so we do not define them more
rigorously in their own sections.

\input{assets/tables/flawDmnDefs}

\phantomsection{}\label{label-flaw-def}
For example, \defLabelDistinct*{}. Definition flaws are quite self-explanatory, but
% Definition flaws occur when a term's definition is incorrect, unclear, etc.;
% this often occurs in glossaries or lists of terms from sources we investigate.
% On the other hand,
label flaws are harder to detect, despite occurring independently. Examples of
label flaws include terms that share the same acronym or contain typos or
redundant information. Sometimes, an author may use one term when they mean
another; while one could argue that their ``internal'' definition of the term
is the cause of this mistake, we consider this a label flaw where the wrong
label is used and we would change the \emph{label} to fix it.
\phantomsection{}\label{trace-flaw-def}%
Additionally, some traceability information is flawed, such as how one document
cites other or even what information is included \emph{within} a document
(see \flawref{see-ref-missing})!

\subsection{Explicitness}\label{explicitness}

% Excluded from macro table since it is only used in this subsection
\def\impKeywordsCode{\seeSrcCode{23dbe41}{scripts/helpers.py}{25}{54}}

When recording information from unstandardized sources written in natural
language, there is a considerable degree of nuance that can get lost. For
example, a source may provide data from which the reader can logically draw a
conclusion, but may not state or prove this conclusion explicitly.
% Most information in sources we investigate is explicit, but this is still a useful
% distinction to make.
We call this nuance ``explicitness'' and capture it when citing sources using
(at least) one of the following keywords: ``implied'', ``inferred'',
``can be'', ``sometimes'', ``should be'', ``ideally'', ``usually'', ``most'',
``likely'', ``often'', ``if'', and ``although''% \impKeywordsCode{}%
\utd{}. While these keywords often appear
directly within the literature, we also use them to track explicitness without
getting distracted by less relevant details by summarizing the relevant
nuance. This allows us to provide a more complete picture of the state of
the literature. All sources cited throughout this \docType{} support their
respective claims explicitly unless specified otherwise, usually via one of the
keywords given above\qtodo{Is this enough to satisfy Dr.~Smith's concern about
    the explicitness of Synonym Relation sources?}.

Any kind of information can be implicit, including the names,
\approachFields{} of identified test approaches. There are many ways that
information can be \emph{not} explicit that we observe in the literature.
Despite ``implicit'' only describing the first of these cases, we use it
(as well as ``implied by'' when describing sources of information) as a
shorthand for all ``not explicit'' manifestations throughout this \docType{}
for clarity.\qtodo{Is this OK?} The following are non-mutually exclusive
examples from the literature of ways that information can be ``implicit'';
we use the keywords from the above list during data collection to identify
them when performing later analysis \impKeywordsCode{}:

%% Maybe convert to \paragraph ?
\begin{description}
    \item[1. The information follows logically.]
          \phantomsection{}\label{imp-case-one}~\\
          \refHelper \citet[pp.~53\==58]{Firesmith2015} lists a set of test
          approaches that are ``based on the[ir] associated quality
          characteristic[s] and \dots{} associated quality attributes''
          \citetext{p.~53}. This matches our definition of ``test type'' in
          \Cref{cats-def}, but this term is used more loosely to refer to
          different kinds of testing---what we would call ``test approaches''.
          We therefore consider these categorizations to be implicit, as in
          \Cref{tab:multiCats\ifnotpaper,tab:infMultiCats\fi}.
    \item [2. The information is not universal.]
          \phantomsection{}\label{imp-case-two}~\\
          \refHelper \citet[p.~372\ifnotpaper, emphasis added\fi]{IEEE2017}%
          \todo{OG ISO/IEC, 2014} \multiAuthHelper{define} ``regression
          testing'' as ``testing required to determine that a change to a
          system component has not adversely affected \emph{functionality,
              reliability or performance} and has not introduced additional
          defects''. While reliability testing, for example, is not
          \emph{always} a subset of regression testing (since it may be
          performed in other ways), it \emph{can be} accomplished by regression
          testing, so there is sometimes a parent-child relation (defined in
          \Cref{par-chd-rels}) between them.
          %   \citet[p.~5\=/8\ifnotpaper, emphasis added\fi]{SWEBOK2024} provides a
          %   similar list: ``regression testing \dots{} \emph{may} involve
          %   functional and non-functional testing, such as reliability,
          %   accessibility, usability, maintainability, conversion, migration, and
          %   compatibility testing.''
    \item[3. The information is \ifnotpaper conditional\else dubious\fi.]
          \phantomsection{}\label{imp-case-three}~\\ \ifnotpaper
          As a more specific case of information not being universal, sometimes
          prerequisites must be satisfied for information to apply. For
          example, branch condition combination testing is equivalent
          to (and is therefore a synonym of) exhaustive testing \emph{if} ``each
          subcondition is viewed as a single input'' \citep[p.~464]{PetersAndPedrycz2000}.
          Likewise, statement testing can be used for (and is therefore a child
          of) unit testing \emph{if} there are ``less than 5000 lines of code''
          \citetext{p.~481\todo{OG Miller et al., 1994}}.
    \item[4. The information is dubious.]
          \phantomsection{}\label{imp-case-four}~\\ \fi
          This happens when there is reason to doubt the information provided.
          If a source claims one thing that is not true, related claims lose
          credibility. For example, \redBoxFlaw*{}
          %   Doubts such as this
          %   can also originate from other sources. \refHelper
          %   \citet[p.~48]{Kam2008} gives ``user scenario testing'' as a synonym
          %   of ``use case testing'', even though ``an actor [in use case testing]
          %   can be \dots{} another system'' \citep[p.~20]{IEEE2021}, which does
          %   not fit as well with the label ``user scenario testing''. However,
          %   since a system can be seen as a ``user'' of the test item, this
          %   synonym relation is treated as implicit instead of as an objective
          %   flaw.
\end{description}

As our research focuses on the flaws present in the literature, the
explicitness of information affects how seriously we take it. We call flaws
based on explicit information ``objective'', since they are self-evident in
the literature. On the other hand, we call flaws based on implicit information
``subjective'', since some level of judgement is required to assess whether
these flaws are \emph{actually} problematic\thesisissueref{176}. By looking for
the indicators of uncertainty mentioned above \impKeywordsCode{}, we can
automatically detect subjective flaws when generating graphs and performing
analysis (see \ifnotpaper \Cref{graph-gen,flaw-analysis}, respectively\else
    \Cref{tools}\fi).
%     These are used when creating
% the glossaries to capture varying degrees of nuance, such as when a test
% approach ``can be'' a child of another or is a synonym of another ``most of the
% time'' but not always. 
For example, we can view subjective flaws separately in
\Cref{tab:flawMnfsts,tab:flawDmns}, since additional context may rectify them.
\ifnotpaper Additionally, \Cref{tab:parSyns,tab:multiCats,tab:infMultiCats}
    contain relations that are explicit, implicit, and both; implicit relations
    are marked by the phrases ``implied by'' and ``inferred from''. \fi

\subsection{Credibility}\label{cred}

In the same way we distinguish between the explicitness of information from
different sources, we also wish to distinguish between the ``explicitness'' of the
sources themselves! Of course, we do not want to overload terms, so we define a
source as more ``credible'' if it:
\begin{itemize}
    \item has gone through a peer-review process,
    \item is written by numerous, well-respected authors,
    \item cites a (comparatively) large number of sources, and/or
    \item is accepted and used in the field of software.
\end{itemize}
Sources may meet only some of these criteria, so we use our judgement (along
with the format of the sources themselves) when comparing them
(cf.~\Cref{sources}).
