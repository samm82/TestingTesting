\section{Terminology}
\label{terminology}

Our research aims to describe the current state of software testing literature,
including its flaws. Since we critique the lack of clarity, consistency, and
robustness in the literature, we do our best to hold ourselves to a higher
standard by defining and using terms consistently. For example, before we can
constructively describe the flaws in the literature, we need to define what
we mean by ``flaw'' and its related terms (\Cref{flaw-def}). We also track if
information found in our sources is more implicit (and therefore more
subjective) by defining the notion of ``rigidity'' (\Cref{rigidity}) and
similarly track the ``trustworthiness'' of the sources themselves
(\Cref{trust}). We make these distinctions to reduce how much our
preconceptions affect our analysis (or at least make it more obvious to future
researchers). To further prevent bias, we do not invent
or add our own classifications or kinds of relations. Instead, the notions of
test approach categories (\Cref{cats-def}), synonyms (\Cref{syn-rels}), and
parent-child relations (\Cref{par-chd-rels}) we present here
follow logically from the literature. We define them here for clarity
since we use them throughout this \docType{}, even though they are
``results'' of our research.

\subsection{Flaws/Inconsistencies}\label{flaw-def}

Before we can start tracking and discussing flaws, we need to be clear about
what we mean by the term. Our research shows that the literature is full of
incorrect, missing, contradictory, unclear, nonatomic, and redundant
information. This should not be the case for a field as rigorous as software
engineering; we should be correct, complete, consistent, and clear, keeping
separate ideas separate and only including what is necessary! We refer to
any instance where one of these ideals is violated as a ``flaw'', as it
implies that something is wrong with the literature and an opportunity to
improve it, while avoiding words that are ``overloaded with too many meanings''
like ``error'' and ``fault'' \citep[p.~12\=/3; see
    \Cref{error-fault-failure}]{SWEBOK2024}. Terms such as these are primarily
used to describe software itself, while we want a term to describe its
supporting documents\footnote{A small literature review reveals that
    established standards (see \Cref{stds}) only use ``flaw'' to refer to
    requirements \citep[p.~38]{IEEE2022}, design \citetext{p.~43}, ``system
    security procedures \dots{} and internal controls''
    % under the term ``vulnerability''
    \citep[p.~194]{IEEE2012}, or code itself \citetext{p.~92}.}. \refHelper
\citet[p.~7\=/9]{SWEBOK2024} mentions that ``techniques and indicators can help
engineers measure technical debt, including \dots{} the number of engineering
flaws and violations'', which aligns with our goal of analyzing what is wrong
with software engineering's testing literature.

\subsubsection{Flaw Manifestations}\label{mnfst-def}

Perhaps the most obvious example of something being ``wrong'' with the
literature is that a piece of information it presents is incorrect---``wrong''
in the literal sense. However, if our standards for correctness require
clarity, consistency, and robustness, then there are many ways for a flaw to
manifest. This is one view we take when observing, recording, and analyzing
flaws: \emph{how}\thesisissueref{155} information is ``wrong''. We observe the
``manifestations'' described in \Cref{tab:flawMnfstDefs} throughout the
literature, and give each a unique key for later analysis and discussion. We
list them in descending order of severity, although this is partially
subjective: while some may disagree with our ranking, it is clear that
information being incorrect is worse than it being repeated. This ordering has
the benefit of serving as a ``flowchart'' for classifying flaws. For example,
if a piece of information is not intrinsically incorrect, then there are five
remaining manifestation types the flaw can be!

\input{assets/tables/flawMnfstDefs}

While we use the general term ``flaw'' to encompass all of these
manifestations, it has its limitations. In particular, when referring to a flaw
with more than one source (such as a contradiction or an overlap), it is
awkward to call it a ``flaw between two sources''\thesisissueref{140}. A more
intuitive way to describe this situation is that there is an ``inconsistency''
between the sources. This clearly indicates that there is disagreement between
the sources, but also does not imply that either one is correct---the
inconsistency could be with some ground truth\qtodo{Is this term well understood?}
if \emph{neither} source is correct\thesisissueref{140,151}! Note that these
cases are not categorized as ``mistakes'' if finding this ground truth requires
analysis that has not been performed yet.

\ifnotpaper \newpage \fi

\subsubsection{Flaw Domains}\label{dmn-def}

Another way to categorize flaws is by \emph{what} information is wrong, which
we call the flaw's ``domain''. We describe those we observe in
\Cref{tab:flawDmnDefs}, and tracking these uncovers which knowledge domains
are less standardized (and should therefore be approached with more rigour)
than others. Note that this is an orthogonal categorization to that of a flaw's
manifestation: each flaw \emph{manifests} in a particular \emph{domain}. This
means that we give each flaw two keys (one for each classification) and
present our observations according to these two views in \Cref{tab:flawMnfsts,%
    tab:flawDmns}. We explicitly define some of these domains in future
chapters and thus present them in that same order. Despite their nuance, the
remaining domains are relatively straightforward, so we do not define them more
rigorously in their own sections.

\input{assets/tables/flawDmnDefs}

\phantomsection{}\label{label-flaw-def}
For example, \defLabelDistinct*{}. Definition flaws are quite self-explanatory, but
% Definition flaws occur when a term's definition is incorrect, unclear, etc.;
% this often occurs in glossaries or lists of terms from sources we investigate.
% On the other hand,
label flaws are harder to detect, despite occurring independently. Examples of
label flaws include terms that share the same acronym or contain typos or
redundant information. Sometimes, an author may use one term when they mean
another; while one could argue that their ``internal'' definition of the term
is the cause of this mistake, we consider this a label flaw where the wrong
label is used and we would change the \emph{label} to fix it.
\phantomsection{}\label{trace-flaw-def}%
Additionally, some traceability information is flawed, such as how one document
cites other or even what information is included \emph{within} a document
(see \flawref{see-ref-missing})!

\subsection{Rigidity}\label{rigidity}

% Excluded from macro table since it is only used in this subsection
\def\impKeywordsCode{\seeSrcCode{82167b7}{scripts/helpers.py}{21}{49}}

When recording information from unstandardized sources written in natural
language, there is a considerable degree of nuance that can get lost. For
example, a source may provide data from which the reader can logically draw a
conclusion, but may not state or prove this conclusion explicitly.
% Most information in sources we investigate is explicit, but this is still a useful
% distinction to make.
We call this nuance ``rigidity'' and capture it when citing sources using
(at least) one of the following keywords: ``implied'', ``inferred'',
``can be'', ``should be'', ``ideally'', ``usually'', ``most'', ``likely'',
``often'', ``if'', and ``although''% \impKeywordsCode{}%
\utd{}. While these keywords often appear
directly within the literature, we also use them to track rigidity without
getting distracted by less relevant details by summarizing the relevant
nuance. This allows us to provide a more complete picture of the state of
the literature. All sources cited throughout this \docType{} support their
respective claims explicitly unless specified otherwise, usually via one of the
keywords given above\qtodo{Is this enough to satisfy Dr.~Smith's concern about
    the rigidity of Synonym Relation sources?}. The following non-mutually
exclusive reasons for information to be considered ``implicit'' emerged, and we
use the keywords from the above list during data collection to identify them
when performing later analysis \impKeywordsCode{}:

%% Maybe convert to \paragraph ?
\begin{enumerate}
    \item \textbf{The information can be logically deduced but is not explicit.}
          The implicit categorizations of ``test type'' by
          \citet[pp.~53\==58]{Firesmith2015} (cf.\qtodo{Is this enough to
              indicate this forward reference isn't that important?}~%
          \Cref{tab:multiCats\ifnotpaper,tab:infMultiCats\fi}) are examples
          of this. The given test approaches are not explicitly called ``test
          types'', as the term is used more loosely to refer to different kinds
          of testing---what should be called ``test approaches'' as per
          \Cref{tab:ieeeCats}. However, this set of test approaches are
          ``based on the associated quality characteristic and its associated
          quality attributes'' \citetext{p.~53}, implying that they are
          test types. Cases such as this are indicated by a question mark or
          one of the following keywords: ``implied'', ``inferred'', or
          ``likely''. \ifnotpaper \par
              Additionally, if a test approach in \ourApproachGlossary{} has a
              name ending in ``~(Testing)'' with a space, the word ``Testing''
              might not be part of its name \emph{or} it might not be a test
              approach at all! For example, the term ``legacy system
              integration'' is used \ifnotpaper by \citeauthor{Gerrard2000a}
                  (\citeyear[pp.~12\==13, Tab.~2]{Gerrard2000a};
                  \citeyear[Tab.~1]{Gerrard2000b})\else in
                  \cite[pp.~12\==13, Tab.~2]{Gerrard2000a},
                  \cite[Tab.~1]{Gerrard2000b}\fi, but the more accurate
              ``legacy system integration testing'' is used in
              \citeyearpar[pp.~30\==31]{Gerrard2000b}. In other cases where a
              term is \emph{not} explicitly labelled as ``testing'', we add the
              suffix ``~(Testing)'' (when it makes sense to do so) and consider
              the test approach to be implied. \fi
    \item \textbf{The information is not universal.}
          \refHelper \citet[p.~372\ifnotpaper, emphasis added\fi]{IEEE2017}%
          \todo{OG ISO/IEC, 2014} \multiAuthHelper{define} ``regression
          testing'' as ``testing required to determine that a change to a
          system component has not adversely affected \emph{functionality,
              reliability or performance} and has not introduced additional
          defects''. While reliability testing, for example, is not
          \emph{always} a subset of regression testing (since it may be
          performed in other ways), it \emph{can be} accomplished by regression
          testing, so there is sometimes a parent-child relation (defined in
          \Cref{par-chd-rels}) between them. \ifnotpaper
          \citet[p.~5-8\ifnotpaper, emphasis added\fi]{SWEBOK2024} provides a
          similar list: ``regression testing \dots{} \emph{may} involve
          functional and non-functional testing, such as reliability,
          accessibility, usability, maintainability, conversion, migration, and
          compatibility testing.'' \fi Cases such as this are indicated by one
          of the following keywords: ``can be'', ``should be'', ``ideally'',
          ``usually'', ``most'', ``likely'', ``often'', or ``if''.
    \item \textbf{The information is conditional.}
          As a more specific case of information not being universal, sometimes
          prerequisities must be satisfied for information to apply. For
          example, branch condition combination testing is equivalent
          to (and is therefore a synonym of) exhaustive testing \emph{if} ``each
          subcondition is viewed as a single input'' \citep[p.~464]{PetersAndPedrycz2000}.
          Likewise, statement testing can be used for (and is therefore a child
          of) unit testing \emph{if} there are ``less than 5000 lines of code''
          \citetext{p.~481\todo{OG Miller et al., 1994}}. Cases such as this
          are indicated by the keyword ``can be'' or ``if''. \ifnotpaper
              \par This can also apply more abstractly at the taxonomy level,
              where a parent-child relation only makes sense if the parent test
              approach exists. This occurs when a source gives a relation
              between qualities but at least one of them does not have an
              explicit approach associated with it (although it may be derived;
              see \Cref{cov-test}). For example, \citet{ISO_IEC2023a} provides
              relations involving dependability and modifiability; these are
              tracked as qualities, not approaches, since only the qualities
              are described. Since the prerequisite of the relevant approach
              existing is \emph{not} satisfied, these relations are omitted
              from any generated graphs (cf.\qtodo{Is this enough to indicate
                  this forward reference isn't that important?}~\recFigs{}%
              \latertodo{Include \Cref{fig:parChdGraphs} if it ends up being
                  sufficient.}). \fi
    \item \textbf{The information is dubious.}
          This happens when there is reason to doubt the information provided.
          If a source claims one thing that is not true, related claims lose
          credibility. For example, the incorrect claim that ``white-box
          testing'', ``grey-box testing'', and ``black-box testing'' are
          synonyms for ``module testing'', ``integration testing'', and
          ``system testing'', respectively, \ifnotpaper (see
              \flawref{dubious-syns}) \fi casts doubt on the claim that
          ``red-box testing'' is a synonym for ``acceptance testing''
          \citep[p.~18]{SneedAndGöschl2000}\todo{OG Hetzel88}\ifnotpaper\
              (see \flawref{dubious-red-box-syn})\fi. Doubts such as this
          can also originate from other sources. \refHelper
          \citet[p.~48]{Kam2008} gives ``user scenario testing'' as a synonym
          of ``use case testing'', even though ``an actor [in use case testing]
          can be \dots{} another system'' \citep[p.~20]{IEEE2021}, which does
          not fit as well with the label ``user scenario testing''. However,
          since a system can be seen as a ``user'' of the test item, this
          synonym relation is treated as implicit instead of as an outright
          flaw. Cases such as this are indicated by a question mark or
          one of the following keywords: ``inferred'', ``should be'',
          ``ideally'', ``likely'', ``if'', or ``although''.
\end{enumerate}

Any kind of information can be implicit, including the names,
\approachFields{} of identified test approaches. Flaws based on implicit
information are themselves implicit. By looking for the
indicators of uncertainty mentioned above \impKeywordsCode{}, we can
automatically detect implicit flaws when generating graphs and performing
analysis (see \ifnotpaper \Cref{graph-gen,flaw-analysis}, respectively\else
    \Cref{tools}\fi).
%     These are used when creating
% the glossaries to capture varying degrees of nuance, such as when a test
% approach ``can be'' a child of another or is a synonym of another ``most of the
% time'' but not always. 
For example, we can view implicit flaws separately in
\Cref{tab:flawMnfsts,tab:flawDmns}, since additional context may rectify them.
\ifnotpaper Additionally, \Cref{tab:parSyns,tab:multiCats,tab:infMultiCats}
    contain relations that are explicit, implicit, and both; implicit relations
    are marked by the phrases ``implied by'' and ``inferred from''. \fi

\subsection{Trustworthiness}\label{trust}

In the same way we distinguish between the rigidity of information from
different sources, we also wish to distinguish between the ``rigidity'' of the
sources themselves! Of course, we do not want to overload terms, so we define a
source as more ``trustworthy'' if it:
\begin{itemize}
    \item has gone through a peer-review process,
    \item is written by numerous, well-respected authors,
    \item cites a (comparatively) large number of sources, and/or
    \item is accepted and used in the field of software.
\end{itemize}
Sources may meet only some of these criteria, so we use our judgement (along
with the format of the sources themselves) when comparing them
(cf.~\Cref{sources}).

\subsection{Approach Categories}\label{cats-def}

While there are many ways to categorize software testing approaches, perhaps
the most widely used is the one given by \ifnotpaper\else \citeauthor{IEEE2022}
\fi \citet{IEEE2022}. This schema divides test approaches---a generic, catch-all
term for any form of ``testing''---into levels, types, techniques, and practices
\citeyearpar[Fig.~2; see \Cref{tab:ieeeCats}]{IEEE2022}. These categories seem
to be pervasive throughout the literature, particularly ``level'' and ``type''.
\phantomsection{}\label{nonIEEE-sources}%
For example, six non-IEEE sources also give unit testing, integration testing,
system testing, and acceptance testing as examples of test levels \ifnotpaper
    (\citealp[pp.~5\=/6 to 5\=/7]{SWEBOK2024}; \citealpISTQB{};
    \citealp[pp.~807\==808]{Perry2006}; \citealp[pp.~443\==445]{PetersAndPedrycz2000};
    \citealp[p.~218]{KuļešovsEtAl2013}\todo{OG Black, 2009};
    \citealp[pp.~9, 13]{Gerrard2000a})\else
    \cite[pp.~443\==445]{PetersAndPedrycz2000},
    \cite[pp.~5\=/6 to 5\=/7]{SWEBOK2024}, \cite{ISTQB},
    \cite[pp.~807\==808]{Perry2006}, \cite[pp.~9, 13]{Gerrard2000a},
    \cite[p.~218]{KuļešovsEtAl2013}\fi,
although they may use a different term for ``test
level'' (see \Cref{tab:ieeeCats}). Because of their widespread use and
their usefulness in dividing the domain of software testing into more
manageable subsets, we use these categories for now. These four subcategories
of test approaches can be loosely described by what they specify as
follows\thesisissueref{21}:
\begin{itemize}
    \item \textbf{Level}: What code is tested
    \item \textbf{Practice}: How the test is structured and executed
    \item \textbf{Technique}: How inputs and/or outputs are derived
    \item \textbf{Type}: Which software quality is evaluated
\end{itemize}
For example, boundary value analysis is a test technique since its inputs are
``the boundaries of equivalence partitions'' \ifnotpaper
    (\citealp[p.~2]{IEEE2022}; \citeyear[p.~1]{IEEE2021}; similar on p.~12 and
    in \citealpISTQB{})%
\else
    \cite[p.~2]{IEEE2022}, \cite[p.~1]{IEEE2021}%
\fi. Similarly, acceptance testing is a test level since its goal is to
``enable a user, customer, or other authorized entity to determine whether to
accept a system or component'' \ifnotpaper (\citealp[p.~5]{IEEE2017}; similar
    in \citeyear[p.~6]{IEEE2021}; \citealp[p.~344]{SakamotoEtAl2013})\else
    \cite[p.~5]{IEEE2017}\fi, which requires the system or component to be
developed and ready for testing.

\ifnotpaper
    % \afterpage{
    \begin{landscape}
        \begin{table*}[hbtp!]
            % Moved earlier to display nicely in paper
            \ieeeCatsTable{}
        \end{table*}
    \end{landscape}
    % }
\fi

Based on their definitions and usage, the categories given in
\Cref{tab:ieeeCats} seem to be orthogonal (excluding the ``approach''
supercategory). For example, ``a test type can be performed at a single test
level or across several test levels''
\ifnotpaper
    (\citealp[p.~15]{IEEE2022}; \citeyear[p.~7]{IEEE2021})%
\else
    \cite[p.~15]{IEEE2022}, \cite[p.~7]{IEEE2021}%
\fi, and ``Keyword-Driven Testing can be applied at all testing levels
\dots{} and for various types of testing'' \citeyearpar[p.~4]{IEEE2016}.
Therefore, we assume these categories to be orthogonal throughout this
\docType{} (e.g., when identifying flaws). We may assess this assumption more
rigorously in the future, but for now, it
implies that a specific test approach can be derived by combining multiple
test approaches from different categories. For example, usability test
script(ing) \citepISTQB{} is a combination of usability testing, a test type
\ifnotpaper (\citealp[pp.~22, 26--27]{IEEE2022};
    \citeyear[pp.~7, 40, Tab.~A.1]{IEEE2021}; implied by its quality;
    \citealp[p.~53]{Firesmith2015})\else \cite[pp.~22, 26--27]{IEEE2022},
    \cite[pp.~7, 40, Tab.~A.1]{IEEE2021}\fi, and scripted testing, a test
practice \citep[pp.~20, 22\ifnotpaper; implied by p.~33\fi]{IEEE2022}.
\ifnotpaper We present the following examples found in the literature, omitting
    their subapproaches for brevity since these are apparent from the name of
    each ``combination approach'':

    \phantomsection{}\label{orth-test-exs}
    \begin{enumerate}
        \item Black box conformance testing \citep[p.~25]{JardEtAl1999}
              %   (combining black box and conformance testing)
              % Specification-based: Technique (IEEE, 2022, p. 22; 2021, p. 8; Washizaki, 2024, p. 5-10; Hamburg and Mogyorodi, 2024; Souza et al., 2017, p. 3; Firesmith, 2015, pp. 46-47; Sakamoto et al., 2013, p. 344; implied by IEEE, 2022, pp. 2-4, 6-9)
              % Conformance: Type (implied by its quality (IEEE, 2017, p. 92; OG PMBOK 5th ed.))
        \item Black-box integration testing \citep[pp.~345\==346]{SakamotoEtAl2013}
              % Specification-based: Technique (IEEE, 2022, p. 22; 2021, p. 8; Washizaki, 2024, p. 5-10; Hamburg and Mogyorodi, 2024; Souza et al., 2017, p. 3; Firesmith, 2015, pp. 46-47; Sakamoto et al., 2013, p. 344; implied by IEEE, 2022, pp. 2-4, 6-9)
              % Integration: Level (IEEE, 2022, pp. 12, 20-22, 26-27; 2021, p. 6; Washizaki, 2024, p. 5-7; Hamburg and Mogyorodi, 2024; Sakamoto et al., 2013, p. 343; Peters and Pedrycz, 2000, Tab. 12.3; van Vliet, 2000, p. 438; implied by Barbosa et al., 2006, p. 3)
              %           OR Technique (implied by Sharma et al., 2021, pp. 601, 603, 605-606)
        \item Checklist-based reviews \citepISTQB{}
              % Checklist-based: Practice (IEEE, 2022, p. 34), Technique (Hamburg and Mogyorodi, 2024)
              % Reviews: Approach
        \item Closed-loop HiL verification \citep[p.~6]{PreußeEtAl2012}
              % Closed Loop: Technique?
              % HiL: Out of Scope (hardware)
        \item Closed-loop protection system testing \citep[p.~331]{ForsythEtAl2004}
              % Closed Loop: Technique?
              % System: Level (IEEE, 2022, pp. 12, 20-22, 26-27; 2021, p. 6; 2017, p. 467; 2016, p. 4; Washizaki, 2024, p. 5-7; Hamburg and Mogyorodi, 2024; Sakamoto et al., 2013, p. 343; Peters and Pedrycz, 2000, Tab. 12.3; van Vliet, 2000, p. 439; implied by Barbosa et al., 2006, p. 3; Gerrard, 2000a, p. 13)
        \item Endurance stability testing \citep[p.~55]{Firesmith2015}
              % Endurance: Type (IEEE, 2013, p. 2; implied by Firesmith, 2015, p. 55)
              %         OR Technique (IEEE, 2021, p. 38)
              % Stability: Type (implied by its quality (IEEE, 2017, p. 434; OG ISO/IEC, 2009) and Firesmith, 2015, p. 55)
        \item End-to-end functionality testing (\citealp[p.~20]{IEEE2021}; \citealp[Tab.~2]{Gerrard2000a})
              % End-to-end: Type (Hamburg and Mogyorodi, 2024)
              %          OR Technique (Firesmith, 2015, p. 47; Sharma et al., 2021, pp. 601, 603, 605-606)
              % Functionality: Type (implied by its quality (IEEE, 2017, p. 196); Firesmith, 2015, p. 53)
        \item Formal reviews \citepISTQB{}
              % Formal: Technique (inferred from informal testing)
              % Reviews: Approach
        \item Grey-box integration testing \citep[p.~344]{SakamotoEtAl2013}
              % Grey-Box: Technique (IEEE, 2021, p. 8; Firesmith, 2015, pp. 46, 48; Sakamoto et al., 2013, p. 344)
              % Integration: Level (IEEE, 2022, pp. 12, 20-22, 26-27; 2021, p. 6; Washizaki, 2024, p. 5-7; Hamburg and Mogyorodi, 2024; Sakamoto et al., 2013, p. 343; Peters and Pedrycz, 2000, Tab. 12.3; van Vliet, 2000, p. 438; implied by Barbosa et al., 2006, p. 3)
              %           OR Technique (implied by Sharma et al., 2021, pp. 601, 603, 605-606)
        \item Incremental integration testing \citep[pp.~601, 603, 605\==606]{SharmaEtAl2021}\todo{OG [19]}
              % Incremental: Practice?
              % Integration: Level (IEEE, 2022, pp. 12, 20-22, 26-27; 2021, p. 6; Washizaki, 2024, p. 5-7; Hamburg and Mogyorodi, 2024; Sakamoto et al., 2013, p. 343; Peters and Pedrycz, 2000, Tab. 12.3; van Vliet, 2000, p. 438; implied by Barbosa et al., 2006, p. 3)
              %           OR Technique (implied by Sharma et al., 2021, pp. 601, 603, 605-606)
        \item Informal reviews \citepISTQB{}
              % Informal: Technique (implied by Kam, 2008, p. 6)
              % Reviews: Approach
        \item Infrastructure compatibility testing \citep[p.~53]{Firesmith2015}
              % Infrastructure: Type (implied by Firesmith, 2015, p. 57)
              %              OR Level (implied by Gerrard, 2000a, p. 13; see \Cref{tab:ieeeCats})
              % Compatibility: Type (IEEE, 2022, pp. 3, 22; 2021, p. 37, Tab. A.1; 2013, p. 2; implied by its quality (ISO/IEC, 2023a); Firesmith, 2015, p. 53)
        \item Invariant-based automatic testing \citep[pp.~184\==185, Tab.~21]{DoğanEtAl2014},
              including for ``AJAX user interfaces'' \citetext{p.~191}
              % Assertion Checking: Practice?
              % Automated: Practice (IEEE, 2022, pp. 20, 22)
              %         OR Technique (implied by p. 35)
        \item Legacy system integration (testing) \citep[Tab.~2]{Gerrard2000a}
              % Legacy: Approach
              % System Integration: Level (IEEE, 2022, pp. 12, 22; 2021, p. 6; Hamburg and Mogyorodi, 2024)
        \item Manual procedure testing \citep[p.~47]{Firesmith2015}
              % Manual: Practice (IEEE, 2022, p. 22)
              %      OR Technique (implied by p. 35)
              % Procedure: Type (IEEE, 2022, pp. 7, 22; 2021, p. 39, Tab. A.1; 2017, p. 337; OG IEEE, 2013)
              %         OR Technique (implied by Firesmith, 2015, p. 47)
        \item Manual security audits \citep[p.~28]{Gerrard2000b}
              % Manual: Practice (IEEE, 2022, p. 22)
              %      OR Technique (implied by p. 35)
              % Security Audits: Technique (IEEE, 2021, p. 40)
              %               OR Type (inferred from security testing)
        \item Model-based GUI testing (\citealp[Tab.~1]{DoğanEtAl2014}; implied by \citealp[p.~356]{SakamotoEtAl2013})
              % Model-based: Practice (IEEE, 2022, p. 22; 2021, p. viii)
              %           OR Technique (Engström and Petersen, 2015, pp. 1-2; Kam, 2008, p. 4; implied by IEEE, 2022, p. 32; 2021, p. 7; 2017, p. 469)
              % GUI: Approach
        \item Model-based web application testing (implied by \citealp[p.~356]{SakamotoEtAl2013})
              % Model-based: Practice (IEEE, 2022, p. 22; 2021, p. viii)
              %           OR Technique (Engström and Petersen, 2015, pp. 1-2; Kam, 2008, p. 4; implied by IEEE, 2022, p. 32; 2021, p. 7; 2017, p. 469)
              % Web Application: Approach
        \item Non-functional search-based testing \citep[Tab.~1]{DoğanEtAl2014}
              % Non-functional: Approach
              % Search-based: Technique (Engström and Petersen, 2015, pp. 1-2)
        \item Offline MBT \citepISTQB{}
              % Offline: Practice?
              % Model-based: Practice (IEEE, 2022, p. 22; 2021, p. viii)
              %           OR Technique (Engström and Petersen, 2015, pp. 1-2; Kam, 2008, p. 4; implied by IEEE, 2022, p. 32; 2021, p. 7; 2017, p. 469)
        \item Online MBT \citepISTQB{}
              % Online: Practice?
              % Model-based: Practice (IEEE, 2022, p. 22; 2021, p. viii)
              %           OR Technique (Engström and Petersen, 2015, pp. 1-2; Kam, 2008, p. 4; implied by IEEE, 2022, p. 32; 2021, p. 7; 2017, p. 469)
        \item Role-based reviews \citepISTQB{}
              % Role-based: Practice?
              % Reviews: Approach
        \item Scenario walkthroughs \citep[Fig.~4]{Gerrard2000a}
              % Scenario: Technique (IEEE, 2022, pp. 9, 22; 2021, pp. 5, 8, 20, Fig. 2; 2017, p. 400; OG 2013; Washizaki, 2024, p. 5-12; Firesmith, 2015, p. 47; Sangwan and LaPlante, 2006, p. 26)
              % Walkthroughs: Technique (IEEE, 2017, p. 508)
        \item Scenario-based reviews \citepISTQB{}
              % Scenario-based: Approach
              % Reviews: Approach
        \item Security attacks \citepISTQB{}
              % Security: Type (IEEE, 2022, pp. 9, 22, 26-27; 2021, pp. 7, 40, Tab. A.1; 2017, p. 405; OG 2013; implied by its quality (ISO/IEC, 2023a; Washizaki, 2024, p. 13-4); Firesmith, 2015, p. 53)
              % Attacks: Practice (IEEE, 2022, p. 34)
              %       OR Technique (implied by Hamburg and Mogyorodi, 2024)
        \item Security audits (\citealp[p.~40]{IEEE2021}; \citealp[p.~28]{Gerrard2000b})
              % Security: Type (IEEE, 2022, pp. 9, 22, 26-27; 2021, pp. 7, 40, Tab. A.1; 2017, p. 405; OG 2013; implied by its quality (ISO/IEC, 2023a; Washizaki, 2024, p. 13-4); Firesmith, 2015, p. 53)
              % Audits: Practice?
        \item Statistical web testing \citep[p.~185]{DoğanEtAl2014}
              % Statistical: Technique (Kam, 2008, pp. 23, 48)      
              % Web Application: Approach
        \item Usability test script(ing) \citepISTQB{}
              % Usability: Type (IEEE, 2022, pp. 22, 26-27; 2021, pp. 7, 40, Tab. A.1; implied by its quality; Firesmith, 2015, p. 53)
              % Scripted: Practice (IEEE, 2022, pp. 20, 22; implied by p. 33)
        \item Web application regression testing \cite[Tab.~21]{DoğanEtAl2014}
              % Web Application: Approach
              % Regression: Technique (implied by IEEE, 2022, p. 35)
              %          OR Level (implied by Barbosa et al., 2006, p. 3)
        \item White-box unit testing \citep[pp.~345\==346]{SakamotoEtAl2013}
              % Structure-based: Technique (IEEE, 2022, p. 22; 2021, p. 8; Washizaki, 2024, pp. 5-10, 5-13; Hamburg and Mogyorodi, 2024; Firesmith, 2015, pp. 46, 49; Sakamoto et al., 2013, p. 344; implied by IEEE, 2022, pp. 2, 4, 6, 9; Barbosa et al., 2006, p. 3)
              % Unit: Level (IEEE, 2022, pp. 12, 20-22, 26-27; 2021, p. 6; 2017, p. 467; 2016, p. 4; Washizaki, 2024, p. 5-6; Hamburg and Mogyorodi, 2024; Sakamoto et al., 2013, p. 343; Peters and Pedrycz, 2000, Tab. 12.3; van Vliet, 2000, p. 438; implied by Barbosa et al., 2006, p. 3)
              %    OR Technique (implied by Engström and Petersen, 2015, pp. 1-2)
    \end{enumerate}

    There are some cases where the subapproaches of the ``compound'' approaches
    above are \emph{not} from separate categories. However, these cases can be
    explained by insufficient data or by edge cases that require special care.
    While we assume that the categories given in \Cref{tab:ieeeCats} are
    orthogonal, further analysis may disprove this. For now, all of these special
    cases are affected by at least one of the following conditions:
    \begin{enumerate}
        \item \textbf{At least one subapproach is categorized inconsistently.}
              When a subapproach has more than one category (see \Cref{multiCats}),
              it is unclear which one should be used to assess orthogonality.
        \item \textbf{At least one subapproach's category is inferred.} When the category
              of a test approach is not given by the literature but is inferred
              from related context (see \Cref{infers}), it is unclear if it can
              be used to assess orthogonality.
        \item \textbf{At least one subapproach is only categorized as an approach.}
              Since ``approach'' is a catch-all categorization, it does not
              need to be orthogonal to its subcategories.
        \item \textbf{A subapproach is explicitly based on another in the same
                  category.} An example of this is stability testing, which
              tests a ``property that an object has with respect to a given
              failure mode if it cannot exhibit that failure mode''
              \citep[p.~434\todo{OG ISO/IEC, 2009}]{IEEE2017}. This notion of
              ``property'' is similar to that of ``quality'' that the test type
              category is built on, so it is acceptable that is implied to be
              a test type by its quality \citep[p.~434]{IEEE2017}%
              \todo{OG ISO/IEC, 2009} and by \citet[p.~55]{Firesmith2015}.
    \end{enumerate}
\fi

In addition to the categories of approach, level, type, technique, and
practice, \citet[Fig.~2]{IEEE2022} also \multiAuthHelper{include}
``static testing'', which seems to be non-orthogonal to the others and
thus less helpful for grouping test approaches.
For example, static assertion checking (mentioned by \ifnotpaper
    \citealp[p.~345]{LahiriEtAl2013}; \citealp[p.~343]{ChalinEtAl2006}\else
    \citealp[p.~343]{ChalinEtAl2006}, \citealp[p.~345]{LahiriEtAl2013}\fi) is a
subapproach of assertion checking, which can also be performed dynamically.
This parent-child relation (defined in \Cref{par-chd-rels}) means that static
assertion checking may inherit assertion checking's inferred category of
``practice''. Based on observations such as this, we categorize testing
approaches, \emph{including} static ones, based on the remaining categories
from \citet{IEEE2022}.
\ifnotpaper However, since there are many ways to categorize test approaches
    (see \Cref{tab:otherCats,tab:otherCategorizations}), considering static
    testing as an orthogonal distinction could make sense in specific contexts
    (see \Cref{static-test}).

\fi
While we can categorize the vast majority of identified test approaches
based on \citet{IEEE2022}'s categories, there are some outliers. For example,
we categorize some test approaches as ``artifacts''\thesisissueref{44,119,39},
since some terms can refer to the application of a
test approach \emph{as well as} the resulting document(s). Therefore, we
do \emph{not} consider these cases to violate our assumption that
\citet{IEEE2022}'s categories are orthogonal and do not include them as flaws%
\thesisissueref{119} in \Cref{multiCats}.
\ifnotpaper
    Additionally, we identify ``test metrics''\thesisissueref{21,22} that
    describe methods for \emph{evaluating} testing as opposed to methods for
    \emph{performing} it and are therefore out-of-scope. Instead,
    we capture the related test approaches that seek to maximize these
    metrics as subsets of coverage-driven testing (see \Cref{cov-test}) and
    experience-based testing \citep[p.~34]{IEEE2022}.
\fi

A side effect of using the terms ``level'', ``type'', ``technique'', and
``practice'' is that they---perhaps excluding ``level''---can
be used interchangably or as synonyms for ``approach''. For example,
\citet[p.~88]{Patton2006} says that if a specific defect is found, it is wise
to look for other defects in the same location and for similar defects in other
locations, but does not provide a name for this approach. After researching in
vain, we ask ChatGPT\footnote{We do \emph{not} take ChatGPT's output to be
    true at face value; this approach seems to be called ``defect-based
    testing'' based on the principle of ``defect clustering''
    \citep{ChatGPT2024}, which \citet{RusEtAl2008} \multiAuthHelper{support}.}
to name the ``\emph{type} of software testing that focuses on looking for bugs
where others have already been found''
\ifnotpaper \citep[emphasis added]{ChatGPT2024}\else \cite{ChatGPT2024}\fi%
\qtodo{Is this sufficient to explain ChatGPT's role in our methodology?},
using the word ``type'' in a general sense, akin to ``kind'' or ``subset''.
Interestingly, ChatGPT ``corrects'' our imprecise term in its response,
using the more correct term ``approach'' (although it may have been biased by
our previous usage of these terms)! Because natural language can be ambiguous,
we need to exercise judgement when determining if these terms are being used
in a general or technical sense. For example,
\citet[p.~45\ifnotpaper, emphasis added\fi]{Kam2008}
defines interface testing as ``an integration \emph{test type} that is
concerned with testing \dots{} interfaces'', but since \ifnotpaper he \else it
\fi does not define ``test type'', this may not have special significance.%
\ifnotpaper\afterpage{\begin{landscape}%
            % Omitted from paper for brevity
            \otherCatsTable{}%
        \end{landscape}} We consider these kinds of ``categorizations'' to be
    inferred and mark them with a question mark (?)~during data collection.
    When we evaluate test approaches categorized more than once in
    \Cref{multiCats}, we list well-defined categorizations in
    \Cref{tab:multiCats} and inferred ones in \Cref{tab:infMultiCats}.

    Other sources \citep[such as][]{SWEBOK2024,BarbosaEtAl2006}
    propose similar yet distinct categories that clash or overlap with those
    from \citep{IEEE2022}, given in \Cref{tab:otherCats}. These could simply map
    to their ``IEEE Equivalents'' or could provide new perspectives and be
    useful in some contexts, either in place of or in tandem with
    \citet{IEEE2022}'s.

    Similarly, we find many other ways to categorize test approaches in the
    literature which may be used in tandem with or in place of those given
    in \Cref{tab:ieeeCats,tab:otherCats}. In general, these are defined less
    systematically but are more fine-grained, seeming to ``specialize''
    categories from \Cref{tab:ieeeCats}. The existence of these categorizations
    is not inherently wrong, as they may be useful for specific teams or in
    certain contexts. For example, functional testing and structural testing
    ``use different sources of information and have been shown to highlight
    different problems'', and deterministic testing and random testing have
    ``conditions that make one approach more effective than the other''
    \citep[p.~5\=/16]{SWEBOK2024}. Unfortunately, even these alternate
    categories are not used consistently (see \flawref{manual-or-keyword})!

    We include the most prominent of these alternate categorizations in
    \Cref{tab:otherCategorizations} for completeness. In each row, the source
    given lists the example approaches and categorizes them as the given
    ``Parent IEEE Category'' unless stated otherwise (in some cases, the source
    gives additional approaches that we omit for brevity). For example, in the
    first row, \citet[pp.~22, 35]{IEEE2022}
    categorize both manual testing and automated testing as test practices.
    Note that since ``approach'' is a catch-all category, it does not require
    an explicit categorization, and that these categorizations may be
    flawed, as stated in the provided footnotes.

    \afterpage{
        \begin{landscape}
            % Omitted from paper for brevity
            \otherCategorizationsTable{}
        \end{landscape}
    }

    \phantomsection{}\label{method-family}
    Another way to subdivide the IEEE categories is by grouping related test
    approaches into a ``class'' or ``family'' with ``commonalities and
    well-identified variabilities that can be
    instantiated'', where ``the commonalities are large and the variabilities
    smaller'' \citep{classFamilyDisc}. Examples of these are the classes of
    combinatorial \citep[p.~15]{IEEE2021} and data flow testing \citetext{p.~3}
    and the family of performance-related testing \perfAsFamily{}; we explore
    the notion of test approach families in more detail in
    \Cref{classFamilyFlaw}. Note that ``there is a lot of overlap between
    different classes of testing'' \citep[p.~8]{Firesmith2015}, meaning that ``one
    category [of test techniques] might deal with combining two or more techniques''
    \citep[p.~5-10]{SWEBOK2024}. For example, ``performance, load and stress
    testing might considerably overlap in many areas'' \citep[p.~1187]{Moghadam2019}.
    A side effect of this is that it is difficult to ``untangle'' these classes;
    for example, take the following sentence: ``whitebox fuzzing extends dynamic
    test generation based on symbolic execution and constraint solving from unit
    testing to whole-application security testing''
    \citep[p.~23]{GodefroidAndLuchaup2011}! This is, in part, why research on
    software testing terminology is so vital.\qtodo{Is this sentence needed?}
\fi

\subsection{Synonym Relations}
\label{syn-rels}

The same approach often has many names. For example,
\emph{specification-based testing} is also called\todo{more in Umar2000}:
\begin{enumerate}
    \item \emph{black-box testing}
          \ifnotpaper
              (\citealp[p.~9]{IEEE2022}; \citeyear[p.~8]{IEEE2021};
              \citeyear[p.~431]{IEEE2017}; \citealp[p.~5\=/10]{SWEBOK2024};
              \citealpISTQB{}; \citealp[p.~46 (without hyphen)]{Firesmith2015};
              \citealp[p.~344]{SakamotoEtAl2013}; \citealp[p.~399]{vanVliet2000})
          \else
              \cite[p.~9]{IEEE2022}, \cite{ISTQB}, \cite[p.~431]{IEEE2017},
              \cite[p.~5\=/10]{SWEBOK2024}, \cite[p.~8]{IEEE2021},
              % \cite[p.~46 (without hyphen)]{Firesmith2015},
              \cite[p.~399]{vanVliet2000},
              \cite[p.~344]{SakamotoEtAl2013}
          \fi
    \item \emph{closed-box testing}
          \ifnotpaper
              (\citealp[p.~9]{IEEE2022}; \citeyear[p.~431]{IEEE2017})
          \else
              \cite[p.~9]{IEEE2022}, \cite[p.~431]{IEEE2017}
          \fi
    \item \emph{functional testing}\footnote{``Functional testing'' may not
              \emph{actually} be a synonym for ``specification-based testing'';
              see \Cref{spec-func-test}.}
          \ifnotpaper
              (\citealp[p.~196]{IEEE2017}; \citealp[p.~44]{Kam2008};
              \citealp[p.~399]{vanVliet2000}; implied by
              \citealp[p.~129]{IEEE2021}; \citeyear[p.~431]{IEEE2017})
          \else
              \cite[p.~196]{IEEE2017}, \cite[p.~399]{vanVliet2000},
              \cite[p.~44]{Kam2008} (implied by \cite[p.~431]{IEEE2017},
              \cite[p.~129]{IEEE2021})
          \fi
    \item \emph{domain testing} \citep[p.~5\=/10]{SWEBOK2024}
    \item \emph{specification-oriented testing} \citep[p.~440, Fig.~12.2]{PetersAndPedrycz2000}
    \item \emph{input domain-based testing} (implied by \citealp[pp.~4\=/7 to
              4\=/8]{SWEBOK2014})
\end{enumerate}
Throughout our work, we use the terms
``specification-based testing'' and ``structure-based testing'' to articulate
the source of the information for designing test cases, but a team or project
also using grey-box testing may prefer the terms ``black-box'' and ``white-box
testing'' for consistency. Thus, synonyms are not inherently problematic,
although they can be (see \Cref{syns}).

We can formally define the synonym relation $S$ on the set $T$ of terms used by
the literature to describe test approaches based on how synonyms are used in
natural language. $S$ is symmetric; for example, since ``windy'' is a synonym
of ``gusty'' \citep{WindySyns}, the inverse is also true \citeyearpar{GustySyns}.
$S$ is also transitive: since ``windy'' is a synonym of ``blustery''
\citeyearpar{WindySyns} and ``gusty'' is a synonym of ``windy'', ``gusty''
is transitively a synonym of ``blustery'' \citeyearpar{GustySyns}. Although
pairs of synonyms in natural language are implied to be distinct
\citeyearpar{WindySyns,GustySyns}, a relation that is symmetric and transitive
is provably reflexive, so we extend $S$ to be reflexive as well, implying that
all terms are trivially synonyms of themselves. Since $S$ is symmetric,
transitive, and reflexive, it is therefore an equivalence relation, which
reflects the role of synonyms in natural language. While synonyms may
emphasize different aspects or express mild variations---``gust'' is defined as
``a sudden brief rush of wind'' \citeyearpar{GustDef}---their core meaning is
nevertheless the same.

Synonym relations are often given explicitly in the literature. For example,
\citet[p.~9]{IEEE2022} \multiAuthHelper{list} ``black-box testing'' and
``closed box testing'' beneath the glossary entry for ``specification-based
testing'', meaning they are synonyms. ``Black-box testing'' is likewise given
under ``functional testing'' in \citeyearpar[p.~196]{IEEE2017}, meaning it is
also a synonym for ``specification-based testing'' through transitivity%
\qtodo{Is this clear/correct? Should I explain this more?}.
However, these relations can also be less ``rigid'' (see \Cref{rigidity});
``functional testing'' is listed in a \emph{cf.} footnote to the glossary entry
for ``specification-based testing'' \citeyearpar[p.~431]{IEEE2017}, which
supports the previous claim but would not necessarily indicate a synonym
relation on its own.

Similarly, \citet[p.~5-10]{SWEBOK2024} says ``\emph{specification-based
    techniques} \dots{} [are] sometimes also called domain
testing techniques'' in the \acs{swebok} V4, from which the synonym of
``domain testing'' follows logically. However, its predecessor V3 only
\emph{implies} the more specific ``input domain-based testing'' as a synonym.
The section on test techniques says ``the classification of testing techniques
presented here is based on how tests are generated: from the software
engineer's intuition and experience, the specifications, the code structure
\dots'' \citep[p.~4\=/7]{SWEBOK2014}, and the first three subsections on the
following page are ``Based on the Software Engineer's Intuition and
Experience'', ``Input Domain-Based Techniques'', and ``Code-Based Techniques''
\citetext{p.~4\=/8}. The order of the introductory list lines up with these
sections, implying that ``input domain-based techniques'' are ``generated[]
from \dots{} the specifications'' (i.e., that input domain-based testing is the
same as specification-based testing). Furthermore, the examples of input
domain-based techniques given---equivalence partitioning, pairwise testing,
boundary-value analysis, and random testing---are all given as children%
\footnote{
    Pairwise testing is given as a child of combinatorial testing, which is
    itself a child of specification-based testing, by \ifnotpaper
        \citep[Fig.~2]{IEEE2021} and \citep[pp.~5\=/11 to 5\=/12]{SWEBOK2024}%
    \else
        \cite[pp.~5\=/11 to 5\=/12]{SWEBOK2024} and \cite[Fig.~2]{IEEE2021}%
    \fi, making it a ``grandchild'' of specification-based testing according to
    these sources.
} of specification-based testing \ifnotpaper
    (\citealp{IEEE2022}; \citeyear[Fig.~2]{IEEE2021}; \citealpISTQB{})\else
    \cite{IEEE2022,ISTQB}, \cite[Fig.~2]{IEEE2021}\fi; even V4 agrees with
this \citep[pp.~5\=/11 to 5\=/12]{SWEBOK2024}!

\subsection{Parent-Child Relations}
\label{par-chd-rels}
Many test approaches are multi-faceted and can be ``specialized'' into others;
for example, there are many subtypes of performance-related testing,
such as load testing and stress testing (see \Cref{perf-test-rec}). These
``specializations'' will be referred to as ``children'' or ``subapproaches''
of the multi-faceted ``parent''. This nomenclature also extends to other
categories (such as ``subtype''; see \Cref{cats-def,tab:ieeeCats})
and software qualities (``subquality''\ifnotpaper; see \Cref{qual-test}\fi).
There are many reasons two approaches may have a parent-child relation, such as:
\begin{enumerate}
    \item \textbf{One is a superset of the other.} In other words, for one
          (parent) test approach to be performed in its entirety, the other
          (child) approach will necessarily be performed as well. This is often
          the case when one ``well-understood'' subset of testing can be
          decomposed into smaller, independently performable approaches.
          When all of these have been completed, we can logically conclude that
          the parent approach has also been performed! In practice, this is
          much harder to prove; although many hierarchies exist \ifnotpaper
              (\citealp[Fig.~2]{IEEE2022}; \citeyear[Fig.~2]{IEEE2021};
              \citealp{Firesmith2015})\else \cite{Firesmith2015},
              \cite[Fig.~2]{IEEE2022}, \cite[Fig.~2]{IEEE2021}\fi, these are
          likely incomplete. As an example, we graph the parent-child relations
          from \ifnotpaper (\citealp[Fig.~2]{IEEE2022}; \citeyear[Fig.~2]{IEEE2021})
          \else \cite[Fig.~2]{IEEE2022}, \cite[Fig.~2]{IEEE2021} \fi in the
          subdomain of specification-based testing in \Cref{fig:specBasedGraph}
          (along with relevant data from other sources).
    \item \textbf{One is ``stronger than'' or ``subsumes'' the other.} When
          comparing adequacy criteria that ``specif[y] requirements for
          testing'' \citep[p.~402]{vanVliet2000}, ``criterion X is stronger
          than criterion Y if, for all programs P and all test sets T,
          X-adequacy implies Y-adequacy'' \citetext{p.~432}. While this
          relation only ``compares the thoroughness of test techniques, not
          their ability to detect faults'' \citetext{p.~434}, it is sufficient
          to consider one a child of the other in a sense. We capture this
          nuance by considering these parent-child relations implicit
          (see \Cref{rigidity}). As an example, we graph the parent-child
          relations from \ifnotpaper (\citealp[Fig.~F.1]{IEEE2021};
              \citealp[Fig.~13.17]{vanVliet2000}) \else
              \citetext{Fig.~13.17}, \cite[Fig.~F.1]{IEEE2021}
          \fi in the subdomain of data flow testing can be found in
          \Cref{fig:subsumesGraph} (along with relevant data from other sources).
    \item \textbf{The parent approach is part of an orthogonal set.}
          When presented with a set of generic test approaches that are
          orthogonal to each other, it is often trivial to classify a given
          test approach as a child of just one them. For example,
          \citeauthor{IEEE2022} say that ``testing can take two forms: static
          and dynamic'' \citeyearpar[p.~17]{IEEE2022} and provide examples of
          subapproaches of static and dynamic testing \citetext{Fig.~1}.
          Likewise, \citeauthor{Gerrard2000a} says ``tests can be automated or
          manual'' \citeyearpar[p.~13]{Gerrard2000a} and gives subapproaches of
          automated and manual testing \ifnotpaper
              \citeyearpar[Tab.~2; ][Tab.~1]{Gerrard2000b}\else
              \citetext{Tab.~2}, \cite[Tab.~1]{Gerrard2000b}\fi. However, the
          orthogonality of these subsets does not mean they are mutually
          exclusive; in these same tables, \citeauthor{Gerrard2000a} labels
          usability testing as both static \emph{and} dynamic and 12 approaches
          as able to ``be done manually \emph{or} using a tool''
          \citeyearpar[p.~13\ifnotpaper, emphasis added\fi]{Gerrard2000a}.
\end{enumerate}

\parChdGraphs{}
