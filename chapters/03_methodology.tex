\section{Methodology}\label{methodology}

Since we wish to analyze the software testing literature, we collect data (both
correct and incorrect) from a wide variety of relevant documents, focusing on
test approaches and supporting information. This results in a big
glossary of software approaches, some glossaries of supplementary terms, and a
list of flaws. To ensure this data can be analyzed and expanded thoroughly and
consistently, we need a process that can be repeated for future developments in
the field of software testing or by independent researchers seeking to verify
our work. Our methodology for documenting the current (messy) state of software
testing terminology consists of:

\input{build/methodOverview}

\subsection{Identifying Sources}\label{ident-sources}
As there is no single authoritative source on software testing terminology,
we need to look at many sources to observe how this terminology is used in
practice. Since we are particularly interested in software engineering, we
start from the vocabulary document for systems and software engineering%
\qtodo{Better name for this?}
\citep{IEEE2017} and two versions of the \acf{swebok}---the newest
one \citep{SWEBOK2014} and one submitted for public review%
\footnote{%
    \refHelper \citep{SWEBOK2024} has been published since we investigated
    these sources; if time permits, we will revisit this published version.
}
\citep{SWEBOK2024}. To gather further sources, we then use a version of
``snowball sampling'',
which ``is commonly used to locate hidden populations \dots{} [via] referrals
from initially sampled respondents to other persons'' \citep{Johnson2014}. We
apply this concept to ``referrals'' between sources. \addTextEx{} We group all
sources into the source tiers we define in \Cref{source-tiers}\listAllSrcs{}.

\subsection{Identifying Relevant Terms}\label{ident-terms}
Before we can consistently track software testing terminology used in the
literature, we must first determine what to record. We use heuristics to guide
this process to increase confidence that we identify all relevant terms, paying
special attention to the following when investigating a new source:
\begin{itemize}
    \item glossaries, taxonomies, hierarchies, and lists of terms,
    \item testing-related terms (e.g., terms containing ``test(ing)'',
          \ifnotpaper ``review(s)'', ``audit(s)'', ``attack(s)''%
              \thesisissueref{55}, \fi ``validation'', or ``verification''),
    \item terms that had emerged as part of already-discovered
          test approaches, \emph{especially} those that were ambiguous
          or prompted further discussion (e.g., terms containing
          ``performance'', ``recovery'', ``component'', ``bottom-up'',
          \ifnotpaper ``boundary'', \fi or ``configuration''), and
    \item terms that imply test approaches\ifnotpaper\footnote{
                  Since these methods for deriving test approaches only arose
                  as research progressed, some examples would have been missed
                  during the first pass(es) of resources investigated earlier
                  in the process. While reiterating over them would be ideal,
                  this may not be possible due to time constraints.
              }% (see \Cref{derived-tests})
          \fi, including:
          \newcommand\derivTest[2]{#1 that may imply \ifnotpaper related \fi test #2}
          \begin{itemize}
              \item \derivTest{coverage metrics}{techniques}%\ifnotpaper
                    % (\citealp[p.~11]{IEEE2022}; \citeyear[p.~5]{IEEE2021a})\else
                    % \cite[p.~11]{IEEE2022}, \cite[p.~5]{IEEE2021a}\fi
                    ,
              \item \phantomsection{}\label{qual-types}
                    \derivTest{software qualities}{types}, and
              \item \phantomsection{}\label{req-apps}
                    \derivTest{\ifnotpaper software \fi requirements}{approaches}.
          \end{itemize}
\end{itemize}
We apply these heuristics to the entirety of most investigated sources,
especially established standards (see \Cref{stds}). However, we only partially
investigate some sources, such as those chosen for a specific area of
interest or based on a test approach that was determined to be out-of-scope.
These include the following sources as described in \Cref{undef-terms}:
\citet{ISO2022,ISO2015,Dominguez-PumarEtAl2020,PierreEtAl2017,
    TrudnowskiEtAl2017,YuEtAl2011,Tsui2007,Goralski1999}.

%% Omitting this since it describes future work outside of this thesis
% During the first pass of data collection, we investigate and record all
% approaches to software testing. Some of these approaches are less
% applicable to our original motivation of test case automation \ifnotpaper
%     (such as static testing; see \Cref{static-test}\thesisissueref{39}) \fi or
% are quite broad\ifnotpaper\ (such as attacks%
%     %; see \Cref{attacks}
%     \thesisissueref{55})\fi, so we will omit them during future analysis.

\subsection{Recording Relevant Terms}\label{record-terms}

\ifnotpaper
    \input{assets/graphs/recAppFlowchart}
\fi

Once we have identified which terms from the literature are relevant, we can
then track them consistently. We do this by building glossaries, mainly
focusing on the one most central to our research---\ourApproachGlossary{}---where
we record all test approaches we identify in the literature. We give each test
approach its own row where we record its name and any given \approachFields*{}
(along with any other notes, such as questions, prerequisites, and other
resources to investigate) following the procedure in \Cref{fig:recAppFlowchart}.
Note that only the name and category fields are required; all other fields
may be left blank, although a lack of definition indicates that the approach
should be investigated further to see if its inclusion is meaningful (see
\Cref{undef-terms}). When recording flawed data, we record it in our glossary as
dubious information (see \Cref{\ifnotpaper imp-case-four\else imp-case-three\fi})
and/or in a separate document if it is clearly flawed.
% If no category is given, the ``approach'' category is assigned
% (with no accompanying citation) as a ``catch-all'' category. 
% Any additional information from other sources is added to
% or merged with the existing information in our glossary where appropriate.
% This includes the generic ``approach'' category being replaced with a more
% specific one, an additional synonym being mentioned, or another source
% describing an already-documented parent-child relation. If any new information
% does not agree with existing information or indicates a potential flaw, â€¦
% When new information does not conflict with existing information, we either
% keep the clearest and most concise version or merge them to paint a more
% complete picture.

\begin{figure}[bt!]
    \includegraphics[width=\linewidth]{assets/images/a-b testing.png}
    \caption{\refHelper \citet[p.~1]{IEEE2022}'s glossary entry for
        ``A/B testing''.}\label{fig:IEEE-A-B-Testing}
\end{figure}

For example, when we first encounter ``A/B Testing'' in
\citep[p.~1, 36]{IEEE2022} as shown in \Cref{fig:IEEE-A-B-Testing}, we apply
our procedure as follows:\qtodo{Should glossary headers be capitalized?}
\begin{enumerate}
    \item Create a new row with the name ``A/B testing'' and the category
          ``Approach''.
    \item Record the synonym ``Split-Run Testing''.
    \item Record the parent ``Statistical Testing''.
    \item Record the definition ``Testing `that allows testers to determine
          which of two systems or components performs better'\,''; note that
          we abstract away information that we have previously captured (i.e.,
          its synonym and parent).
\end{enumerate}
\ifnotpaper\newpage\noindent\fi
Note that we accompany all of these entries (excluding its name) with the
citation ``\citep[pp.~1, 36]{IEEE2022}''. This document provides more
information which we capture as follows:
\begin{enumerate}
    \item Record the note ``It `can be time-consuming, although tools can be
          used to support it', `is a means of solving the test oracle problem
          by using the existing system as a partial oracle', and is `not a test
          case generation technique as test inputs are not generated'\,''
          \citetext{p.~36}.
    \item Replace the category of ``Approach'' with the more specific
          ``Practice'' \citetext{Fig.~2}; note that this
          is consistent with the exclusion of ``Technique'' as a possible
          category for this approach from page~36.
\end{enumerate}
As we investigate other sources, we learn more about this approach. \ifnotpaper
\else \citeauthor{Firesmith2015} \fi \citet[p.~58]{Firesmith2015} includes it
in his taxonomy as shown in \Cref{fig:Firesmith-A-B-Testing}. We add to our
entry for ``A/B Testing'' as follows:

\begin{minipage}{0.45\linewidth}
    \vspace{0.5cm}
    \includegraphics[width=\linewidth]{assets/images/a-b testing 2.png}
    \captionof{figure}{A/B testing's inclusion in \ifnotpaper \else
            \citeauthor{Firesmith2015} \fi \citet[p.~58]{Firesmith2015}'s
        taxonomy.}\label{fig:Firesmith-A-B-Testing}
    \vspace{0.5cm}
\end{minipage}
\begin{minipage}{\ifnotpaper 0.53\else 0.5\fi\linewidth}
    \begin{enumerate}
        \item Add the parent ``Usability Testing''.
        \item Since usability testing is a test type \ifnotpaper
                  (\citealp[pp.~22, 26\=/27]{IEEE2022};
                  \citeyear[pp.~7, 40, Tab.~A.1]{IEEE2021b};
                  implied by its quality; \citealp[p.~53]{Firesmith2015})\else
                  \cite[pp.~22, 26\=/27]{IEEE2022},
                  \cite[pp.~7, 40, Tab.~A.1]{IEEE2021b}\fi, add the category
              ``Type'' with the citation ``(inferred from usability testing)''.
    \end{enumerate}
\end{minipage}\qtodo{Is my justification for inferring ``Type'' sufficient?}
This second change introduces an inference \ifnotpaper (defined in \Cref{infers})
\fi that violates our assumption \ifnotpaper from \Cref{orth-approach} \fi that
categories are orthogonal, so we consider this to be an inferred flaw that we
automatically detect and document \ifnotpaper (see \Cref{auto-flaw-analysis,%
        tab:infMultiCats}, respectively). This results in the corresponding row
    in \Cref{tab:approachGlossaryExcerpt}, although we exclude the ``Notes''
    column for brevity\else (details omitted for brevity)\fi.

\phantomsection{}\label{qual-supp-procedure}
We use this same procedure to track software qualities and supplementary
terminology that is either shared by multiple approaches or too complicated to
explain inline. We create a separate glossary for both qualities and
supplementary terms, each with a similar format to \ourApproachGlossary.
Since these terms do not have categories, the process of recording them is much
simpler, only requiring us to record the name, definition, and synonym(s) of
these terms, along with any additional notes. The only new information
we capture is the ``precedence'' for a software quality to have an associated
test type, since each test type measures a particular software quality (see
\Cref{tab:ieeeCats}). These precedences are instances where a given software
quality is related to, is covered by, or is a child, parent, or prerequisite
of another quality with an associated test type, as given by the literature.

Tracking information about software qualities helps us investigate the
literature more thoroughly, since these data may become relevant based on
information from other yet-uninvestigated sources. When the literature mentions
(or implies) a test approach that corresponds to a software quality we have
recorded, we first follow the procedure given in \Cref{fig:recAppFlowchart}
with the information provided in the source that mentions it. We then remove
the relevant data from our quality glossary and repeat our procedure with it
to upgrade the quality to a test type in \ourApproachGlossary{}.%
\qtodo{Does this make sense? I tried rewording it for brevity/clarity}

% For example, analyzability%
% \footnote{This may be spelled ``analyzability'' \citep[p.~18]{IEEE2017} or
%     ``analysability'' \citep{ISO_IEC2023a}; since this is a dialectal
%     difference, we do not count this as a label flaw (described in
%     \Cref{label-flaw-def}).}, modifiability, modularity, reusability, and
% testability are all subqualities of maintainability \ifnotpaper
%     (\citealp{ISO_IEC2023a}; \citealp[Tab.~A.1]{IEEE2021b};
%     \citealp[p.~7\=/10]{SWEBOK2024}) \else
%     \cite[p.~7\=/10]{SWEBOK2024}, \cite[Tab.~A.1]{IEEE2021b},
%     \cite{ISO_IEC2023a} \fi which has an associated test type
% \ifnotpaper
%     (\citealp[pp.~5, 22]{IEEE2022}; \citeyear[p.~38, Tab.~A.1]{IEEE2021b})\else
%     \cite[pp.~5, 22]{IEEE2022}, \cite[p.~38, Tab.~A.1]{IEEE2021b}\fi. This sets
% a ``precedent'' for each of these subqualities having its own associated test
% type (e.g., reusability testing). \ifnotpaper
%     Sometimes, the literature provides a relation between qualities where at
%     least one of them does not have an explicit approach associated with it
%     (although it may be inferred). We record this information regardless, since
%     it may become relevant if an associated test type emerges as alluded to
%     \hyperref[qual-types]{earlier}. For example, \citet{ISO_IEC2023a} provides
%     relations involving dependability and modifiability, but only in terms of
%     them as qualities, not types of testing. We record these relations in case
%     an associated test type is explicitly named by the literature, which would
%     inherit these relations. \fi

% \subsubsection{Derived Test Approaches}\label{derived-tests}

% Throughout this research, we noticed many groups of test approaches that arise
% from some underlying area of software (testing) knowledge. The legitimacy of
% extrapolating new test approaches from these knowledge domains is heavily
% implied by the literature, but not explicitly stated as a general rule. Regardless,
% since the field of software is ever-evolving, it is crucial to be able to
% adapt to, talk about, and understand new developments in software testing.
% Bases for defining new test approaches suggested by the literature include
% coverage metrics, software qualities, and attacks. These are meaningful
% enough to merit analysis and are therefore in scope. Requirements may also
% imply related test approaches, but this mainly results in test approaches
% that would be out of scope. \ifnotpaper Other test approaches found in the
%     literature are derived from programming languages or other orthogonal test
%     approaches, but these are out of scope as this information is better
%     captured by other approaches (see \Cref{lang-test,orth-test}, respectively).

%     \paragraph{Coverage-driven Techniques}\label{cov-test}

%     Test techniques are able to ``identify test coverage items \dots{} and
%     derive corresponding test cases''
%     \ifnotpaper
%         (\citealp[p.~11]{IEEE2022}; similar in \citeyear[p.~467]{IEEE2017})
%     \else
%         \cite[p.~11]{IEEE2022} (similar in \cite[p.~467]{IEEE2017})
%     \fi
%     in a ``systematic'' way
%     \citeyearpar[p.~464]{IEEE2017}.
%     \ifnotpaper
%         This allows for ``the coverage achieved by a specific test design
%         technique'' to be calculated as a percentage of ``the number of test
%         coverage items covered by executed test cases'' \citeyearpar[p.~30]{IEEE2021b}.
%         %     ``Coverage levels can range
%         %     from 0\% to 100\%'' and may or may not include ``infeasible'' test coverage
%         %     items, which are ``not \dots{} executable or [are] impossible to be covered by a
%         %     test case'' \citetext{p.~30}. Perhaps more interestingly, the further
%         %     implication is
%         % \else
%         %     This means
%     \fi % that
%     Therefore, a given coverage metric implies a test approach aimed to
%     maximize it. For example, path testing ``aims to execute all entry-to-exit
%     control flow paths in a \acs{sut}'s control flow graph'' \citep[p.~5-13]{SWEBOK2024},
%     thus maximizing the path coverage
%     \ifnotpaper
%         \citep[see][Fig.~1\thesisissueref{63}]{SharmaEtAl2021}\else
%         (see \cite[Fig.~1]{SharmaEtAl2021}\thesisissueref{63})\fi.

%     \paragraph{Quality-driven Types}\label{qual-test}

%     Since test types are ``focused on specific quality characteristics''
%     \ifnotpaper
%         (\citealp[p.~15]{IEEE2022}; \citeyear[p.~7]{IEEE2021b};
%         \citeyear[p.~473]{IEEE2017}\todo{OG IEEE 2013})%
%     \else
%         \cite[p.~15]{IEEE2022}, \cite[p.~7]{IEEE2021b}, \cite[p.~473]{IEEE2017}%
%     \fi, they can be derived from software qualities: ``capabilit[ies] of
%     software product[s] to satisfy stated and implied needs when used under
%     specified conditions'' \citep[p.~424]{IEEE2017}\todo{OG ISO/IEC 2014}. This
%     is supported by reliability and performance testing, which are both examples of
%     test types \citeyearpar{IEEE2022, IEEE2021b} that are based on their underlying
%     qualities \citep[p.~18]{FentonAndPfleeger1997}.
%     % \ifnotpaper
%     %     For quantifying quality-driven testing, measurements should include
%     %     an entity to be measured, a specific attribute to measure, and the actual
%     %     measure (i.e., units, starting state, ending state, what to include)
%     %     \citetext{p.~36} where attributes must be
%     %     defined before they can be measured \citetext{p.~38}.
%     %
%     % \fi
%     Because of the importance of software qualities to defining test types, we track
%     \qualityCount{} software qualities\thesisissueref{21,23,27} in addition to our
%     tracked test approaches following the procedure in \Cref{qual-supp-procedure}.
%     We then ``upgrade'' software qualities to test types when they are mentioned
%     (or implied) by a source by removing its entry from this quality glossary
%     and adding an associated test approach to \ourApproachGlossary{}, also outlined
%     in \Cref{methodology}. Examples of this include conformance testing \ifnotpaper
%         (\citealp[p.~5\=/7]{SWEBOK2024}; \citealp[p.~25]{JardEtAl1999}; implied
%         by \citealp[p.~93]{IEEE2017})\else \cite[p.~5\=/7]{SWEBOK2024},
%         \cite[p.~25]{JardEtAl1999}\fi, efficiency testing
%     \citep[p.~44]{Kam2008}, and survivability testing \citep[p.~40]{GhoshAndVoas1999}.

%     \paragraph{Attacks}\label{attacks}
%     While attacks can be ``malicious'' \citep[p.~7]{IEEE2017}, they are also
%     described as a test approach \ifnotpaper
%         (\citeyear[pp.~4, 34]{IEEE2022}; \citeyear[p.~4]{IEEE2021b};
%         \citeyear[p.~7]{IEEE2019a}; \citealpISTQB{}; implied by
%         \citealp[p.~5\=/10]{SWEBOK2024}; \citealp[p.~26]{Bas2024};
%         \citealp[p.~87\==89]{Patton2006})\else \cite[pp.~4, 34]{IEEE2022},
%         \cite[p.~4]{IEEE2021b}; \cite[p.~7]{IEEE2019a}; \cite{ISTQB}\fi.
%     This is supported by the fact that penetration testing is also called
%     ``ethical hacking testing'' \citep[p.~13\=/4]{SWEBOK2024} or just ``ethical
%     hacking'' \citep[p.~28; see \flawref{ethical-hacking}]{Gerrard2000b}. This
%     means that software attacks, such as code injection and password cracking
%     \citepISTQB{}, can also be used for testing software if they are performed
%     systematically to test and improve the software (i.e., \emph{without} the
%     malicious intent).

%     \paragraph{Requirements-driven Approaches}\label{req-test}
%     While not as universally applicable, some kinds of requirements have associated
%     types of testing (e.g., functional, non-functional, security). This may mean
%     that others kinds of requirements \emph{also} have associated test approaches;
%     for example, we infer ``technical testing'' from the existence of technical
%     requirements \citep[p.~463]{IEEE2017} and requirements-based testing
%     (see \Cref{infers}). \ifnotpaper Even assuming this is true, some kinds of
%         requirements do not apply to the code itself so their relevant inferred
%         test approaches are out of scope (see \Cref{phys-req-test,nontech-req-test}).\fi
% \fi

\subsection{Implicit Information}\label{imp-info}

As described in \Cref{explicitness}, the use of natural language introduces
significant nuance that we need to document. Keywords such as \impKeywords{}
indicate that information from the literature is \emph{not} explicit. These
keywords often appear directly within the literature, but even when they do
not, we use them to track explicitness in \ourApproachGlossary{} to
provide a more complete summary of the state of software testing literature
without getting distracted by less relevant details. We find the following
non-mutually exclusive cases of implicit information from the literature:

%% Maybe convert to \paragraph ?
\begin{description}
    \item[1. The information follows logically.]
          \phantomsection{}\label{imp-case-one}~\\
          \citeauthor{Firesmith2015} \citeyearpar[pp.~53\==58]{Firesmith2015}
          lists a set of test approaches that are ``based on the[ir] associated
          quality characteristic[s] and \dots{} associated quality attributes''
          \citetext{p.~53}. This matches our definition of ``test type'', but
          this term is used more loosely in this document to refer to different
          kinds of testing that we would call ``test approaches'' (see
          \Cref{cats-def}). We therefore consider \citeauthor{Firesmith2015}'s
          categorizations of ``test type'' to be implicit\ifnotpaper\
              (such as in \Cref{tab:multiCats,tab:infMultiCats})\fi.
    \item [2. The information is not universal.]
          \phantomsection{}\label{imp-case-two}~\\
          \refHelper \citet[p.~372\ifnotpaper, emphasis added\fi]{IEEE2017}%
          \todo{OG ISO/IEC, 2014} \multiAuthHelper{define} ``regression
          testing'' as ``testing required to determine that a change to a
          system component has not adversely affected \emph{functionality,
              reliability or performance} and has not introduced additional
          defects''. While reliability testing, for example, is not
          \emph{always} a subset of regression testing (since it may be
          performed in other ways), it \emph{can be} accomplished by regression
          testing. This means that these parent-child relations between these
          pairs of approaches only exist \emph{sometimes}, but this qualifier
          is given implicitly.
          %   \citet[p.~5\=/8\ifnotpaper, emphasis added\fi]{SWEBOK2024} provides a
          %   similar list: ``regression testing \dots{} \emph{may} involve
          %   functional and non-functional testing, such as reliability,
          %   accessibility, usability, maintainability, conversion, migration, and
          %   compatibility testing.''
    \item[3. The information is \ifnotpaper conditional\else dubious\fi.]
          \phantomsection{}\label{imp-case-three}~\\ \ifnotpaper
          As a more specific case of information not being universal, sometimes
          prerequisites must be satisfied for information to apply. For example,
          branch condition combination testing is equivalent to (and is therefore
          implied to be a synonym of) exhaustive testing \emph{if} ``each
          subcondition is viewed as a single input'' \citep[p.~464]{PetersAndPedrycz2000}.
          Likewise, statement testing can be used for (and is therefore implied
          to be a child of) unit testing \emph{if} there are ``less than 5000
          lines of code'' \citetext{p.~481\todo{OG Miller et al., 1994}}.
    \item[4. The information is dubious.]
          \phantomsection{}\label{imp-case-four}~\\ \fi
          This happens when there is reason to doubt the information provided.
          If a source claims one thing that is not true, related claims lose
          credibility. For example, \redBoxFlaw*{}
          %   Doubts such as this
          %   can also originate from other sources. \refHelper
          %   \citet[p.~48]{Kam2008} gives ``user scenario testing'' as a synonym
          %   of ``use case testing'', even though ``an actor [in use case testing]
          %   can be \dots{} another system'' \citep[p.~20]{IEEE2021b}, which does
          %   not fit as well with the label ``user scenario testing''. However,
          %   since a system can be seen as a ``user'' of the test item, this
          %   synonym relation is treated as implicit instead of as an objective
          %   flaw.
\end{description}

When we encounter information that meets one of these criteria, we use an
appropriate keyword to capture this nuance in \ourApproachGlossary{}\ifnotpaper\
    (see \Cref{tab:impCaseKeywords})\fi. This also helps us identify implicit
information when performing later analysis. Despite ``implicit'' only
describing the first of these cases, we use it (as well as ``implied by'' when
describing sources of information) as a shorthand for all ``not explicit''
manifestations throughout this \docType{} for clarity.

\ifnotpaper
    \input{assets/tables/impCaseKeywords}

    Regarding the last entry in \Cref{tab:impCaseKeywords}, if a test approach in
    \ourApproachGlossary{} has a name ending in ``~(Testing)'' (space
    included), then the word ``Testing'' might not be part of its name
    \emph{or} it might not be a test approach at all! For example, the term
    ``legacy system integration'' is used in \ifnotpaper
        \citeauthor{Gerrard2000a} (\citeyear[pp.~12\==13, Tab.~2]{Gerrard2000a};
        \citeyear[Tab.~1]{Gerrard2000b})\else
        \cite[pp.~12\==13, Tab.~2]{Gerrard2000a},
        \cite[Tab.~1]{Gerrard2000b}\fi, but the more accurate
    ``legacy system integration testing'' is used in
    \citeyearpar[pp.~30\==31]{Gerrard2000b}. In other cases where a
    term is \emph{not} explicitly labelled as ``testing'', we add the
    suffix ``~(Testing)'' (when it makes sense to do so) and consider
    the test approach to be implied.
\fi

\subsection{Undefined Terms}\label{undef-terms}

The literature mentions many software testing terms without defining them.
While this includes test approaches, software qualities, and more general
software terms, we focus on the former as the main focus of our research.
In particular, \ifnotpaper \citet{IEEE2022} and \citet{Firesmith2015} \else
    \cite{Firesmith2015} and \cite{IEEE2022} \fi name many undefined test
approaches. Once we exhaust the standards in \Cref{stds}, we
perform miniature literature reviews on these subsets to ``fill in'' the
missing definitions (along with any relations), essentially ``snowballing''
on these terms as described in \Cref{ident-sources}. This process uncovers
even more approaches, although some are
out of scope, such as \acf{emsec} testing\ifnotpaper, \else\ and \fi
aspects of \acf{orthat} \ifnotpaper and loop testing (see \Cref{hard-test}),
    and HTML testing (see \Cref{lang-test})\else (see \Cref{scope-overview})\fi.
We investigate the following terms (and their respective related terms) in the
sources given:
\input{build/undefTerms}

\ifnotpaper
    Applying our procedure shown in \Cref{fig:recAppFlowchart} to these sources
    uncovers \the\numexpr \TotalAfter - \TotalBefore\relax\ new approaches and
    \the\numexpr \TotalAfter - \UndefAfter - \TotalBefore + \UndefBefore\relax\ new
    definitions. These definitions are either for existing undefined approaches or
    new uncovered approaches; while not every new approach is presented alongside
    a definition, if we assume that each of these definitions is for a new approach,
    we can deduce that about \the\numexpr 100 - 100 * (\UndefAfter - \UndefBefore) /
    (\TotalAfter - \TotalBefore)\relax\% of added test approaches are defined. This
    indicates that this procedure leads to a higher proportion of defined terms
    (\the\numexpr 100 - 100 * \UndefBefore / \TotalBefore\relax\% vs.~%
    \the\numexpr 100 - 100 * \UndefAfter / \TotalAfter\relax\%), as shown in
    \Cref{fig:undefPies}, which helps verify that our procedure constructively
    uncovers \emph{and} defines new terminology. Further iterating on it would
    continue this trend, creating something approaching a complete taxonomy.

    \input{assets/graphs/undefPies}
\fi

\subsection{Stopping Criteria}\label{stop-crit}

Unfortunately, we cannot continue looking for new approaches indefinitely. We
therefore need a ``stopping criteria'' to let us know when we are ``finished''
looking for test approaches in the literature. Our original plan was to repeat
step~\ref{step:repeat-process} until we got diminishing returns, implying that
we achieved something close to a complete taxonomy! However, this is infeasible
due to time constraints, so we impose our stopping point artificially. With
more time, we would find definitions for all terms we uncover as described in
\Cref{undef-terms}.

\ifnotpaper
    In addition to these undefined terms, some terms do not appear in
    the literature at all! While most test approaches arise as a result of our
    snowballing approach, we each have preexisting knowledge of what test
    approaches exist (a form of experience-based testing, if you will).
    As an example, we are surprised that property-based testing is not mentioned
    in any sources investigated, even considering it as a potential stopping point
    during our research\thesisissueref{57,81,88,125}%\qtodo{I think these issue
    % refs, along with some others may actually be worth keeping in our final
    % thesis/paper; thoughts?}
    . Test approaches such as these that arise independently of snowballing may
    serve as starting points for continued research if we do not find them in
    the literature using our iterative approach. The following terms come from
    previous knowledge, conversations with colleagues, research for other
    projects, or ad hoc cursory research to see what other test approaches exist:
    \newline

    \begin{minipage}{\textwidth}
        \begin{multicols}{2}
            \begin{enumerate}
                \item Chaos engineering
                \item Chosen-ciphertext \ifnotpaper\else \\ \fi attacks
                \item Concolic testing
                \item Concurrent testing
                \item Destructive testing
                \item Dogfooding
                \item Implementation-based testing
                \item Interaction-based \ifnotpaper\else \\ \fi testing
                      \ifnotpaper\else\columnbreak\fi
                \item Lunchtime attacks\ifnotpaper%
                          \footnote{In previous meetings, Dr.~Smith mentioned
                              that with the number of test approaches that suggest
                              that people just like to label everything as
                              ``testing'', he would not be surprised if something
                              like ``Monday morning testing'' existed. While
                              independently researching chosen-ciphertext attacks
                              out of curiosity, this prediction of a time-based
                              test approach came true with ``lunchtime attacks''.}
                      \fi
                \item Parallel testing
                \item Property-based testing
                \item Pseudo-random bit \ifnotpaper\else \\ \fi testing
                \item Rubber duck testing
                \item Scream testing
                \item Shadow testing
                \item Situational testing
            \end{enumerate}
        \end{multicols}
    \end{minipage}
\fi
