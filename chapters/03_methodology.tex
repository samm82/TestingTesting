\section{Methodology}\label{methodology}

% Since we wish to analyze the software testing literature,
We collect data % (both correct and incorrect) 
from a wide variety of documents related to software testing, focusing on
test approaches and supporting information. This results in a large
glossary of software approaches, some glossaries of supplementary terms, and a
list of flaws. To ensure this data can be analyzed and expanded thoroughly and
consistently, we need a process that can be repeated for future developments in
the field of software testing or by independent researchers seeking to verify
our work. Our methodology is as follows:
% for documenting the current (messy) state of software
% testing terminology consists of:

\input{build/methodOverview}

\subsection{Identifying Sources}\label{ident-sources}
As there is no single authoritative source on software testing terminology,
we need to look at many sources to observe how this terminology is used in
practice. % Since we are particularly interested in software engineering,
We start from the vocabulary document for systems and software engineering%
\qtodo{Better name for this?} \citep{IEEE2017} and three versions of the
\acf{swebok} (\citealp{SWEBOK2014}; \citealp{SWEBOK2024};
\citeyear{SWEBOK2025}; see \Cref{swebok-update}).
% ---the newest one \citep{SWEBOK2014} and one submitted for public review \citep{SWEBOK2024}
To gather further sources, we then use a version of ``snowball sampling'',
which ``is commonly used to locate hidden populations \dots{} [via] referrals
from initially sampled respondents to other persons'' \citep{Johnson2014}. We
apply this concept to ``referrals'' between sources. \addTextEx{} We group all
sources into the source tiers we define in \Cref{source-tiers}\listAllSrcs{}.

\subsection{Identifying Relevant Terms}\label{ident-terms}
Before we can consistently track software testing terminology used in the
literature, we must first determine what to record. We use heuristics to guide
this process to increase confidence that we identify all relevant terms, paying
special attention to the following when investigating a new source:
\begin{itemize}
    \item glossaries, taxonomies, hierarchies, and lists of terms,
    \item testing-related terms (e.g., terms containing ``test(ing)'',
          \ifnotpaper ``review(s)'', ``audit(s)'', ``attack(s)''%
              \thesisissuenote{55}, \fi ``validation'', or ``verification''),
    \item terms that had emerged as part of already-discovered
          test approaches, \emph{especially} those that were ambiguous
          or prompted further discussion (e.g., terms containing
          ``performance'', ``recovery'', ``component'', ``bottom-up'',
          \ifnotpaper ``boundary'', \fi or ``configuration''), and
    \item \phantomsection{}\label{implied-approaches}
          terms that imply test approaches, including:
          \NewDocumentCommand{\derivTest}{m m o}{#1 that may imply \ifnotpaper
                  related \fi test #2\ifnotpaper\IfNoValueF{#3}{\footnote{
                          See \Cref{#3} for more detailed discussion.
                      }}\fi}
          \def\covHelper{\item \derivTest{coverage metrics}{techniques}[cov-test],
          }
          \begin{itemize}
              % Slight ordering hack to make paper display nicely
              \ifnotpaper\else\covHelper\fi
              \item \phantomsection{}\label{qual-types}
                    \derivTest{software qualities}{types}[qual-test],
                    \ifnotpaper\covHelper\fi
                    and
              \item \phantomsection{}\label{req-apps}
                    \derivTest{\ifnotpaper software \fi requirements}{approaches}.
          \end{itemize}
\end{itemize}

%% Omitting this since it describes future work outside of this thesis
% During the first pass of data collection, we investigate and record all
% approaches to software testing. Some of these approaches are less
% applicable to our original motivation of test case automation \ifnotpaper
%     (such as static testing; see \Cref{static-test}\thesisissuetodo{39}) \fi or
% are quite broad\ifnotpaper\ (such as attacks%
%     %; see \Cref{attacks}
%     \thesisissuetodo{55})\fi, so we will omit them during future analysis.

\ifnotpaper\newpage\fi
\subsection{Recording Relevant Information}\label{record-info}

% Once we have identified which terms from the literature are relevant, we can
% then track them consistently %. We do this 
% by building glossaries. %where we record
% % the data we find in the literature, mainly focusing on test approaches.
% We give each test approach its own row in \ourApproachGlossary{}, recording its
% name and any given \approachFields*{} (along with any other notes, such as
% questions, prerequisites, and other resources to investigate) following the
% procedure in \Cref{fig:recAppFlowchart}. Note that only the name and category
% fields are required%; all other fields may be left blank
% , although a lack of definition indicates that the approach
% should be investigated further to see if its inclusion is meaningful (see
% \Cref{undef-terms}).

Once we have identified which terms from the literature are relevant, we can
then track them consistently by building glossaries.
We give each test approach its own row in \ourApproachGlossary{}, recording its
name and any given \approachFields*{} (along with any other notes, such as
questions, prerequisites, and other resources to investigate) following the
procedure in \Cref{fig:recAppFlowchart}.
Note that only the name and category fields are required; all other fields
may be left blank, although a lack of definition indicates that the approach
should be investigated further to see if its inclusion is meaningful (see
\Cref{undef-terms}). Flawed data may be documented here as dubious information
(see \Cref{imp-info})\ifnotpaper\ and/or as described in \Cref{record-flaws}\fi.
% When recording flawed data, we record it in our glossary
% as dubious information (see \Cref{imp-info}) and/or in a separate
% document if it is clearly flawed\ifnotpaper\ (see \Cref{record-flaws})\fi.
We also include the source(s) of this information in a consistent format
\ifnotpaper described in \Cref{citation-syntax} \fi to allow for more
detailed analysis of these data.

% If no category is given, the ``approach'' category is assigned
% (with no accompanying citation) as a ``catch-all'' category. 
% Any additional information from other sources is added to
% or merged with the existing information in our glossary where appropriate.
% This includes the generic ``approach'' category being replaced with a more
% specific one, an additional synonym being mentioned, or another source
% describing an already-documented parent-child relation. If any new information
% does not agree with existing information or indicates a potential flaw, â€¦
% When new information does not conflict with existing information, we either
% keep the clearest and most concise version or merge them to paint a more
% complete picture.

For example, when we first encounter ``A/B Testing'' in
\citet[p.~1]{IEEE2022} as shown in \Cref{fig:IEEE-A-B-Testing}, we apply
our procedure as follows:\qtodo{Should glossary headers be capitalized?}
\ifnotpaper\begin{figure}[h!]\else\begin{figure}[b!]\fi
    \includegraphics[width=\linewidth]{assets/images/a-b testing.png}
    \caption{\citepos[p.~1]{IEEE2022} glossary entry for
        ``A/B testing''.}\label{fig:IEEE-A-B-Testing}
\end{figure}
\begin{enumerate}
    \item Create a new row with the name ``A/B testing'' and the category
          ``Approach''.
    \item Record the synonym ``Split-Run Testing''.
    \item Record the parent ``Statistical Testing''.
    \item Record the definition ``Testing `that allows testers to determine
          which of two systems or components performs better'\,''; note that
          we abstract away information that we have previously captured (i.e.,
          its synonym and parent).
\end{enumerate}
In addition to repeating this information on \citetext{p.~36}, this source also
provides the following information, which we capture as follows:
\begin{enumerate}
    \item Record the note ``It `can be time-consuming, although tools can be
          used to support it', `is a means of solving the test oracle problem
          by using the existing system as a partial oracle', and is `not a test
          case generation technique as test inputs are not generated'\,''
          \citetext{p.~36}.
    \item Replace the category of ``Approach'' with the more specific
          ``Practice'' \citetext{Fig.~2}; note that this
          is consistent with the exclusion of ``Technique'' as a possible
          category for this approach \citetext{p.~36}.
\end{enumerate}

\ifnotpaper
    \input{assets/graphs/recAppFlowchart}
    \clearpage
\fi

As we investigate other sources, we learn more about this approach. \ifnotpaper
\else \citeauthor{Firesmith2015} \fi \citet[p.~58]{Firesmith2015} includes it
in his taxonomy as shown in \Cref{fig:Firesmith-A-B-Testing}. We add to our
entry for ``A/B Testing'' as follows:

\begin{minipage}{0.45\linewidth}
    \vspace{0.5cm}
    \includegraphics[width=\linewidth]{assets/images/a-b testing 2.png}
    \captionof{figure}{A/B testing's inclusion in \citepos[p.~58]{Firesmith2015}
        taxonomy.}\label{fig:Firesmith-A-B-Testing}
    \vspace{0.5cm}
\end{minipage}
\begin{minipage}{\ifnotpaper 0.53\else 0.5\fi\linewidth}
    \begin{enumerate}
        \item Add the parent ``Usability Testing''.
        \item Since usability testing is a test type \ifnotpaper
                  (\citealp[pp.~22, 26\=/27]{IEEE2022};
                  \citeyear[pp.~7, 40, Tab.~A.1]{IEEE2021c};
                  implied by its quality; \citealp[p.~53]{Firesmith2015})\else
                  \cite[pp.~22, 26\=/27]{IEEE2022},
                  \cite[pp.~7, 40, Tab.~A.1]{IEEE2021c}\fi, add the category
              ``Type'' with the citation ``(inferred from usability testing)''.
    \end{enumerate}
\end{minipage}
This second change introduces an inference \ifnotpaper (defined in \Cref{infers})
\fi that violates our assumption \ifnotpaper from \Cref{orth-approach} \fi that
categories are orthogonal, so we consider this to be an inferred flaw that we
automatically detect and document \ifnotpaper (see \Cref{auto-flaw-detect,%
        tab:infMultiCats}, respectively). This results in the corresponding row
    in \Cref{tab:approachGlossaryExcerpt}, although we exclude the ``Notes''
    column for brevity\else (details omitted for brevity)\fi.

\phantomsection{}\label{qual-supp-procedure}
We use this same procedure to track software qualities and supplementary
terminology that is either shared by multiple approaches or too complicated to
explain inline. We create a separate glossary for both qualities and
supplementary terms, each with a similar format to \ourApproachGlossary.
Since these terms do not have categories, the process of recording them is much
simpler, only requiring us to record the name, definition, and synonym(s) of
these terms, along with any additional notes. The only new information
we capture is the ``precedence'' for a software quality to have an associated
test type, since each test type measures a particular software quality (see
\Cref{tab:ieeeCats}). These precedences are instances where a given software
quality is related to, is covered by, or is a child, parent, or prerequisite
of another quality with an associated test type, as given by the literature.

Tracking information about software qualities helps us investigate the
literature more thoroughly, since these data may become relevant based on
information from other yet-uninvestigated sources. When the literature mentions
(or implies) a test approach that corresponds to a software quality we have
recorded, we first follow the procedure given in \Cref{fig:recAppFlowchart}
with the information provided in the source that mentions it. We then remove
the relevant data from our quality glossary and repeat our procedure with it
to upgrade the quality to a test type in \ourApproachGlossary{}.

% For example, analyzability%
% \footnote{This may be spelled ``analyzability'' \citep[p.~18]{IEEE2017} or
%     ``analysability'' \citep{ISO_IEC2023a}; since this is a dialectal
%     difference, we do not count this as a label flaw (described in
%     \Cref{label-flaw-def}).}, modifiability, modularity, reusability, and
% testability are all subqualities of maintainability \ifnotpaper
%     (\citealp{ISO_IEC2023a}; \citealp[Tab.~A.1]{IEEE2021c};
%     \citealp[p.~7\=/10]{SWEBOK2024}) \else
%     \cite[p.~7\=/10]{SWEBOK2024}, \cite[Tab.~A.1]{IEEE2021c},
%     \cite{ISO_IEC2023a} \fi which has an associated test type
% \ifnotpaper
%     (\citealp[pp.~5, 22]{IEEE2022}; \citeyear[p.~38, Tab.~A.1]{IEEE2021c})\else
%     \cite[pp.~5, 22]{IEEE2022}, \cite[p.~38, Tab.~A.1]{IEEE2021c}\fi. This sets
% a ``precedent'' for each of these subqualities having its own associated test
% type (e.g., reusability testing). \ifnotpaper
%     Sometimes, the literature provides a relation between qualities where at
%     least one of them does not have an explicit approach associated with it
%     (although it may be inferred). We record this information regardless, since
%     it may become relevant if an associated test type emerges as alluded to
%     \hyperref[qual-types]{earlier}. For example, \citet{ISO_IEC2023a} provides
%     relations involving dependability and modifiability, but only in terms of
%     them as qualities, not types of testing. We record these relations in case
%     an associated test type is explicitly named by the literature, which would
%     inherit these relations. \fi

% \subsubsection{Derived Test Approaches}\label{derived-tests}

% Throughout this research, we noticed many groups of test approaches that arise
% from some underlying area of software (testing) knowledge. The legitimacy of
% extrapolating new test approaches from these knowledge domains is heavily
% implied by the literature, but not explicitly stated as a general rule. Regardless,
% since the field of software is ever-evolving, it is crucial to be able to
% adapt to, talk about, and understand new developments in software testing.
% Bases for defining new test approaches suggested by the literature include
% coverage metrics, software qualities, and attacks. These are meaningful
% enough to merit analysis and are therefore in scope. Requirements may also
% imply related test approaches, but this mainly results in test approaches
% that would be out of scope. \ifnotpaper Other test approaches found in the
%     literature are derived from programming languages or other orthogonal test
%     approaches, but these are out of scope as this information is better
%     captured by other approaches (see \Cref{lang-test,orth-test}, respectively).

%     \paragraph{Attacks}\label{attacks}
%     While attacks can be ``malicious'' \citep[p.~7]{IEEE2017}, they are also
%     described as a test approach \ifnotpaper
%         (\citeyear[pp.~4, 34]{IEEE2022}; \citeyear[p.~4]{IEEE2021c};
%         \citeyear[p.~7]{IEEE2019a}; \citealpISTQB{}; implied by
%         \citealp[p.~5\=/10]{SWEBOK2024}; \citealp[p.~26]{Bas2024};
%         \citealp[p.~87\==89]{Patton2006})\else \cite[pp.~4, 34]{IEEE2022},
%         \cite[p.~4]{IEEE2021c}; \cite[p.~7]{IEEE2019a}; \cite{ISTQB}\fi.
%     This is supported by the fact that penetration testing is also called
%     ``ethical hacking testing'' \citep[p.~13\=/4]{SWEBOK2024} or just ``ethical
%     hacking'' \citep[p.~28; see \flawref{ethical-hacking}]{Gerrard2000b}. This
%     means that software attacks, such as code injection and password cracking
%     \citepISTQB{}, can also be used for testing software if they are performed
%     systematically to test and improve the software (i.e., \emph{without} the
%     malicious intent).

%     \paragraph{Requirements-driven Approaches}\label{req-test}
%     While not as universally applicable, some kinds of requirements have associated
%     types of testing (e.g., functional, non-functional, security). This may mean
%     that others kinds of requirements \emph{also} have associated test approaches;
%     for example, we infer ``technical testing'' from the existence of technical
%     requirements \citep[p.~463]{IEEE2017} and requirements-based testing
%     (see \Cref{infers}). \ifnotpaper Even assuming this is true, some kinds of
%         requirements do not apply to the code itself so their relevant inferred
%         test approaches are out of scope (see \Cref{phys-req-test,nontech-req-test}).\fi
% \fi
\ifnotpaper\subsubsection{Recording Implicit Information}
\else\subsection{Recording Implicit Information}
\fi\label{imp-info}

As described in \Cref{explicitness}, the use of natural language introduces
significant nuance that we need to document. Keywords such as \impKeywords{}
indicate that information from the literature is \emph{not} explicit. These
keywords often appear directly within the literature, but even when they do
not, we use them to track explicitness in \ourApproachGlossary{} to
provide a more complete summary of the state of software testing literature
without getting distracted by less relevant details. We find the following
non-mutually exclusive cases of implicit information from the literature:

\begin{enumerate}
    \item \textbf{The information follows logically} from the source and
          information from others, but is not explicitly stated.
    \item \textbf{The information is not universal} but still applies in
          certain cases. \ifnotpaper
    \item \textbf{The information is conditional}, requiring certain
          prerequisites to be satisfied (a more specific case of information
          not being universal). \fi
    \item \textbf{The information is dubious}; while it is present in the
          literature, there is reason to doubt its accuracy.
\end{enumerate}

% %% Maybe convert to \paragraph ?
% \begin{description}
%     \item[1. The information follows logically.]
%           \phantomsection{}\label{imp-case-one}~\\
%           \citeauthor{Firesmith2015} \citeyearpar[pp.~53\==58]{Firesmith2015}
%           lists a set of test approaches that are ``based on the[ir] associated
%           quality characteristic[s] and \dots{} associated quality attributes''
%           \citetext{p.~53}. This matches our definition of ``test type'', but
%           this term is used more loosely in this document to refer to different
%           kinds of testing that we would call ``test approaches'' (see
%           \Cref{cats-def}). We therefore consider \citeauthor{Firesmith2015}'s
%           categorizations of ``test type'' to be implicit\ifnotpaper\
%               (such as in \Cref{tab:multiCats,tab:infMultiCats})\fi.
%     \item [2. The information is not universal.]
%           \phantomsection{}\label{imp-case-two}~\\
%           \refHelper \citet[p.~372\ifnotpaper, emphasis added\fi]{IEEE2017}%
%           \todo{OG ISO/IEC, 2014} \multiAuthHelper{define} ``regression
%           testing'' as ``testing required to determine that a change to a
%           system component has not adversely affected \emph{functionality,
%               reliability or performance} and has not introduced additional
%           defects''. While reliability testing, for example, is not
%           \emph{always} a subset of regression testing (since it may be
%           performed in other ways), it \emph{can be} accomplished by regression
%           testing. This means that these parent-child relations between these
%           pairs of approaches only exist \emph{sometimes}, but this qualifier
%           is given implicitly.
%           %   \citet[p.~5\=/8\ifnotpaper, emphasis added\fi]{SWEBOK2024} provides a
%           %   similar list: ``regression testing \dots{} \emph{may} involve
%           %   functional and non-functional testing, such as reliability,
%           %   accessibility, usability, maintainability, conversion, migration, and
%           %   compatibility testing.''
%     \item[3. The information is \ifnotpaper conditional\else dubious\fi.]
%           \phantomsection{}\label{imp-case-three}~\\ \ifnotpaper
%           As a more specific case of information not being universal, sometimes
%           prerequisites must be satisfied for information to apply. For example,
%           branch condition combination testing is equivalent to (and is therefore
%           implied to be a synonym of) exhaustive testing \emph{if} ``each
%           subcondition is viewed as a single input'' \citep[p.~464]{PetersAndPedrycz2000}.
%           Likewise, statement testing can be used for (and is therefore implied
%           to be a child of) unit testing \emph{if} there are ``less than 5000
%           lines of code'' \citetext{p.~481\todo{OG Miller et al., 1994}}.
%     \item[4. The information is dubious.]
%           \phantomsection{}\label{imp-case-four}~\\ \fi
%           This happens when there is reason to doubt the information provided.
%           If a source claims one thing that is not true, related claims lose
%           credibility. For example, \redBoxFlaw*{}
%           %   Doubts such as this
%           %   can also originate from other sources. \refHelper
%           %   \citet[p.~48]{Kam2008} gives ``user scenario testing'' as a synonym
%           %   of ``use case testing'', even though ``an actor [in use case testing]
%           %   can be \dots{} another system'' \citep[p.~20]{IEEE2021c}, which does
%           %   not fit as well with the label ``user scenario testing''. However,
%           %   since a system can be seen as a ``user'' of the test item, this
%           %   synonym relation is treated as implicit instead of as an objective
%           %   flaw.
% \end{description}

When we encounter information that meets one of these criteria, we use an
appropriate keyword to capture this nuance in \ourApproachGlossary{}\ifnotpaper\
    (see \Cref{tab:impCaseKeywords})\fi. This also helps us identify implicit
information when performing later analysis. Despite ``implicit'' only
describing the first of these cases, we use it (as well as ``implied by'' when
describing sources of information) as a shorthand for all ``not explicit''
information throughout this \docType{} for clarity.

\ifnotpaper
    Regarding the last entry in \Cref{tab:impCaseKeywords}, if a test approach in
    \ourApproachGlossary{} has a name ending in ``~(Testing)'' (space
    included), then the word ``Testing'' might not be part of its name
    \emph{or} it might not be a test approach at all! For example, the term
    ``legacy system integration'' is used in \ifnotpaper
        \citeauthor{Gerrard2000a} (\citeyear[pp.~12\==13, Tab.~2]{Gerrard2000a};
        \citeyear[Tab.~1]{Gerrard2000b})\else
        \cite[pp.~12\==13, Tab.~2]{Gerrard2000a},
        \cite[Tab.~1]{Gerrard2000b}\fi, but the more accurate
    ``legacy system integration testing'' is used in
    \citeyearpar[pp.~30\==31]{Gerrard2000b}. In other cases where a
    term is \emph{not} explicitly labelled as ``testing'', we add the
    suffix ``~(Testing)'' (when it makes sense to do so) and consider
    the test approach to be implied.

    \input{assets/tables/impCaseKeywords}

    \subsubsection{Recording Flaws}\label{record-flaws}
    While we can detect some subsets of flaws automatically by analyzing
    \ourApproachGlossary{} (see \Cref{auto-flaw-detect}), most
    are too complex and need to be tracked manually. We record these more
    detailed flaws % as \LaTeX{} enumeration items in a separate document
    along with extra information such as the flaw's manifestation (defined in
    \Cref{mnfst-def}), domain (defined in \Cref{dmn-def}), and source(s)
    responsible, following the format in \Cref{flaw-comment-syntax}. This
    helps us analyze these flaws later as described in
    \Cref{flaw-comment-analysis}.

    \phantomsection{}\label{multi-view-flaws}
    It is important to note that when a flaw can be viewed in multiple ways,
    we record multiple pairs of manifestations and domains. For example,%
    \seeRefMissing*{} This is trivially an example of a missing definition,
    but is also an example of incorrect traceability information. Therefore,
    we record both of these views\thesisissuenote{157}, although
    % include one comment line for \texttt{(WRONG, TRACE)} and one for \texttt{(MISS, DEFS)}
    % The comment that appears first determines where this flaw is displayed,
    % although all pairs are counted as their own flaws; 
    we display this flaw as an example of incorrect traceability information as
    \flawref{see-ref-missing} since we determine this to be more meaningful.
    % as described in \Cref{flaw-comment-analysis}
    \newpage
\fi

\subsection{Undefined Terms}\label{undef-terms}

The literature mentions many software testing terms without defining them.
While this includes test approaches, software qualities, and more general
software terms, we focus on the former as the main focus of our research.
In particular, \ifnotpaper \citet{IEEE2022} and \citet{Firesmith2015} \else
    \cite{Firesmith2015} and \cite{IEEE2022} \fi name many undefined test
approaches. Once we exhaust the standards in \Cref{stds}, we
perform miniature literature reviews on these subsets to ``fill in'' the
missing definitions (along with any relations), essentially ``snowballing''
on these terms as described in \Cref{ident-sources}. This process uncovers
even more approaches, on which we can then repeat this process.

\subsection{Stopping Criteria}\label{stop-crit}

Unfortunately, continuing to look for test approaches indefinitely is
infeasible. We therefore need a ``stopping criteria'' to let us know when we
are ``finished'' looking for test approaches in the literature. A reasonable
heuristic is to repeat step~\ref{step:repeat-process} until it yields
diminishing returns; i.e., investigating new sources does not reveal new
approaches, relations between them, or information about them. This implies
that something close to a complete taxonomy has been achieved!
