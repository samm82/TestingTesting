\section{Methodology}\label{methodology}

% Since we wish to analyze the software testing literature,
We collect data % (both correct and incorrect) 
from a wide variety of documents related to software testing, focusing on
test approaches and supporting information. This results in a large
glossary of software approaches, some glossaries of supplementary terms, and a
list of flaws. To ensure this data can be analyzed and expanded thoroughly and
consistently, we need a process that can be repeated for future developments in
the field of software testing or by independent researchers seeking to verify
our work. Our methodology is as follows:
% for documenting the current (messy) state of software
% testing terminology consists of:

\input{build/methodOverview}

\subsection{Identifying Sources}\label{ident-sources}
As there is no single authoritative source on software testing terminology,
we need to look at many sources to observe how this terminology is used in
practice.
% Since we are particularly interested in software engineering,
We start from the vocabulary document for systems and software engineering%
\qtodo{Better name for this?}
\citep{IEEE2017} and two versions of the \acf{swebok}---the newest
one \citep{SWEBOK2014} and one submitted for public review%
\footnote{%
    \refHelper \citet{SWEBOK2024} has been published since we investigated
    these sources; if time permits, we will revisit this published version.
}
\citep{SWEBOK2024}. To gather further sources, we then use a version of
``snowball sampling'',
which ``is commonly used to locate hidden populations \dots{} [via] referrals
from initially sampled respondents to other persons'' \citep{Johnson2014}. We
apply this concept to ``referrals'' between sources. \addTextEx{} We group all
sources into the source tiers we define in \Cref{source-tiers}\listAllSrcs{}.

\subsection{Identifying Relevant Terms}\label{ident-terms}
Before we can consistently track software testing terminology used in the
literature, we must first determine what to record. We use heuristics to guide
this process to increase confidence that we identify all relevant terms, paying
special attention to the following when investigating a new source:
\begin{itemize}
    \item glossaries, taxonomies, hierarchies, and lists of terms,
    \item testing-related terms (e.g., terms containing ``test(ing)'',
          \ifnotpaper ``review(s)'', ``audit(s)'', ``attack(s)''%
              \thesisissueref{55}, \fi ``validation'', or ``verification''),
    \item terms that had emerged as part of already-discovered
          test approaches, \emph{especially} those that were ambiguous
          or prompted further discussion (e.g., terms containing
          ``performance'', ``recovery'', ``component'', ``bottom-up'',
          \ifnotpaper ``boundary'', \fi or ``configuration''), and
    \item terms that imply test approaches\ifnotpaper\footnote{
                  Since these methods for deriving test approaches only arose
                  as research progressed, some examples would have been missed
                  during the first pass(es) of resources investigated earlier
                  in the process. While reiterating over them would be ideal,
                  this may not be possible due to time constraints.
              }% (see \Cref{derived-tests})
          \fi, including:
          \newcommand\derivTest[2]{#1 that may imply \ifnotpaper related \fi test #2}
          \begin{itemize}
              \item \derivTest{coverage metrics}{techniques}%\ifnotpaper
                    % (\citealp[p.~11]{IEEE2022}; \citeyear[p.~5]{IEEE2021a};
                    % \citealp[p.~2]{Reid1996})\else
                    % \cite[p.~11]{IEEE2022}, \cite[p.~5]{IEEE2021a},
                    % \cite[p.~2]{Reid1996}\fi
                    ,
              \item \phantomsection{}\label{qual-types}
                    \derivTest{software qualities}{types}, and
              \item \phantomsection{}\label{req-apps}
                    \derivTest{\ifnotpaper software \fi requirements}{approaches}.
          \end{itemize}
\end{itemize}

%% Omitting this since it describes future work outside of this thesis
% During the first pass of data collection, we investigate and record all
% approaches to software testing. Some of these approaches are less
% applicable to our original motivation of test case automation \ifnotpaper
%     (such as static testing; see \Cref{static-test}\thesisissueref{39}) \fi or
% are quite broad\ifnotpaper\ (such as attacks%
%     %; see \Cref{attacks}
%     \thesisissueref{55})\fi, so we will omit them during future analysis.

\subsection{Recording Relevant Information}\label{record-info}

\ifnotpaper
    \input{assets/graphs/recAppFlowchart}
\fi

Once we have identified which terms from the literature are relevant, we can
then track them consistently. We do this by building glossaries, mainly
focusing on the one most central to our research---\ourApproachGlossary{}---where
we record all test approaches we identify in the literature. We give each test
approach its own row where we record its name and any given \approachFields*{}
(along with any other notes, such as questions, prerequisites, and other
resources to investigate) following the procedure in \Cref{fig:recAppFlowchart}.
Note that only the name and category fields are required; all other fields
may be left blank, although a lack of definition indicates that the approach
should be investigated further to see if its inclusion is meaningful (see
\Cref{undef-terms}). When recording flawed data, we record it in our glossary
as dubious information (see \Cref{imp-info}) and/or in a separate
document if it is clearly flawed\ifnotpaper\ (see \Cref{record-flaws})\fi.
% If no category is given, the ``approach'' category is assigned
% (with no accompanying citation) as a ``catch-all'' category. 
% Any additional information from other sources is added to
% or merged with the existing information in our glossary where appropriate.
% This includes the generic ``approach'' category being replaced with a more
% specific one, an additional synonym being mentioned, or another source
% describing an already-documented parent-child relation. If any new information
% does not agree with existing information or indicates a potential flaw, â€¦
% When new information does not conflict with existing information, we either
% keep the clearest and most concise version or merge them to paint a more
% complete picture.

\begin{figure}[bt!]
    \includegraphics[width=\linewidth]{assets/images/a-b testing.png}
    \caption{\refHelper \citet[p.~1]{IEEE2022}'s glossary entry for
        ``A/B testing''.}\label{fig:IEEE-A-B-Testing}
\end{figure}

For example, when we first encounter ``A/B Testing'' in
\citet[p.~1, 36]{IEEE2022} as shown in \Cref{fig:IEEE-A-B-Testing}, we apply
our procedure as follows:\qtodo{Should glossary headers be capitalized?}
\begin{enumerate}
    \item Create a new row with the name ``A/B testing'' and the category
          ``Approach''.
    \item Record the synonym ``Split-Run Testing''.
    \item Record the parent ``Statistical Testing''.
    \item Record the definition ``Testing `that allows testers to determine
          which of two systems or components performs better'\,''; note that
          we abstract away information that we have previously captured (i.e.,
          its synonym and parent).
\end{enumerate}
\ifnotpaper\newpage\noindent\fi
We also include the relevant citation \ifnotpaper
    ``\citep[pp.~1, 36]{IEEE2022}''\footnote{In \ourApproachGlossary{}, we
        \emph{actually} cite this as ``(\hyperlink{cite.IEEE2022}{
            \textcolor{blue!50!black}{IEEE}},  % With help from ChatGPT
        \citeyear[pp.~1, 36]{IEEE2022})'' for brevity, as most standards are
        written by ISO/IEC \emph{and} IEEE (see \Cref{fig:ieeeSourceSets});
        these glossary citations are handled by our choice of \BibTeX{} keys,
        such as \texttt{IEEE2022} in this example.}
\else in the author-year citation format \fi with these entries (excluding its
name) to track where they came from. This source also provides the following
information, which we capture as follows:
\begin{enumerate}
    \item Record the note ``It `can be time-consuming, although tools can be
          used to support it', `is a means of solving the test oracle problem
          by using the existing system as a partial oracle', and is `not a test
          case generation technique as test inputs are not generated'\,''
          \citetext{p.~36}.
    \item Replace the category of ``Approach'' with the more specific
          ``Practice'' \citetext{Fig.~2}; note that this
          is consistent with the exclusion of ``Technique'' as a possible
          category for this approach \citetext{p.~36}.
\end{enumerate}
As we investigate other sources, we learn more about this approach. \ifnotpaper
\else \citeauthor{Firesmith2015} \fi \citet[p.~58]{Firesmith2015} includes it
in his taxonomy as shown in \Cref{fig:Firesmith-A-B-Testing}. We add to our
entry for ``A/B Testing'' as follows:

\begin{minipage}{0.45\linewidth}
    \vspace{0.5cm}
    \includegraphics[width=\linewidth]{assets/images/a-b testing 2.png}
    \captionof{figure}{A/B testing's inclusion in \ifnotpaper \else
            \citeauthor{Firesmith2015} \fi \citet[p.~58]{Firesmith2015}'s
        taxonomy.}\label{fig:Firesmith-A-B-Testing}
    \vspace{0.5cm}
\end{minipage}
\begin{minipage}{\ifnotpaper 0.53\else 0.5\fi\linewidth}
    \begin{enumerate}
        \item Add the parent ``Usability Testing''.
        \item Since usability testing is a test type \ifnotpaper
                  (\citealp[pp.~22, 26\=/27]{IEEE2022};
                  \citeyear[pp.~7, 40, Tab.~A.1]{IEEE2021c};
                  implied by its quality; \citealp[p.~53]{Firesmith2015})\else
                  \cite[pp.~22, 26\=/27]{IEEE2022},
                  \cite[pp.~7, 40, Tab.~A.1]{IEEE2021c}\fi, add the category
              ``Type'' with the citation ``(inferred from usability testing)''.
    \end{enumerate}
\end{minipage}
This second change introduces an inference \ifnotpaper (defined in \Cref{infers})
\fi that violates our assumption \ifnotpaper from \Cref{orth-approach} \fi that
categories are orthogonal, so we consider this to be an inferred flaw that we
automatically detect and document \ifnotpaper (see \Cref{auto-flaw-detect,%
        tab:infMultiCats}, respectively). This results in the corresponding row
    in \Cref{tab:approachGlossaryExcerpt}, although we exclude the ``Notes''
    column for brevity\else (details omitted for brevity)\fi.

\ifnotpaper
    \phantomsection{}\label{record-cites}
    As implied throughout the previous example, we capture relevant citation
    information in our glossary in the author-year citation format. When citing
    the same authors or sources multiple times, we ``reuse'' information from
    previous citations
    when applicable. For example, the first row of \Cref{tab:exampleGlossary}
    contains the citation ``(Author, 2022; 2021)'', which means that this
    information was present in two documents by Author: one written in
    2022 and one in 2021. The following citation, ``(2022)'', contains no author,
    which means it was written by the same one as the previous citation (Author).
    This reduces duplication and improves maintainability when recording this
    information. When processing these data when visualizating relations
    (see \Cref{\ifnotpaper app-rel-vis\else tools\fi}), we do so according to this
    logic \seeSrcCode{231bd5e}{scripts/csvToGraph.py}{54}{95} so we can
    consistently track the source(s) of data throughout our analysis.
\fi

\phantomsection{}\label{qual-supp-procedure}
We use this same procedure to track software qualities and supplementary
terminology that is either shared by multiple approaches or too complicated to
explain inline. We create a separate glossary for both qualities and
supplementary terms, each with a similar format to \ourApproachGlossary.
Since these terms do not have categories, the process of recording them is much
simpler, only requiring us to record the name, definition, and synonym(s) of
these terms, along with any additional notes. The only new information
we capture is the ``precedence'' for a software quality to have an associated
test type, since each test type measures a particular software quality (see
\Cref{tab:ieeeCats}). These precedences are instances where a given software
quality is related to, is covered by, or is a child, parent, or prerequisite
of another quality with an associated test type, as given by the literature.

Tracking information about software qualities helps us investigate the
literature more thoroughly, since these data may become relevant based on
information from other yet-uninvestigated sources. When the literature mentions
(or implies) a test approach that corresponds to a software quality we have
recorded, we first follow the procedure given in \Cref{fig:recAppFlowchart}
with the information provided in the source that mentions it. We then remove
the relevant data from our quality glossary and repeat our procedure with it
to upgrade the quality to a test type in \ourApproachGlossary{}.

% For example, analyzability%
% \footnote{This may be spelled ``analyzability'' \citep[p.~18]{IEEE2017} or
%     ``analysability'' \citep{ISO_IEC2023a}; since this is a dialectal
%     difference, we do not count this as a label flaw (described in
%     \Cref{label-flaw-def}).}, modifiability, modularity, reusability, and
% testability are all subqualities of maintainability \ifnotpaper
%     (\citealp{ISO_IEC2023a}; \citealp[Tab.~A.1]{IEEE2021c};
%     \citealp[p.~7\=/10]{SWEBOK2024}) \else
%     \cite[p.~7\=/10]{SWEBOK2024}, \cite[Tab.~A.1]{IEEE2021c},
%     \cite{ISO_IEC2023a} \fi which has an associated test type
% \ifnotpaper
%     (\citealp[pp.~5, 22]{IEEE2022}; \citeyear[p.~38, Tab.~A.1]{IEEE2021c})\else
%     \cite[pp.~5, 22]{IEEE2022}, \cite[p.~38, Tab.~A.1]{IEEE2021c}\fi. This sets
% a ``precedent'' for each of these subqualities having its own associated test
% type (e.g., reusability testing). \ifnotpaper
%     Sometimes, the literature provides a relation between qualities where at
%     least one of them does not have an explicit approach associated with it
%     (although it may be inferred). We record this information regardless, since
%     it may become relevant if an associated test type emerges as alluded to
%     \hyperref[qual-types]{earlier}. For example, \citet{ISO_IEC2023a} provides
%     relations involving dependability and modifiability, but only in terms of
%     them as qualities, not types of testing. We record these relations in case
%     an associated test type is explicitly named by the literature, which would
%     inherit these relations. \fi

% \subsubsection{Derived Test Approaches}\label{derived-tests}

% Throughout this research, we noticed many groups of test approaches that arise
% from some underlying area of software (testing) knowledge. The legitimacy of
% extrapolating new test approaches from these knowledge domains is heavily
% implied by the literature, but not explicitly stated as a general rule. Regardless,
% since the field of software is ever-evolving, it is crucial to be able to
% adapt to, talk about, and understand new developments in software testing.
% Bases for defining new test approaches suggested by the literature include
% coverage metrics, software qualities, and attacks. These are meaningful
% enough to merit analysis and are therefore in scope. Requirements may also
% imply related test approaches, but this mainly results in test approaches
% that would be out of scope. \ifnotpaper Other test approaches found in the
%     literature are derived from programming languages or other orthogonal test
%     approaches, but these are out of scope as this information is better
%     captured by other approaches (see \Cref{lang-test,orth-test}, respectively).

%     \paragraph{Coverage-driven Techniques}\label{cov-test}

%     Test techniques are able to ``identify test coverage items \dots{} and
%     derive corresponding test cases''
%     \ifnotpaper
%         (\citealp[p.~11]{IEEE2022}; similar in \citeyear[p.~467]{IEEE2017})
%     \else
%         \cite[p.~11]{IEEE2022} (similar in \cite[p.~467]{IEEE2017})
%     \fi
%     in a ``systematic'' way
%     \citeyearpar[p.~464]{IEEE2017}.
%     \ifnotpaper
%         This allows for ``the coverage achieved by a specific test design
%         technique'' to be calculated as a percentage of ``the number of test
%         coverage items covered by executed test cases'' \citeyearpar[p.~30]{IEEE2021c}.
%         %     ``Coverage levels can range
%         %     from 0\% to 100\%'' and may or may not include ``infeasible'' test coverage
%         %     items, which are ``not \dots{} executable or [are] impossible to be covered by a
%         %     test case'' \citetext{p.~30}. Perhaps more interestingly, the further
%         %     implication is
%         % \else
%         %     This means
%     \fi % that
%     Therefore, a given coverage metric implies a test approach aimed to
%     maximize it. For example, path testing ``aims to execute all entry-to-exit
%     control flow paths in a \acs{sut}'s control flow graph'' \citep[p.~5-13]{SWEBOK2024},
%     thus maximizing the path coverage
%     \ifnotpaper
%         \citep[see][Fig.~1\thesisissueref{63}]{SharmaEtAl2021}\else
%         (see \cite[Fig.~1]{SharmaEtAl2021}\thesisissueref{63})\fi.

%     \paragraph{Quality-driven Types}\label{qual-test}

%     Since test types are ``focused on specific quality characteristics''
%     \ifnotpaper
%         (\citealp[p.~15]{IEEE2022}; \citeyear[p.~7]{IEEE2021c};
%         \citeyear[p.~473]{IEEE2017}\todo{OG IEEE 2013})%
%     \else
%         \cite[p.~15]{IEEE2022}, \cite[p.~7]{IEEE2021c}, \cite[p.~473]{IEEE2017}%
%     \fi, they can be derived from software qualities: ``capabilit[ies] of
%     software product[s] to satisfy stated and implied needs when used under
%     specified conditions'' \citep[p.~424]{IEEE2017}\todo{OG ISO/IEC 2014}. This
%     is supported by reliability and performance testing, which are both examples of
%     test types \citeyearpar{IEEE2022, IEEE2021c} that are based on their underlying
%     qualities \citep[p.~18]{FentonAndPfleeger1997}.
%     % \ifnotpaper
%     %     For quantifying quality-driven testing, measurements should include
%     %     an entity to be measured, a specific attribute to measure, and the actual
%     %     measure (i.e., units, starting state, ending state, what to include)
%     %     \citetext{p.~36} where attributes must be
%     %     defined before they can be measured \citetext{p.~38}.
%     %
%     % \fi
%     Because of the importance of software qualities to defining test types, we track
%     \qualityCount{} software qualities\thesisissueref{21,23,27} in addition to our
%     tracked test approaches following the procedure in \Cref{qual-supp-procedure}.
%     We then ``upgrade'' software qualities to test types when they are mentioned
%     (or implied) by a source by removing its entry from this quality glossary
%     and adding an associated test approach to \ourApproachGlossary{}, also outlined
%     in \Cref{methodology}. Examples of this include conformance testing \ifnotpaper
%         (\citealp[p.~5\=/7]{SWEBOK2024}; \citealp[p.~25]{JardEtAl1999}; implied
%         by \citealp[p.~93]{IEEE2017})\else \cite[p.~5\=/7]{SWEBOK2024},
%         \cite[p.~25]{JardEtAl1999}\fi, efficiency testing
%     \citep[p.~44]{Kam2008}, and survivability testing \citep[p.~40]{GhoshAndVoas1999}.

%     \paragraph{Attacks}\label{attacks}
%     While attacks can be ``malicious'' \citep[p.~7]{IEEE2017}, they are also
%     described as a test approach \ifnotpaper
%         (\citeyear[pp.~4, 34]{IEEE2022}; \citeyear[p.~4]{IEEE2021c};
%         \citeyear[p.~7]{IEEE2019a}; \citealpISTQB{}; implied by
%         \citealp[p.~5\=/10]{SWEBOK2024}; \citealp[p.~26]{Bas2024};
%         \citealp[p.~87\==89]{Patton2006})\else \cite[pp.~4, 34]{IEEE2022},
%         \cite[p.~4]{IEEE2021c}; \cite[p.~7]{IEEE2019a}; \cite{ISTQB}\fi.
%     This is supported by the fact that penetration testing is also called
%     ``ethical hacking testing'' \citep[p.~13\=/4]{SWEBOK2024} or just ``ethical
%     hacking'' \citep[p.~28; see \flawref{ethical-hacking}]{Gerrard2000b}. This
%     means that software attacks, such as code injection and password cracking
%     \citepISTQB{}, can also be used for testing software if they are performed
%     systematically to test and improve the software (i.e., \emph{without} the
%     malicious intent).

%     \paragraph{Requirements-driven Approaches}\label{req-test}
%     While not as universally applicable, some kinds of requirements have associated
%     types of testing (e.g., functional, non-functional, security). This may mean
%     that others kinds of requirements \emph{also} have associated test approaches;
%     for example, we infer ``technical testing'' from the existence of technical
%     requirements \citep[p.~463]{IEEE2017} and requirements-based testing
%     (see \Cref{infers}). \ifnotpaper Even assuming this is true, some kinds of
%         requirements do not apply to the code itself so their relevant inferred
%         test approaches are out of scope (see \Cref{phys-req-test,nontech-req-test}).\fi
% \fi
\ifnotpaper\subsubsection{Recording Implicit Information}
\else\subsection{Recording Implicit Information}
\fi\label{imp-info}

As described in \Cref{explicitness}, the use of natural language introduces
significant nuance that we need to document. Keywords such as \impKeywords{}
indicate that information from the literature is \emph{not} explicit. These
keywords often appear directly within the literature, but even when they do
not, we use them to track explicitness in \ourApproachGlossary{} to
provide a more complete summary of the state of software testing literature
without getting distracted by less relevant details. We find the following
non-mutually exclusive cases of implicit information from the literature:

\begin{enumerate}
    \item \textbf{The information follows logically} from the source and
          information from others, but is not explicitly stated.
    \item \textbf{The information is not universal} but still applies in
          certain cases. \ifnotpaper
    \item \textbf{The information is conditional}, requiring certain
          prerequisites to be satisfied (a more specific case of information
          not being universal). \fi
    \item \textbf{The information is dubious}; while it is present in the
          literature, there is reason to doubt its accuracy.
\end{enumerate}

% %% Maybe convert to \paragraph ?
% \begin{description}
%     \item[1. The information follows logically.]
%           \phantomsection{}\label{imp-case-one}~\\
%           \citeauthor{Firesmith2015} \citeyearpar[pp.~53\==58]{Firesmith2015}
%           lists a set of test approaches that are ``based on the[ir] associated
%           quality characteristic[s] and \dots{} associated quality attributes''
%           \citetext{p.~53}. This matches our definition of ``test type'', but
%           this term is used more loosely in this document to refer to different
%           kinds of testing that we would call ``test approaches'' (see
%           \Cref{cats-def}). We therefore consider \citeauthor{Firesmith2015}'s
%           categorizations of ``test type'' to be implicit\ifnotpaper\
%               (such as in \Cref{tab:multiCats,tab:infMultiCats})\fi.
%     \item [2. The information is not universal.]
%           \phantomsection{}\label{imp-case-two}~\\
%           \refHelper \citet[p.~372\ifnotpaper, emphasis added\fi]{IEEE2017}%
%           \todo{OG ISO/IEC, 2014} \multiAuthHelper{define} ``regression
%           testing'' as ``testing required to determine that a change to a
%           system component has not adversely affected \emph{functionality,
%               reliability or performance} and has not introduced additional
%           defects''. While reliability testing, for example, is not
%           \emph{always} a subset of regression testing (since it may be
%           performed in other ways), it \emph{can be} accomplished by regression
%           testing. This means that these parent-child relations between these
%           pairs of approaches only exist \emph{sometimes}, but this qualifier
%           is given implicitly.
%           %   \citet[p.~5\=/8\ifnotpaper, emphasis added\fi]{SWEBOK2024} provides a
%           %   similar list: ``regression testing \dots{} \emph{may} involve
%           %   functional and non-functional testing, such as reliability,
%           %   accessibility, usability, maintainability, conversion, migration, and
%           %   compatibility testing.''
%     \item[3. The information is \ifnotpaper conditional\else dubious\fi.]
%           \phantomsection{}\label{imp-case-three}~\\ \ifnotpaper
%           As a more specific case of information not being universal, sometimes
%           prerequisites must be satisfied for information to apply. For example,
%           branch condition combination testing is equivalent to (and is therefore
%           implied to be a synonym of) exhaustive testing \emph{if} ``each
%           subcondition is viewed as a single input'' \citep[p.~464]{PetersAndPedrycz2000}.
%           Likewise, statement testing can be used for (and is therefore implied
%           to be a child of) unit testing \emph{if} there are ``less than 5000
%           lines of code'' \citetext{p.~481\todo{OG Miller et al., 1994}}.
%     \item[4. The information is dubious.]
%           \phantomsection{}\label{imp-case-four}~\\ \fi
%           This happens when there is reason to doubt the information provided.
%           If a source claims one thing that is not true, related claims lose
%           credibility. For example, \redBoxFlaw*{}
%           %   Doubts such as this
%           %   can also originate from other sources. \refHelper
%           %   \citet[p.~48]{Kam2008} gives ``user scenario testing'' as a synonym
%           %   of ``use case testing'', even though ``an actor [in use case testing]
%           %   can be \dots{} another system'' \citep[p.~20]{IEEE2021c}, which does
%           %   not fit as well with the label ``user scenario testing''. However,
%           %   since a system can be seen as a ``user'' of the test item, this
%           %   synonym relation is treated as implicit instead of as an objective
%           %   flaw.
% \end{description}

When we encounter information that meets one of these criteria, we use an
appropriate keyword to capture this nuance in \ourApproachGlossary{}\ifnotpaper\
    (see \Cref{tab:impCaseKeywords})\fi. This also helps us identify implicit
information when performing later analysis. Despite ``implicit'' only
describing the first of these cases, we use it (as well as ``implied by'' when
describing sources of information) as a shorthand for all ``not explicit''
manifestations throughout this \docType{} for clarity.

\ifnotpaper
    \input{assets/tables/impCaseKeywords}

    Regarding the last entry in \Cref{tab:impCaseKeywords}, if a test approach in
    \ourApproachGlossary{} has a name ending in ``~(Testing)'' (space
    included), then the word ``Testing'' might not be part of its name
    \emph{or} it might not be a test approach at all! For example, the term
    ``legacy system integration'' is used in \ifnotpaper
        \citeauthor{Gerrard2000a} (\citeyear[pp.~12\==13, Tab.~2]{Gerrard2000a};
        \citeyear[Tab.~1]{Gerrard2000b})\else
        \cite[pp.~12\==13, Tab.~2]{Gerrard2000a},
        \cite[Tab.~1]{Gerrard2000b}\fi, but the more accurate
    ``legacy system integration testing'' is used in
    \citeyearpar[pp.~30\==31]{Gerrard2000b}. In other cases where a
    term is \emph{not} explicitly labelled as ``testing'', we add the
    suffix ``~(Testing)'' (when it makes sense to do so) and consider
    the test approach to be implied.

    \clearpage
    \subsubsection{Recording Flaws}\label{record-flaws}
    While we can detect some subsets of flaws automatically by analyzing
    \ourApproachGlossary{} (see \Cref{auto-flaw-detect}), most
    are too complex and need to be tracked manually. We record these more
    detailed flaws as \LaTeX{} enumeration items in a separate document along
    with comments (described in \Cref{flaw-comment-syntax}) to help us analyze
    these flaws. These comments contain information such as the flaw's
    manifestation (defined in \Cref{mnfst-def}), domain (defined in
    \Cref{dmn-def}), and source(s) responsible.

    \phantomsection{}\label{multi-view-flaws}
    It is important to note that some flaws are multi-faceted, and thus can be
    viewed in multiple ways. For example,\seeRefMissing*{} This is trivially an
    example of a missing definition, but it is also an example of incorrect
    traceability information. Therefore, we include one comment line for each
    % one for \texttt{(WRONG, TRACE)} and one for \texttt{(MISS, DEFS)}
    view\thesisissueref{157}.
    The comment that appears first determines where this flaw is displayed,
    although all pairs are counted as their own flaws; we display this flaw
    as an example of incorrect traceability information as
    \flawref{see-ref-missing} since we determine this to be more meaningful.
\fi

\subsection{Undefined Terms}\label{undef-terms}

The literature mentions many software testing terms without defining them.
While this includes test approaches, software qualities, and more general
software terms, we focus on the former as the main focus of our research.
In particular, \ifnotpaper \citet{IEEE2022} and \citet{Firesmith2015} \else
    \cite{Firesmith2015} and \cite{IEEE2022} \fi name many undefined test
approaches. Once we exhaust the standards in \Cref{stds}, we
perform miniature literature reviews on these subsets to ``fill in'' the
missing definitions (along with any relations), essentially ``snowballing''
on these terms as described in \Cref{ident-sources}. This process uncovers
even more approaches, on which we can then repeat this process. \ifnotpaper
    While we are only able to do this partially due to time constraints, we
    outline what we accomplished and potential next steps in
    \Cref{future-undef-terms}. \fi

\subsection{Stopping Criteria}\label{stop-crit}

Unfortunately, we cannot continue looking for new approaches indefinitely. We
therefore need a ``stopping criteria'' to let us know when we are ``finished''
looking for test approaches in the literature. Our original plan was to repeat
step~\ref{step:repeat-process} until we got diminishing returns, implying that
we achieved something close to a complete taxonomy! However, this was infeasible
due to time constraints. We also considered the discovery of property-based testing
as a potential stopping point for our research\thesisissueref{57,81,88,125}%
\qtodo{These issue refs might be useful in our final documents.} as we
were surprised that it is not mentioned by any sources we investigated. Due to
time constraints, we had to stop our snowballing approach before we added
property-based testing to \ourApproachGlossary{}. With more time, we would find
definitions for all uncovered terms as described in \Cref{undef-terms},
\ifnotpaper as well as those that did not arise as described in
    \Cref{future-miss-terms}, \fi but unfortunately, we impose our stopping
point artificially.
