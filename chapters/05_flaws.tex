\section{Observed Flaws}\label{flaws}

% Moved earlier to display nicely in paper
% \flawMnfstsTable{}
% \flawDmnsTable{}

After gathering all these data\footnote{Available in \texttt{ApproachGlossary.csv},
    \texttt{QualityGlossary.csv}, and \texttt{SuppGlossary.csv} at \ifblind{
        [Repository link suppressed]}{
        \url{https://github.com/samm82/TestingTesting}}.}, we find many
flaws. \Cref{fig:flawBars} shows the source tiers (see \Cref{source-tiers})
responsible for these flaws, which reveals a lot about software testing literature:
\begin{enumerate}
    \item Established standards (\Cref{stds}) aren't actually standardized, since:
          \begin{enumerate}
              \item other documents disagree with them \emph{very} frequently and
              \item they are the most internally inconsistent source tier!
          \end{enumerate}
    \item Less standardized (or ``credible''; see \Cref{cred}) documents,
          such as terminology collections and textbooks (\Cref{metas,texts},
          respectively) are also not followed to the extent they should be.
    \item Documents across the board have flaws within the same document,
          between documents with the same author(s), or even with reality%
          \qtodo{Is this too strong of a synonym for ``ground truth''?}!
\end{enumerate}

\ifnotpaper
    \afterpage{\input{assets/graphs/flawBars}}
    % \input{assets/graphs/flawPies}
\fi

To better understand and analyze these flaws, we group them by their
manifestations and their domains as defined in \Cref{flaw-def}.
We present the total number of flaws by manifestation and by domain
in \Cref{tab:flawMnfsts,tab:flawDmns}, respectively, where a given
row corresponds to the number of flaws either within that source tier and/or
with a ``more credible'' one (i.e., a previous row in the table; see
\Cref{cred,source-tiers}). We also group these flaws by their explicitness
(defined in \Cref{explicitness}) by counting (Obj)ective and (Sub)jective flaws
separately (see \Cref{tab:flawMnfsts,tab:flawDmns}), since additional
context may rectify them.
\ifnotpaper Additionally, \Cref{tab:parSyns,tab:multiCats,tab:infMultiCats}
    contain relations that are explicit, implicit, and both; implicit relations
    are marked by the phrases ``implied by''. \fi

Since we give each flaw a manifestation \emph{and} a domain, the totals per
source and grand totals in these tables are equal. As an example, \tourFlaw*{}.
This is a contradictory definition, so it appears \ifnotpaper as both \else in
\fi \flawref{tour-def-contra,tour-def}. Within these sections, we \ifnotpaper
    list ``more significant'' flaws first, followed by ``less significant''
    ones we omit from the paper version of this thesis. We \else
    omit ``less significant'' flaws for brevity, \fi then sort these flaws
by their source tier (see \Cref{source-tiers}).

We only list flaws we automatically uncover based on their domain \ifnotpaper
    (see \Cref{auto-flaw-analysis}) \fi in their corresponding
domain section for clarity, although they still contribute to the counts in
\Cref{tab:flawMnfsts}. Moreover, certain ``subsets'' of testing contain many
interconnected flaws, which we present in subsections as a ``third view''. This
keeps related information together but causes a further apparent mismatch
between the counts in \Cref{tab:flawMnfsts,tab:flawDmns} and the number of
flaws in \Cref{flawMnfsts,flawDmns}; we still include these flaws in the
manifestation and domain counts\ifnotpaper\ (\Cref{aug-flaw-analysis} outlines
    how this is done)\fi. These subsets include functional testing
(\Cref{func-test-flaw}), \ifnotpaper operational (acceptance) testing
    (\Cref{oat-flaw}), \fi recovery testing (\Cref{recov-flaw}), scalability
testing (\Cref{scal-flaw}), and compatibility testing (\Cref{compat-flaw}).
\ifnotpaper Finally, we present inferred flaws, as described in \Cref{infers},
    for completeness in \Cref{infer-flaws}; these do not contribute to any
    counts. % since they require further analysis to determine
    % if they are flaws at all!
    We colour inferred approaches and relations grey in our generated graphs,
    such as \recFigs{}.

    % \input{build/flawTable}

    \begin{landscape}
        \flawMnfstsTable{}
        \flawDmnsTable{}
    \end{landscape}

\fi

\subsection{Flaws by Manifestation}\label{flawMnfsts}

The following sections list observed flaws grouped by \emph{how} they manifest
as presented in \Cref{mnfst-def}. These include mistakes (\Cref{wrong}),
omissions (\Cref{miss}), contradictions (\Cref{contra}), ambiguities
(\Cref{ambi}), overlaps (\Cref{over}), and redundancies (\Cref{redun}).

\subsubsection{Mistakes}\label{wrong}

There are many ways that information can be incorrect which we identify
in \Cref{tab:brkdwnWrong}. We provide an example below for those that are
unintuitive\ifnotpaper; see \Cref{wrong-full} for the full list of mistakes\fi.

\input{assets/tables/brkdwnWrong}

\paragraph{Information is incorrect based on an assertion from another source}
\errorGuessFlaw{}

\paragraph{Information is provided with an incorrect scope}
\parSheetTestFlaw{} Conversely, \tolTestFlaw*{}

\paragraph{Incorrect information makes other information incorrect}
\redBoxFlaw{}

\subsubsection{Omissions}\label{miss}
We find seven cases where a definition (see \Cref{defs}) is omitted\ifnotpaper,
which we list in \Cref{miss-full}\fi.

\subsubsection{Contradictions}\label{contra}
There are many cases where multiple sources of information (sometimes within
the same document!) disagree. We find this happen with four categories, six
synonym relations, four parent-child relations, sixteen definitions, and two
labels. \ifnotpaper These can be found in \Cref{contra-full}.\fi

\subsubsection{Ambiguities}\label{ambi}
Some information given in the literature is unclear; there is definitely
something ``wrong'', but we cannot deduce the intent of the original author(s).
We identify the kinds of ambiguous information given in \Cref{tab:brkdwnAmbi}%
\ifnotpaper; see \Cref{ambi-full} for the full list of ambiguities\fi.

\input{assets/tables/brkdwnAmbi}

\subsubsection{Overlaps}\label{over}
While information given in the literature should be atomic, this is not
always the case. We find three definitions that overlap, three terms with
multiple definitions, and three terms that share acronyms. \ifnotpaper We list
    these in \Cref{over-full}; note that we track two of the terms with
    multiple definitions as the same flaw since they are related. \fi

\subsubsection{Redundancies}\label{redun}
We find redundancies in two terms, one definition, and one parent-child
relation\ifnotpaper\ as listed in \Cref{redun-full}\fi.

\subsection{Flaws by Domain}\label{flawDmns}

The following sections list observed flaws grouped by \emph{what} information
is flawed as presented in \Cref{dmn-def}. This includes test approach
categories (\Cref{cats}), synonym relations (\Cref{syns}), parent-child
relations (\Cref{pars}), definitions (\Cref{defs}), labels (\Cref{labels}),
scope (\Cref{scope}), and traceability information (\Cref{trace}).

\subsubsection{Approach Category Flaws}\label{cats}

While the IEEE categorization of testing approaches described in
\Cref{tab:ieeeCats} is useful, it is not without its faults. One issue,
which is not inherent to the categorization itself, is the fact that it is not
used consistently\ifnotpaper\ (see \Cref{tab:otherCats})\fi. The most
blatant example of this is that \ifnotpaper \else ISO/IEC and IEEE \fi
\citet[p.~286]{IEEE2017} describe mutation testing as a
methodology, even though this is not one of the categories \emph{they} created!
Additionally, the boundaries between approaches within a category may
be unclear: ``although each technique is defined independently of all others,
in practice [sic] some can be used in combination with other techniques''
\citep[p.~8]{IEEE2021}. For example, ``the test coverage items derived by
applying equivalence partitioning can be used to identify the input parameters
of test cases derived for scenario testing'' \citetext{p.~8}. Even the categories
themselves are not consistently defined, and some approaches are categorized
differently by different sources; these differences are tracked so
they can be analyzed more systematically\thesisissueref{21}.

\input{build/flawDmnCats}

% Moved here to display nicely in paper
\ifnotpaper\else\input{assets/misc/multiCats}\fi

\phantomsection{}\label{multiCats}

Some category flaws can be detected automatically, such as test
approaches with more than one category. These are given in \Cref{tab:multiCats}
and include experience-based testing, which is of particular note.
\expBasedCatMain{} These authors say ``experience-based testing practices like
exploratory testing \dots\ are not \dots\ techniques for designing test cases'',
although they ``can use \dots\ test techniques'' \citeyearpar[p.~viii]{IEEE2021},
which they support in \citeyearpar[p.~33]{IEEE2022} along with scripted testing.
This implies that ``experience-based test design techniques'' are used \emph{by}
the practice of experience-based testing which is not \emph{itself} a test
technique (and similarly with scripted testing). If this is the case, it blurs
the line between ``practice'' and ``technique'', which may explain why
experience-based testing is categorized inconsistently in the literature%
\thesisissueref{64}.

\ifnotpaper
    \phantomsection{}\label{classFamilyFlaw}
    However, this might mean that a practice such as experience-based testing
    can be viewed as a ``class of test case design techniques''
    \citep[p.~4]{IEEE2022}. If \emph{this} is the case, then test approaches
    that are a collection of specific subtechniques may be considered
    practices. The following test approaches are each described as a ``class'',
    ``family'', or ``collection'' of techniques by the sources given, which
    seems to support this:
    \begin{itemize}
        \item Combinatorial testing (\citealp[p.~3]{IEEE2022};
              \citeyear[p.~2]{IEEE2021}; \citealp[p.~5-11]{SWEBOK2024})
        \item Data flow testing (\citeyear[p.~3]{IEEE2021};
              implied by \citealp[p.~5-13]{SWEBOK2024})
        \item Performance(-related) testing (\citealp[p.~38]{IEEE2021};
              \perfAsFamily*{})
        \item Security testing \citep[implied by][p.~40]{IEEE2021}
        \item Fault tolerance testing \citep[implied by][p.~4\=/11]{SWEBOK2024}
    \end{itemize}
    Of the above, security testing is an outlier, since it is consistently
    categorized as a test type (\citealp[pp.~9, 22, 26--27]{IEEE2022};
    \citeyear[pp.~7, 40, Tab.~A.1]{IEEE2021}; \citeyear[p.~405]{IEEE2017}%
    \todo{OG 2013}; implied by its quality (\citealp{ISO_IEC2023a};
    \citealp[p.~13-4]{SWEBOK2024}); \citealp[p.~53]{Firesmith2015}) despite
    consisting of ``a number of techniques'' \cite[p.~40]{IEEE2021}, although
    this may be \distinctIEEE{technique} In addition, specification-based
    testing and structure-based testing may also be considered ``families''
    since they are quite broad with many subtechniques and are described as
    ``complementary'' alongside experience-based testing
    \citep[p.~8, Fig.~2]{IEEE2021}.
\fi

Subapproaches of experience-based tesing, such as error guessing and
exploratory testing, are also categorized ambiguously, causing confusion on how
categories and parent-child relations (see \Cref{par-chd-rels}) interact.
\refHelper \citet[p.~34\ifnotpaper, emphasis added\fi]{IEEE2022}
\multiAuthHelper{say} that a previous standard \citeyearpar{IEEE2021}
``describes the experience-based test \emph{design technique} of error
guessing. Other experience-based test \emph{practices} include (but are not
limited to) exploratory testing \dots, tours, attacks, and checklist-based
testing''. This seems to imply that error guessing is both a technique
\emph{and} a practice, which does not make sense if these categories are
orthogonal. \ifnotpaper Similarly, the conflicting categorizations of beta
    testing in \Cref{tab:multiCats} may propagate to its children closed beta
    testing and open beta testing. Since we infer this (see \Cref{infers}), we
    omit it from that table and instead include it in \Cref{tab:infMultiCats}.
\fi These kinds of inconsistencies between parent and child test approach
categorizations may indicate that categories are not transitive or that more
thought must be given to them.

\ifnotpaper
    \begin{landscape}
        \input{build/multiCats}
    \end{landscape}
\else % Not yet present in paper
\fi

\subsubsection{Synonym Relation Flaws}\label{syns}

While synonyms do not inherently signify a flaw (as we discuss in
\Cref{syn-rels}), the software testing literature is full of incorrect and
ambiguous synonyms that do. As described in \Cref{relevantSyns}, we pay
special attention to synonyms between independently defined approaches (which
may be flaws) and to intransitive synonyms (which definitely \emph{are} flaws).
\ifnotpaper We present explicit (see \Cref{explicitness}) synonym relations that
    fit either of these criteria in \Cref{fig:expSynGraph}\utd{}, which we
    automatically generate from \ourApproachGlossary{} and manually modify for
    legibility. These relations are given as described by the literature and are
    therefore flawed.

    \begin{figure}[b!]
        \centering
        \includegraphics[width=\textwidth]{assets/graphs/manual/expSynLegend.pdf}
        \includegraphics[width=\textwidth]{assets/graphs/manual/expSynGraph.pdf}
        % \vspace{-7mm}
        \caption{Visually meaningful synonym relations given explicitly by the
            literature.}\label{fig:expSynGraph}
    \end{figure}

    We provide the full list of synonyms that violate transitivity (along with
    their sources) in \Cref{multiSyns} and discuss and other kinds of flawed
    synonym relations in \Cref{parSyns,func-test-flaw,scal-flaw,compat-flaw}.
\fi We also observe and manually track the following synonym flaws:

\input{build/flawDmnSyns}

% Moved here to display nicely in paper
\ifnotpaper\else\input{assets/misc/parSyns}\fi

\subsubsection{Parent-Child Relation Flaws}\label{pars}

Parent-child relations (defined in \Cref{par-chd-rels}) are also not immune to
flaws\ifnotpaper, such as the following: \input{build/flawDmnPars} \par \else;
for example, \perfSecParFlaw*{}\fi \phantomsection{}\label{selfPars}%
Additionally, some self-referential definitions imply that a test
approach is a parent of itself. Since these are by nature self-contained within
a given source, these are counted \emph{once} as objective flaws within
their sources in \Cref{tab:flawMnfsts,tab:flawDmns}. \ifnotpaper We identify
    the following \selfParCount{} examples through automatic analysis of our
    generated graphs (see \Cref{selfParDef}):
    \input{build/selfPars} Interestingly, performance testing is \emph{not}
    described as a subapproach of usability testing by \citep{Gerrard2000a,
        Gerrard2000b}, which would have been more meaningful information to
    capture. \else For example, performance and usability testing are both
    given as subapproaches of themselves \cite[Tab.~2]{Gerrard2000a},
    \cite[Tab.~1]{Gerrard2000b}.\fi

\phantomsection{}\label{parSyns}
There are also pairs of synonyms where one is described as a subapproach
of the other, abusing the meaning of ``synonym'' and causing confusion.
We identify \parSynCount{} of these pairs through automatic analysis of the
generated graphs, \ifnotpaper which are \else with the most prominent \fi
given in \Cref{tab:parSyns}\ifnotpaper\ (additional pairs where we infer a flaw
    are given in \Cref{infParSyns} for completeness)\fi. Of
particular note is the relation between path testing and exhaustive testing.
While \citet[p.~421]{vanVliet2000} claims that path testing done completely
``is equivalent to exhaustively testing the program''\footnote{The
    contradictory definitions of path testing given in \flawref{path-test}
    add another layer of complexity to this claim.}, this overlooks the effects
of input data \ifnotpaper
    (\citealp[pp.~129--130]{IEEE2021}; \citealp[p.~121]{Patton2006};
    \citealp[p.~467]{PetersAndPedrycz2000})
\else
    \cite[p.~121]{Patton2006}, \cite[p.~129]{IEEE2021},
    \cite[p.~467]{PetersAndPedrycz2000}
\fi and implementation issues \citetext{p.~476} % \citep[p.~476]{PetersAndPedrycz2000}
on the code's behaviour. Exhaustive testing
requires ``all combinations of input values \emph{and} preconditions \dots{}
[to be] tested'' \ifnotpaper (\citealp[p.~4, emphasis added]{IEEE2022};
    similar in \citealpISTQB{}; \citealp[p.~121]{Patton2006})\else
    \cite[p.~4]{IEEE2022} (similar in \citealpISTQB{},
    \cite[p.~121]{Patton2006})\fi.
% Flaw count (WRONG, SYNS): {vanVliet2000} | {IEEE2022} ISTQB {Patton2006}

\ifnotpaper
    \begin{landscape}
        \input{build/parSyns}
    \end{landscape}
\else % Moved earlier to display nicely in paper
\fi

\subsubsection{Definition Flaws}\label{defs}

Perhaps the most interesting category for those seeking to understand how to
apply a given test approach, there are many flaws with how test
approaches, as well as supporting terms, are defined:

\input{build/flawDmnDefs}

% TODO: re-investigate this after going through the rest of ISO/IEC/IEEE 29119
\ifnotpaper
    Also of note: \citep{IEEE2022, IEEE2021}, from the
    ISO/IEC/IEEE 29119 family of standards, mention the following 23 test
    approaches without defining them. This means that out of the 114 test
    approaches they mention, about 20\% have no associated definition!

    However, the previous version of this standard, \citeyearpar{IEEE2013},
    generally explained two, provided references for two, and explicitly defined
    one of these terms, for a total of five definitions that could (should) have
    been included in \citeyearpar{IEEE2022}! These terms have been
    \underline{underlined}\ifnotpaper%
        , \emph{italicized}, and \textbf{bolded}, respectively%
    \fi. Additionally, entries marked with an asterisk* were defined (at least
    partially) in \citeyearpar{IEEE2017}, which would have been available when
    creating this family of standards. These terms bring the total count of terms
    that could (should) have been defined to nine; almost 40\% of undefined test
    approaches could have been defined!

    \begin{itemize}
        \item \underline{Acceptance Testing*}
        \item Alpha Testing*
        \item Beta Testing*
        \item Capture-Replay Driven Testing
        \item Data-driven Testing
        \item Error-based Testing
        \item Factory Acceptance Testing
        \item Fault Injection Testing
        \item Functional Suitability Testing (also mentioned but not defined in
              \citep{IEEE2017})
        \item \underline{Integration Testing}*
        \item Model Verification
        \item Operational Acceptance Testing
        \item Orthogonal Array Testing
        \item Production Verification Testing
        \item Recovery Testing* (Failover/Recovery Testing, Back-up/Recovery
              Testing, \formatPaper{\textbf}{Backup and Recovery Testing*},
              Recovery*; see \Cref{recov-flaw})
        \item Response-Time Testing
        \item \formatPaper{\emph}{Reviews} (ISO/IEC 20246) (Code Reviews*)
        \item Scalability Testing (defined as a synonym of ``capacity
              testing''; see \Cref{scal-flaw})
        \item Statistical Testing
        \item System Integration Testing (System Integration*)
        \item System Testing* (also mentioned but not defined in \citep{IEEE2013})
        \item \formatPaper{\emph}{Unit Testing*}
              (IEEE Std 1008-1987, IEEE Standard for
              Software Unit Testing implicitly listed in the bibliography!)
        \item User Acceptance Testing
    \end{itemize}
\fi

\subsubsection{Label Flaws}\label{labels}

While some flaws exist because the definition of a term is wrong,
others exist because a term's \emph{name} or \emph{label} is wrong!
\defLabelDistinct{}, which we define in \Cref{label-flaw-def}.
We observe the following label flaws:

\input{build/flawDmnLabels}

\subsubsection{Scope Flaws}\label{scope}

Some information is given that is technically correct, but it is either given
too narrowly, too broadly, or when it should not have been (i.e., it is
irrelevant)\thesisissueref{182}! We observe the following scope flaws as
defined in \Cref{scope-flaw-def}:

\input{build/flawDmnScope}

\subsubsection{Traceability Flaws}\label{trace}

\ifnotpaper
    The following are examples of traceability flaws, as defined in
    \Cref{trace-flaw-def}:
    \input{build/flawDmnTrace}
\else
    Sometimes a document cites another for a piece of information that does not
    appear! For example, \citet[p.~184]{DoÄŸanEtAl2014} \multiAuthHelper{claim}
    that \citet{SakamotoEtAl2013} \multiAuthHelper{define} ``prime path
    coverage'', but it does not.
\fi

\subsection{Functional Testing}\label{func-test-flaw}

``Functional testing'' is described alongside many other, likely related,
terms. This leads to confusion about what distinguishes these terms, as shown
by the following five:

\subsubsection{Specification-based Testing}\label{spec-func-test}
This is defined as ``testing in which the principal test basis is the external
inputs and outputs of the test item'' \citep[p.~9]{IEEE2022}. This agrees
with a definition of ``functional testing'': ``testing that
\dots\ focuses solely on the outputs generated in response to
selected inputs and execution conditions'' \citep[p.~196]{IEEE2017}.
\todo{\citet[p.~399]{vanVliet2000} may list these as synonyms; investigate}
Notably, \citet{IEEE2017} lists both as synonyms of
``black-box testing'' \citetext{pp. 431, 196, respectively}, despite them
sometimes being defined separately. For example, the \acf{istqb} defines
``specification-based testing'' as ``testing based on an analysis of the
specification of the component or system'' \ifnotpaper (and gives ``black-box
    testing'' as a synonym) \fi and ``functional testing'' as ``testing
performed to evaluate if a component or system satisfies functional
requirements'' \ifnotpaper (specifying no synonyms) \citepISTQB{};
    % Flaw count (CONTRA, SYNS): {IEEE2022} {IEEE2017} | ISTQB
    the latter references \citet[p.~196]{IEEE2017}
    (``testing conducted to evaluate the compliance of a system or
    component with specified functional requirements'') which
    \emph{has} ``black-box testing'' as a synonym, and mirrors
    \citet[p.~21]{IEEE2022} (testing ``used to check the implementation
    of functional requirements'')\else \cite{ISTQB}\fi. Overall,
specification-based testing \citep[pp.~2-4,~6-9,~22]{IEEE2022} \ifnotpaper and
    black-box testing (\citealp[p.~5-10]{SWEBOK2024};
    \citealp[p.~3]{SouzaEtAl2017})
    % \else \cite[p.~3]{SouzaEtAl2017}, \cite[p.~5-10]{SWEBOK2024}
    are test design techniques \else is a test design technique \fi used to
``derive corresponding test cases'' \citep[p.~11]{IEEE2022} from
``selected inputs and execution conditions'' \citep[p.~196]{IEEE2017}.

\subsubsection{Correctness Testing}
\label{corr-func-test}
\refHelper \citet[p.~5-7]{SWEBOK2024} says ``test cases can be designed to
check that the functional specifications are correctly implemented, which is
variously referred to in the literature as conformance testing, correctness
testing or functional testing''; this mirrors previous definitions
of ``functional testing'' \ifnotpaper
    (\citealp[p.~21]{IEEE2022}; \citeyear[p.~196]{IEEE2017})
\else
    \cite[p.~21]{IEEE2022}, \cite[p.~196]{IEEE2017}
\fi but groups it with ``correctness
testing''. Since ``correctness'' is a software quality \ifnotpaper
    (\citealp[p.~104]{IEEE2017}; \citealp[p.~3-13]{SWEBOK2024}) \else
    \cite[p.~104]{IEEE2017}, \cite[p.~3-13]{SWEBOK2024} \fi which is
what defines a ``test type'' \citep[p.~15]{IEEE2022}\ifnotpaper\
    (see \Cref{qual-supp-procedure})\fi,
it seems consistent to label ``functional testing'' as a ``test type''
\ifnotpaper
    \citetext{\citealp[pp.~15,~20,~22]{IEEE2022};
        \citeyear[pp.~7,~38,~Tab.~A.1]{IEEE2021}; \citeyear[p.~4]{IEEE2016}}%
\else
    \cite[pp.~15,~20,~22]{IEEE2022}, \cite[pp.~7,~38,~Tab.~A.1]{IEEE2021},
    \cite[p.~4]{IEEE2016}\fi. However, this conflicts with its categorization
as a ``technique'' if considered a synonym of specification-based testing
(see \Cref{spec-func-test}).
% Flaw count (CONTRA, CATS): {IEEE2022} {IEEE2021} {IEEE2016} implied by {IEEE2017} {SWEBOK2024} {IEEE2022} | {IEEE2022} {SWEBOK2024} {SouzaEtAl2017} {IEEE2017}
Additionally, ``correctness testing'' is listed separately from ``functionality
testing'' by \citet[p.~53]{Firesmith2015}.
% Flaw count (CONTRA, SYNS): {SWEBOK2024} | {Firesmith2015}

\subsubsection{Conformance Testing}
Testing that ensures ``that the functional specifications are correctly
implemented'', and can be called ``conformance testing'' or ``functional
testing'' \citep[p.~5-7]{SWEBOK2024}.
``Conformance testing'' is later defined as testing used ``to
verify that the \acs{sut} conforms to standards, rules,
specifications, requirements, design, processes, or practices''
\citep[p.~5-7]{SWEBOK2024}. This definition seems to be a superset
of testing methods mentioned earlier as the latter includes ``standards,
rules, requirements, design, processes, \dots\ [and]'' practices in
\emph{addition} to specifications!
% Flaw count (OVER, SYNS): {SWEBOK2024} | implied by {SWEBOK2024}

A complicating factor is that ``compliance testing'' is also
(plausibly) given as a synonym of ``conformance testing''
\citep[p.~43]{Kam2008}. However, ``conformance
testing'' can also be defined as testing that evaluates the degree
to which ``results \dots\ fall within the limits that define
acceptable variation for a quality requirement''
\citep[p.~93]{IEEE2017}\todo{OG PMBOK 5th ed.}, which seems to
describe something different.
% Flaw count (AMBI, SYNS): {Kam2008} | implied by {IEEE2017}

% TODO: pull out into Recommendations
% Perhaps this second definition of
% ``conformance testing'' should be used, and the previous definition
% of ``compliance testing'' should be used for describing compliance with
% external standards, rules, etc.~to keep them distinct.

\subsubsection{Functional Suitability Testing}
Procedure testing is
called a ``type of functional suitability testing''
\citep[p.~7]{IEEE2022} but no definition of that term is given.
``Functional suitability'' is the
``capability of a product to provide functions that meet stated and
implied needs of intended users when it is used under specified
conditions'', including meeting ``the functional specification''
\citep{ISO_IEC2023a}. This seems to align with the definition of
``functional testing'' as related to ``black-box/%
specification-based testing''.
\ifnotpaper
    ``Functional suitability'' has
    three child terms: ``functional completeness'' (the ``capability of
    a product to provide a set of functions that covers all the
    specified tasks and intended users' objectives''), ``functional
    correctness'' (the ``capability of a product to provide accurate
    results when used by intended users''), and ``functional
    appropriateness'' (the ``capability of a product to provide
    functions that facilitate the accomplishment of specified tasks and
    objectives'') \citep{ISO_IEC2023a}. Notably, ``functional
    correctness'', which includes precision and accuracy
    (\citealp{ISO_IEC2023a}; \citealpISTQB{}), \else ``Functional
    correctness'', a child of ``functional suitability'', is the ``capability
    of a product to provide accurate results when used by intended users''
    \cite{ISO_IEC2023a} and \fi seems to align with
the quality/ies that would be tested by ``correctness'' testing.

\subsubsection{Functionality Testing}
``Functionality'' is defined as the
``capabilities of the various \dots\ features provided by a product''
\citep[p.~196]{IEEE2017} and is said to be a synonym of
``functional suitability'' \citepISTQB{}, although it seems
like it should really be a synonym of ``functional completeness'' based on
\citep{ISO_IEC2023a}, which would make ``functional suitability'' a
% Flaw count (CONTRA, SYNS): ISTQB | implied by {ISO_IEC2023a}
subapproach. Its associated test type
is implied to be a subapproach of build verification testing
\citepISTQB{} and made distinct from ``functional testing''%
\ifnotpaper; interestingly, security is described as a subapproach of both
non-functional and functionality testing\fi\ \citep[Tab.~2]{Gerrard2000a}.
``Functionality testing'' is listed separately from ``correctness testing'' by
\citet[p.~53]{Firesmith2015}.

\ifnotpaper
    \subsection{Operational (Acceptance) Testing}\label{oat-flaw}
    \paragraph{\texttt{(CONTRA, LABELS)}}
    % Flaw count (CONTRA, LABELS): {IEEE2022} ISTQB | {SWEBOK2024} {ISO_IEC2018} {IEEE2017} {SWEBOK2014}
    There are two names that the literature gives to this test approach:
    \begin{itemize}
        \item \emph{\acf{operat}} (\citealp[p.~22]{IEEE2022};
              \citealpISTQB{}) and
        \item \emph{\acf{ot}} (\citealp{ISO_IEC2018};
              \citealp[p.~303]{IEEE2017}; \citealp[p.~6\=/9, in the context of
                  software engineering operations]{SWEBOK2024};
              \citealp[pp.~4\=/6, 4\=/9]{SWEBOK2014}).
    \end{itemize}

    \paragraph{\texttt{(CONTRA, SYNS)}}
    % Flaw count (CONTRA, SYNS): {Firesmith2015}
    % Assertion: {LambdaTest2024} {BocchinoAndHamilton1996}
    \refHelper \citet[p.~30]{Firesmith2015} lists the above terms separately,
    but they are considered synonyms elsewhere \citep{LambdaTest2024,
        BocchinoAndHamilton1996}\todo{find more academic sources}; since
    \citeauthor{Firesmith2015} does not define these terms, it is hard to
    evaluate \ifnotpaper his \else its \fi distinction.
\fi

\subsection{Recovery Testing}\label{recov-flaw}

``Recovery testing'' is ``testing \dots\ aimed at verifying
software restart capabilities after a system crash or other disaster''
\citep[p.~5\=/9]{SWEBOK2024} including ``recover[ing] the data directly affected
and re-establish[ing] the desired state of the system'' \ifnotpaper
    (\citealp{ISO_IEC2023a}; similar in \citealp[p.~7\=/10]{SWEBOK2024})
\else \cite{ISO_IEC2023a} (similar in \cite[p.~7\=/10]{SWEBOK2024}) \fi
so that the system ``can perform required functions'' \citep[p.~370]{IEEE2017}.
However, the literature also describes similar test approaches with vague or
non-existent distinctions between them. We describe these approaches and their
flaws here and present the relations between them in \Cref{fig:rec-graph-current}.

\NewDocumentCommand\subDRT{s}{given as a subtype of ``disaster/recovery
    testing'' which tests if ``operation of the test item can
    be transferred to a different operating site''
    \IfBooleanTF{#1}{\citetext{p.~37}}{\citeyearpar[p.~37]{IEEE2021}},
    even though this is \emph{explicitly} excluded from its definition on the
    same page!}

%% again, maybe convert to \paragraph ?
\begin{itemize}
    \item \emph{Recoverability testing} evaluates ``how well a system or
          software can recover data during an interruption or failure''
          \ifnotpaper (\citealp[p.~7\=/10]{SWEBOK2024}; \else
              \cite[p.~7\=/10]{SWEBOK2024} (\fi similar in \citealp{ISO_IEC2023a})
          and ``re-establish the desired state of the system''
          \citeyearpar{ISO_IEC2023a}. \refHelper \citet[p.~47]{Kam2008}
          gives this as a synonym for ``recovery testing''.
    \item \emph{Disaster/recovery testing} evaluates if a system
          can ``return to normal operation after a hardware
          or software failure'' \citep[p.~140]{IEEE2017} or if ``operation of
          the test item can be transferred to a different operating site and
          \dots\ be transferred back again once the failure has been
          resolved'' \citeyearpar[p.~37]{IEEE2021}.
          \begin{itemize}
              \item \texttt{(OVER, DEFS)}
                    % Flaw count (OVER, DEFS): {IEEE2017} | {IEEE2021}
                    These two definitions seem to describe different aspects of
                    the system, where the first is intrinsic to the
                    hardware/software and the second might not be, making this
                    term nonatomic.
          \end{itemize}
    \item \emph{Backup and recovery testing} ``measures the
          degree to which system state can be restored from backup within
          specified parameters of time, cost, completeness, and accuracy in
          the event of failure'' \citep[p.~2]{IEEE2013}. This may be what is
          meant by ``recovery testing'' in the context of performance-related
          testing \citeyearpar[Fig.~2]{IEEE2022}.
    \item \emph{Backup/recovery testing} determines the ability of a system
          ``to restor[e] from back-up memory in the event of failure, without
          transfer[ing] to a different operating site or back-up system''
          \citep[p.~37]{IEEE2021}.
          \begin{itemize}
              %   \item \texttt{(OVER, DEFS)}
              %         % Flaw count (OVER, DEFS): {IEEE2021} | {IEEE2017}
              %         This definition corresponds to the first definition of
              %         ``disaster/recovery testing'' given by
              %         \citeyearpar[p.~140]{IEEE2017}.
              \item \texttt{(CONTRA, PARS)}
                    % Flaw count (CONTRA, PARS): {IEEE2021} | {IEEE2021}
                    This \subDRT{}
              \item \texttt{(OVER, LABELS)}
                    % Flaw count (OVER, LABELS): {IEEE2021} | {IEEE2013}
                    Its name is also quite similar to ``backup and
                    recovery testing'', adding further confusion.
          \end{itemize}
    \item \emph{Failover/recovery testing} determines the
          ability ``to mov[e] to a back-up system in the event of failure,
          without transfer[ing] to a different operating site''
          \citep[p.~37]{IEEE2021}.
          \begin{itemize}
              \item \texttt{(CONTRA, PARS)}
                    % Flaw count (CONTRA, PARS): {IEEE2021} | {IEEE2021}
                    This is also \subDRT*{}
              \item \texttt{(AMBI, PARS)}
                    % Flaw count (AMBI, PARS): {IEEE2021}
                    % Assertion: {SWEBOK2024} ISTQB
                    While not explicitly related to recovery, \emph{failover
                        testing} ``validates the SUT's ability to manage heavy
                    loads or unexpected failure to continue typical operations''
                    \citep[p.~5\=/9]{SWEBOK2024} by entering a ``backup
                    operational mode in which [these responsibilities] \dots\
                    are assumed by a secondary system'' \citepISTQB{}. Its name
                    implies that it is a child of ``failover/recovery testing''
                    but its definition makes it more broad (as it includes handling
                    ``heavy loads'' where failover/recovery testing does not)
                    which may reverse the direction of this relation.
              \item \texttt{(AMBI, SYNS)}
                    % Flaw count (AMBI, SYNS): {IEEE2021} | implied by {Firesmith2015}
                    \refHelper \citet[p.~56]{Firesmith2015} uses the term
                    ``failover and recovery testing'' which may be a synonym of
                    ``failover/recovery testing''.
          \end{itemize}
    \item \emph{Restart \& recovery (testing)} is listed as a test approach by
          \citet[Fig.~5]{Gerrard2000a} but is not defined \texttt{(MISS, DEFS)}
          % Flaw count (MISS, DEFS): {Gerrard2000a}
          and may simply be a synonym to ``recovery testing'' \texttt{(AMBI, SYNS)}.
          % Flaw count (AMBI, SYNS): {Gerrard2000a}
\end{itemize}

\subsection{Scalability Testing}\label{scal-flaw}

\paragraph{\texttt{(CONTRA, SYNS)}}

% Flaw count (CONTRA, SYNS): {IEEE2021} | {Firesmith2015} {Bas2024}
\citeauthor{IEEE2021} \citeyearpar[p.~39]{IEEE2021} give
``scalability testing'' as a synonym of ``capacity testing''
while other sources differentiate between the two \ifnotpaper
    (\citealp[p.~53]{Firesmith2015}; \citealp[pp.~22\==23]{Bas2024})%
\else \cite[p.~53]{Firesmith2015}, \cite[pp.~22\==23]{Bas2024}\fi.

\paragraph{\texttt{(CONTRA, DEFS)}}

% Flaw count (CONTRA, DEFS): {IEEE2021} | implied by {ISO_IEC2023a}
\citeauthor{IEEE2021} \citeyearpar[p.~39]{IEEE2021} also include the external
modification of the system as part of ``scalability'' but
\citet{ISO_IEC2023a} \multiAuthHelper{describe} it as testing the ``capability
of a product to handle growing or shrinking workloads or to adapt its capacity
to handle variability'', implying that this is done by the system itself.

\paragraph{\texttt{(WRONG, LABELS)}}

% Flaw count (WRONG, LABELS): {SWEBOK2024}
\citeauthor{SWEBOK2024} \citeyearpar[p.~5\=/9]{SWEBOK2024} says ``scalability
testing evaluates the capability to use and learn the system and the user
documentation'' and ``focuses on the system's effectiveness in supporting user
tasks and the ability to recover from user errors''. \swebokScalDef{}.

% \subsection{Performance Testing}
% \label{perf-test-ambiguity}

% Similarly, ``performance'' and ``performance efficiency'' are both given as
% software qualities by \ifnotpaper\citeauthor{IEEE2017}\else
%       \cite[p.~319]{IEEE2017}\fi, with the latter defined as the ``performance
% relative to the amount of resources used under stated conditions''
% \ifnotpaper\citeyearpar[p.~319]{IEEE2017} \fi or the ``capability of a product
% to perform its functions within specified time and throughput parameters and be
% efficient in the use of resources under specified conditions'' \citep{ISO_IEC2023a}.
% Initially, there didn't seem to be any meaningful distinction between the two,
% although the term ``performance testing'' is defined
% \ifnotpaper\citeyearpar[p.~320]{IEEE2017}\else\citetext{p.~320}\fi\
% and used by \ifnotpaper\citeauthor{IEEE2017}\else\cite{IEEE2017}\fi\ and the term
% ``performance efficiency testing'' is \emph{also} used by
% \ifnotpaper\citeauthor{IEEE2017}\else\cite{IEEE2017}\fi\ (but not defined
% explicitly). \ifnotpaper Further discussion\thesisissueref{43} brought us to
%       the conclusion \else It can then be concluded \fi that ``performance
% efficiency testing'' is a subset of ``performance testing'', and the
% difference of ``relative to the amount of resources used'' or ``be efficient in
% the use of resources'' between the two is meaningful.

\subsection{Compatibility Testing}\label{compat-flaw}

\paragraph{\texttt{(OVER, DEFS)}}
% Flaw count (OVER, DEFS): {IEEE2022} | {IEEE2017} ISTQB {ISO_IEC2023a}
``Compatibility testing'' is defined as ``testing that measures the
degree to which a test item can function satisfactorily alongside
other independent products in a shared environment (co-existence),
and where necessary, exchanges information with other systems or
components (interoperability)'' \citep[p.~3]{IEEE2022}. This
definition is nonatomic as it combines the ideas of ``co-existence''
and ``interoperability''.

\paragraph{\texttt{(WRONG, SYNS)}}
% Flaw count (WRONG, SYNS): implied by {IEEE2021} | {IEEE2022}
The ``interoperability'' element of ``compatibility testing'' is explicitly
excluded by \citet[p.~37]{IEEE2021}, (incorrectly) implying that ``compatibility
testing'' and ``co-existence testing'' are synonyms.

\paragraph{\texttt{(AMBI, SYNS)}}
% Flaw count (AMBI, SYNS): {Kam2008} | {IEEE2022}
Furthermore, the definition of ``compatibility testing'' in
\citet[p.~43]{Kam2008} unhelpfully says ``see \emph{interoperability testing}'',
adding another layer of confusion to the direction of their relationship.

\paragraph{\texttt{(WRONG, LABELS)}}
% Flaw count (WRONG, LABELS): {IEEE2022}
% Assertion: {ISO_IEC2023a} {IEEE2022} {IEEE2021} {IEEE2017} ISTQB
\refHelper \citet[pp.~22, 43]{IEEE2022} \multiAuthHelper{say}
``interoperability testing helps confirm that applications can work on multiple
operating systems and devices'', but this seems to instead describe
``portability testing'', which evaluates the ``capability of a product to be
adapted to changes in its requirements, contexts of use, or system environment''
\ifnotpaper
    (\citealp{ISO_IEC2023a}; similar in \citealp[p.~7]{IEEE2022};
    \citeyear[pp.~184, 329]{IEEE2017}; \citealpISTQB{})\else
    \cite{ISO_IEC2023a} (similar in \citeyear[pp.~184, 329]{IEEE2017},
    \citealp[p.~7]{IEEE2022}, \cite{ISTQB})\fi, such as being ``transferred
from one hardware \dots{} environment to another'' \citep[p.~39]{IEEE2021}.

\ifnotpaper
    \subsection{Inferred Flaws}\label{infer-flaws}
    Throughout our research, we infer many potential flaws as described in
    \Cref{infers}. Some of these have a conflicting source while others do not.
    Since these are more subjective and are based on our own judgement, we
    exclude them from any counts of the numbers of flaws but give them here for
    completeness.

    \subsubsection{Inferred Synonym Flaws}\label{infMultiSyns}
    See \Cref{multiSyns}.

    \begin{enumerate}
        \input{build/infMultiSyns}
    \end{enumerate}

    Additionally, \citet[p.~46]{Kam2008} gives ``program testing'' as a synonym
    of ``component testing'' but it probably should be a synonym of ``system
    testing'' instead.
    % \item \refHelper \citet[p.~46]{Kam2008} gives ``program testing'' as a
    % synonym of ``component testing'' but it probably should be a synonym of
    % ``system testing'' instead.

    \subsubsection{Inferred Parent Flaws}\label{infParSyns}
    As discussed in \Cref{parSyns}, some pairs of synonyms also have a
    parent-child relation, abusing the meaning of ``synonym'' and causing
    confusion. While \Cref{tab:parSyns} gives the cases where both relations
    are supported by the literature, some are less explicit. For example, while
    ``\emph{dynamic testing} is sometimes called \dots{} dynamic analysis''
    (\citealp[p.~438]{PetersAndPedrycz2000}; implied by
    \citealp[p.~149]{IEEE2017}; \citealpISTQB{}), it could be inferred from
    their static counterparts that dynamic analysis is a \emph{child} of dynamic
    testing (see \citealp[pp.~9, 17, 25, 28]{IEEE2022}). Additionally, the
    following automatically generated lists contain examples where at least
    one of these conflicting relations is \emph{not} supported by the
    literature but may, nonetheless, be correct. The relations in the first two
    lists are explicitly given in the literature but may be incorrect, while
    those in the third list are unsubtantiated by the literature and require
    more thought before a recommendation can be made.

    \input{build/infParSyns}

    In addition to this type of flaw, \citep[Tab.~2]{Gerrard2000a} does
    \emph{not} give ``functionality testing'' as a parent of ``end-to-end
    functionality testing''. Finally, \citet[p.~119]{Patton2006} says that
    branch testing is ``the simplest form of path testing'' which is also
    implied by \citet[Fig.~F.1]{IEEE2021}\todo{OG Reid, 1996} and
    \citet[p.~433]{vanVliet2000}. This is true in the example
    \citeauthor{Patton2006} gives, but is not necessarily generalizable; one
    could test the behaviour at branches without testing even a \emph{subset}
    of complete paths, which \citet[p.~316]{IEEE2017} give as a definition of
    ``path testing'' (see \flawref{path-test})!

    \subsubsection{Inferred Category Flaws}
    See \Cref{multiCats}.

    \input{build/infMultiCats}

    \subsubsection{Other Inferred Flaws}
    The following are flaws that, if were more concrete, would also be
    included alongside the other flaws:
    \begin{itemize}
        \item ``Fuzz testing'' is ``tagged'' (?) as ``artificial
              intelligence'' \citep[p.~5]{IEEE2022}.
        \item \citeauthor{Gerrard2000b}'s definition for ``security
              audits'' seems too specific, only applying to ``the products
              installed on a site'' and ``the known vulnerabilities for
              those products'' \citeyearpar[p.~28]{Gerrard2000b}.
    \end{itemize}
\fi