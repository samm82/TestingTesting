\section{Observed Flaws}\label{flaws}

% Moved earlier to display nicely in paper
% \flawMnfstsTable{}
% \flawDmnsTable{}

After gathering all these data\footnote{Available in \texttt{ApproachGlossary.csv},
    \texttt{QualityGlossary.csv}, and \texttt{SuppGlossary.csv} at \ifblind{
        [Repository link suppressed]}{
        \url{https://github.com/samm82/TestingTesting}}.}, we find many
flaws. \Cref{fig:flawBars} shows the source tiers (see \Cref{source-tiers})
responsible for these flaws, which reveals a lot about software testing literature:
\begin{enumerate}
    \item Established standards (\Cref{stds}) aren't actually standardized, since:
          \begin{enumerate}
              \item other documents disagree with them \emph{very} frequently and
              \item they are the most internally inconsistent source tier!
          \end{enumerate}
    \item Less standardized (or ``credible''; see \Cref{cred}) documents,
          such as terminology collections and textbooks (\Cref{metas,texts},
          respectively) are also not followed to the extent they should be.
    \item Documents across the board have flaws within the same document,
          between documents with the same author(s), or even with reality%
          \qtodo{Is this too strong of a synonym for ``ground truth''?}!
\end{enumerate}

\ifnotpaper
    \afterpage{\input{assets/graphs/flawBars}}
    % \input{assets/graphs/flawPies}
\fi

To better understand and analyze these flaws, we group them by their
manifestations and their domains as defined in \Cref{flaw-def}.
We present the total number of flaws by manifestation and by domain
in \Cref{tab:flawMnfsts,tab:flawDmns}, respectively, where a given
row corresponds to the number of flaws either within that source tier and/or
with a ``more credible'' one (i.e., a previous row in the table; see
\Cref{cred,source-tiers}). We also group these flaws by their explicitness
(defined in \Cref{explicitness}) by counting (Obj)ective and (Sub)jective flaws
separately, since additional context may rectify them.
Since we give each flaw a manifestation \emph{and} a domain, the totals per
source and grand totals in these tables are equal.

We summarize the flaws that we discover manually \ifnotpaper (see
    \Cref{aug-flaw-analysis}) \fi in \Cref{flawMnfsts} based on their
manifestation (defined in \Cref{mnfst-def}). This lets us keep flaws that we
automatically uncover \ifnotpaper (see \Cref{auto-flaw-analysis}) \fi separate;
we summarize these based on their domain (defined in \Cref{dmn-def}) in
\Cref{flawDmns}.
\ifnotpaper We list \emph{all} these flaws in \Cref{flaws-full} to balance
    completeness and brevity and denote implicit relations with the phrase
    ``implied by'' in \Cref{tab:parSyns,tab:multiCats,tab:infMultiCats} as
    described in \Cref{explicitness}. \fi Moreover, certain ``subsets'' of
testing contain many interconnected flaws, which we present in subsections as a
``third view'' to keep related information together. Therefore, the counts of
flaws given in \Cref{tab:flawMnfsts,tab:flawDmns} can be thought of as the sum
the flaws we describe by manifestation, domain, and ``subset''. These subsets
include \ifnotpaper operational (acceptance) testing (\Cref{oat-flaw}), \fi
recovery testing (\Cref{recov-flaw}), scalability testing (\Cref{scal-flaw}),
and compatibility testing (\Cref{compat-flaw}). \ifnotpaper Finally, we also
    infer some flaws as described in \Cref{infers}, which do not contribute to
    any counts due to their subjectivity; we list them in \Cref{infer-flaws}
    for completeness. \fi

\ifnotpaper
    % \input{build/flawTable}

    \begin{landscape}
        \flawMnfstsTable{}
        \flawDmnsTable{}
    \end{landscape}

\fi

\subsection{Flaws by Manifestation}\label{flawMnfsts}

The following sections list observed flaws grouped by \emph{how} they manifest
as presented in \Cref{mnfst-def}. These include mistakes (\Cref{wrong}),
omissions (\Cref{miss}), contradictions (\Cref{contra}), ambiguities
(\Cref{ambi}), overlaps (\Cref{over}), and redundancies (\Cref{redun}).

\subsubsection{Mistakes}\label{wrong}

There are many ways that information can be incorrect which we identify
in \Cref{tab:brkdwnWrong}. We provide an example below for those that are
unintuitive\ifnotpaper; see \Cref{wrong-full} for the full list of mistakes\fi.

\input{assets/tables/brkdwnWrong}

\paragraph{Information is incorrect based on an assertion from another source}
\errorGuessFlaw{}

\paragraph{Information is provided with an incorrect scope}
\parSheetTestFlaw{} Conversely, \tolTestFlaw*{}

\paragraph{Incorrect information makes other information incorrect}
\redBoxFlaw{}

\subsubsection{Omissions}\label{miss}
We find four cases where a definition is omitted and one where a category is
omitted \ifnotpaper which we list in \Cref{miss-full}\fi.

\subsubsection{Contradictions}\label{contra}
There are many cases where multiple sources of information (sometimes within
the same document!) disagree. We find this happen with six categories, five
synonym relations, seven parent-child relations, eighteen definitions, and two
labels. \ifnotpaper These can be found in \Cref{contra-full}.\fi

\subsubsection{Ambiguities}\label{ambi}
Some information given in the literature is unclear; there is definitely
something ``wrong'', but we cannot deduce the intent of the original author(s).
We identify the kinds of ambiguous information given in \Cref{tab:brkdwnAmbi}%
\ifnotpaper; see \Cref{ambi-full} for the full list of ambiguities\fi.

\input{assets/tables/brkdwnAmbi}

\subsubsection{Overlaps}\label{over}
While information given in the literature should be atomic, this is not
always the case. We find three definitions that overlap, two terms with
multiple definitions, and three terms that share acronyms. \ifnotpaper We list
    these in \Cref{over-full}; note that we track two of the terms with
    multiple definitions as the same flaw since they are related. \fi

\subsubsection{Redundancies}\label{redun}
We find redundancies in two parent-child relations, one definition, and two
labels\ifnotpaper\ as listed in \Cref{redun-full}\fi.

\subsection{Flaws by Domain}\label{flawDmns}

The following sections present flaws that we detect automatically \ifnotpaper
    (see \Cref{auto-flaw-analysis}) \fi grouped by \emph{what} information
is flawed as presented in \Cref{dmn-def}. We also provide more detailed
information on specific areas of these domains that may require further
investigation. The domains we focus on here are test approach categories
(\Cref{cats}), synonym relations (\Cref{syns}), and parent-child relations
(\Cref{pars})%\ifnotpaper, and definitions (\Cref{defs})\fi
.

\subsubsection{Approach Category Flaws}\label{cats}

While the IEEE categorization of testing approaches described in
\Cref{tab:ieeeCats} is useful, it is not without its faults. One issue,
which is not inherent to the categorization itself, is the fact that it is not
used consistently\ifnotpaper\ (see \Cref{tab:otherCats})\fi. The most
blatant example of this is that \ifnotpaper \else \citeauthor{IEEE2017} \fi
\citet[p.~286]{IEEE2017} describe mutation testing as a
methodology, even though this is not one of the categories \emph{they} created!
Additionally, the boundaries between approaches within a category may
be unclear: ``although each technique is defined independently of all others,
in practice [sic] some can be used in combination with other techniques''
\citep[p.~8]{IEEE2021c}. For example, ``the test coverage items derived by
applying equivalence partitioning can be used to identify the input parameters
of test cases derived for scenario testing'' \citetext{p.~8}. Even the categories
themselves are not consistently defined, and some approaches are categorized
differently by different sources; we track these differences so we can analyze
them more systematically\thesisissueref{21}.

% Moved here to display nicely in paper
\ifnotpaper\else\input{assets/misc/multiCats}\fi

\phantomsection{}\label{multiCats}
In particular, \multiCatIntro{}. We identify \multiCatCount{} such cases%
\ifnotpaper\ that we summarize in \Cref{tab:multiCatTable}\fi; we consider these
flaws as they violate our assumption of orthogonality (see
\Cref{orth-approach}). We list \ifnotpaper all \else some \fi of these flaws
along with their sources in \Cref{tab:multiCats}\ifnotpaper\ for completeness.
    \begin{wraptable}{r}{6cm}
        \vspace{-0.5cm}
        \input{build/multiCatTable}
        \vspace{-1cm}
    \end{wraptable}
\fi These flaws mostly involve the category of ``test \multiCatMax{}'', which
shows up in \multiCatMaxCount{} of them. This may simply be because authors use
the term ``test technique'' in a more general sense where we would use the term
``test approach''.\phantomsection{}\label{cats-intrans} A more
specific reason for this is how the line between ``test technique'' and
``test practice'' can be blurred when these categories are \emph{not}
transitive. For example, ``some test practices, such as exploratory testing or
model-based testing are sometimes [incorrectly] referred to as `test techniques'
\dots{} as they are not themselves providing a way to create test cases, but
instead use test design techniques to achieve that'' \ifnotpaper
    (\citealp[p.~11]{IEEE2022}; \citeyear[p.~5]{IEEE2021a})\else
    \cite[p.~11]{IEEE2022}, \cite[p.~5]{IEEE2021a}\fi. This is further
supported for exploratory testing \ifnotpaper (\citeyear[p.~33]{IEEE2022};
    \citeyear[p.~viii]{IEEE2021c}; \citeyear[p.~13]{IEEE2013}) \else
    \cite[p.~33]{IEEE2022}, \cite[p.~viii]{IEEE2021c}, \cite[p.~13]{IEEE2013}
\fi as well as for experience-based testing \ifnotpaper
    (\citeyear[p.~4]{IEEE2022}; \citeyear[pp.~viii, 4]{IEEE2021c};
    \citeyear[p.~33]{IEEE2013}) \else \cite[p.~4]{IEEE2022},
    \cite[pp.~viii, 4]{IEEE2021c}, \cite[p.~33]{IEEE2013} \fi and scripted
testing \citeyearpar[p.~33]{IEEE2022}\ifnotpaper\ and \emph{seems} to be
    further supported for model-based testing
    (\citealp[pp.~1\==2]{Engstr√∂mAndPetersen2015}; \citealp[p.~4]{Kam2008})\fi.

\ifnotpaper
    As described in \Cref{infers}, we infer that child approaches inherit their
    parents' categories. However, there seem to be exceptions to this, which
    may indicate that categories are \emph{not} transitive or that they are
    \emph{except} in certain cases. For example, the practice of experience-based
    testing has many subtechniques, as described above, but it \emph{also} has
    subpractices, including tours \citep[p.~34]{IEEE2022} and exploratory testing
    (although the latter is categorized inconsistently; see \Cref{tab:multiCats}).
    Similarly, the conflicting categorizations of beta testing in
    \Cref{tab:multiCats} may propagate to its children closed beta testing and
    open beta testing. When we infer these flaws, we exclude them from
    \Cref{tab:multiCatTable,tab:multiCats} and instead include them in
    \Cref{tab:infMultiCats} for completeness.
\fi

\subsubsection{Synonym Relation Flaws}\label{syns}

While synonyms do not inherently signify a flaw (as we discuss in
\Cref{syn-rels}), the software testing literature is full of incorrect and
ambiguous synonyms that do. As described in \Cref{relevantSyns}, we pay
special attention to synonyms between independently defined approaches (which
may be flaws) and to intransitive synonyms (which definitely \emph{are} flaws).
\ifnotpaper We present explicit (see \Cref{explicitness}) synonym relations that
    fit either of these criteria in \Cref{fig:expSynGraph}\utd{}, which we
    automatically generate from \ourApproachGlossary{} and manually modify for
    legibility. These relations are given as described by the literature and are
    therefore flawed.

    We provide the full list of synonyms that violate transitivity (along with
    their sources) in \Cref{multiSyns} and discuss and other kinds of flawed
    synonym relations in \Cref{parSyns,scal-flaw,compat-flaw}.
\fi

% Moved here to display nicely in paper
\ifnotpaper\else\input{assets/misc/parSyns}\fi

\subsubsection{Parent-Child Relation Flaws}\label{pars}
\phantomsection{}\label{selfPars}
Parent-child relations are also not immune to flaws. For example,
self-referential definitions mean that a test approach is a parent of itself,
violating the irreflexivity of this relation as defined in \Cref{par-chd-rels}.
Since these are by nature self-contained within a given source, we count these
\emph{once} as objective flaws within
their sources in \Cref{tab:flawMnfsts,tab:flawDmns}. \ifnotpaper We identify
    the following \selfParCount{} examples through automatic analysis of our
    generated graphs (see \Cref{selfParDef}):
    \input{build/selfPars} Interestingly, performance testing is \emph{not}
    described as a subapproach of usability testing by \citep{Gerrard2000a,
        Gerrard2000b}, which would have been more meaningful information to
    capture. \else For example, performance and usability testing are both
    given as subapproaches of themselves \cite[Tab.~2]{Gerrard2000a},
    \cite[Tab.~1]{Gerrard2000b}.\fi

\ifnotpaper
    \begin{figure}[bt!]
        \centering
        \includegraphics[width=\textwidth]{assets/graphs/manual/expSynLegend.pdf}
        \includegraphics[width=\textwidth]{assets/graphs/manual/expSynGraph.pdf}
        % \vspace{-7mm}
        \caption{Visually meaningful \hyperref[syn-rels]{synonym} relations
            given explicitly by the literature.}\label{fig:expSynGraph}
    \end{figure}

    \begin{figure}[bt!]
        \centering
        \includegraphics[width=0.48\linewidth]{assets/graphs/manual/expParSynLegend.pdf}
        \includegraphics[width=0.5\linewidth]{assets/graphs/manual/expParSynGraph.pdf}
        % \vspace{-7mm}
        \caption{Pairs of test approaches with a \hyperref[par-chd-rels]{parent-child}
            \emph{and} \hyperref[syn-rels]{synonym} relation given explicitly by the
            literature.}\label{fig:expParSynGraph}
    \end{figure}
    \clearpage
\else
\fi

\phantomsection{}\label{parSyns}
\parSynIntro{}\ifnotpaper. We visualize the pairs where both
relations are explicit in \Cref{fig:expParSynGraph} and list all
identified pairs in \Cref{tab:parSyns}, as well as pairs where we infer a
flaw in \Cref{infParSyns} for completeness\else\ and list the most
prominent in \Cref{tab:parSyns}\fi. Of
particular note is the relation between path testing and exhaustive testing.
While \citet[p.~421]{vanVliet2000} claims that path testing done completely
``is equivalent to exhaustively testing the program''\ifnotpaper\footnote{The
        contradictory definitions of path testing given in \flawref{path-test}
        add another layer of complexity to this claim.}\fi, this overlooks the
effects of input data \ifnotpaper
    (\citealp[pp.~129\==130]{IEEE2021c}; \citealp[p.~121]{Patton2006};
    \citealp[p.~467]{PetersAndPedrycz2000})
\else
    \cite[p.~467]{PetersAndPedrycz2000}, \cite[p.~121]{Patton2006},
    \cite[p.~129\==130]{IEEE2021c}
\fi and implementation issues \citetext{p.~476} % \citep[p.~476]{PetersAndPedrycz2000}
on the code's behaviour. Exhaustive testing
requires ``all combinations of input values \emph{and} preconditions \dots{}
[to be] tested'' \ifnotpaper (\citealp[p.~4, emphasis added]{IEEE2022};
    similar in \citealpISTQB{}; \citealp[p.~121]{Patton2006})\else
    \cite[p.~4]{IEEE2022} (similar in \citealpISTQB{},
    \cite[p.~121]{Patton2006})\fi.
% Flaw count (WRONG, SYNS): {vanVliet2000} | {IEEE2022} {IEEE2021c} ISTQB {PetersAndPedrycz2000} {Patton2006}

% % TODO: re-investigate this after going through the rest of ISO/IEC/IEEE 29119
% \ifnotpaper
%     \subsubsection{Definition Flaws}\label{defs}

%     \citet{IEEE2022, IEEE2021a, IEEE2021b, IEEE2021c}, from the
%     ISO/IEC/IEEE 29119 family of standards, mention the following 28 test
%     approaches without defining them. This means that out of the 119 test
%     approaches they mention, more than 20\% have no associated definition!

%     However, the previous version of this standard, \citeyearpar{IEEE2013},
%     generally explained two, provided references for two, and explicitly defined
%     one of these terms, for a total of five definitions that could (should) have
%     been included in \citeyearpar{IEEE2022}! These terms have been
%     \underline{underlined}\ifnotpaper%
%         , \emph{italicized}, and \textbf{bolded}, respectively%
%     \fi. Additionally, entries marked with an asterisk* were defined (at least
%     partially) in \citeyearpar{IEEE2017}, which would have been available when
%     creating this family of standards. These terms bring the total count of
%     terms that could (should) have been defined to eleven; almost 40\% of
%     these undefined test approaches could have been defined!

%     \begin{itemize}
%         \item \underline{Acceptance Testing*}
%         \item Alpha Testing*
%         \item Beta Testing*
%         \item BCP Testing
%         \item Capture-Replay Driven Testing
%         \item Configuration Testing*
%         \item Data-driven Testing
%         \item Error-based Testing
%         \item Factory Acceptance Testing
%         \item Fault Injection Testing
%         \item Functional Suitability Testing (also mentioned but not defined in
%               \citep{IEEE2017}; Functional Suitability*)
%         \item Inspections*
%         \item \underline{Integration Testing}*
%         \item Maintenance Testing
%         \item Model Verification
%         \item Operational Acceptance Testing
%         \item Orthogonal Array Testing
%         \item Peer Reviews
%         \item Production Verification Testing
%         \item Recovery Testing* (Failover/Recovery Testing, Back-up/Recovery
%               Testing, \formatPaper{\textbf}{Backup and Recovery Testing*},
%               Recovery*; see \Cref{recov-flaw})
%         \item Response-Time Testing
%         \item \formatPaper{\emph}{Reviews} (ISO/IEC 20246) (Code Reviews*)
%               (IEEE 1028, the IEEE ``Standard for Software Reviews and Audits'',
%               is cited in the bibliography!)
%         \item Scalability Testing (given as a synonym of ``capacity
%               testing''; see \Cref{scal-flaw})
%         \item Statistical Testing
%         \item System Integration Testing (System Integration*)
%         \item System Testing* (also mentioned but not defined in \citep{IEEE2013})
%         \item \formatPaper{\emph}{Unit Testing*}
%               (IEEE 1008, the IEEE ``Standard for Software Unit Testing'',
%               is cited in the bibliography!)
%         \item User Acceptance Testing
%     \end{itemize}
% \fi

\ifnotpaper
    \subsection{Operational (Acceptance) Testing}\label{oat-flaw}
    \paragraph{\texttt{(CONTRA, LABELS)}}
    % Flaw count (CONTRA, LABELS): {IEEE2022} ISTQB | {SWEBOK2024} {ISO_IEC2018} {IEEE2017} {SWEBOK2014}
    There are two names that the literature gives to this test approach:
    \begin{itemize}
        \item \emph{\acf{operat}} (\citealp[p.~22]{IEEE2022};
              \citealpISTQB{}) and
        \item \emph{\acf{ot}} (\citealp{ISO_IEC2018};
              \citealp[p.~303]{IEEE2017}; \citealp[p.~6\=/9, in the context of
                  software engineering operations]{SWEBOK2024};
              \citealp[pp.~4\=/6, 4\=/9]{SWEBOK2014}).
    \end{itemize}

    \paragraph{\texttt{(CONTRA, SYNS)}}
    % Flaw count (CONTRA, SYNS): {Firesmith2015}
    % Assertion: {LambdaTest2024} {BocchinoAndHamilton1996}
    \refHelper \citet[p.~30]{Firesmith2015} lists the above terms separately,
    but they are considered synonyms elsewhere \citep{LambdaTest2024,
        BocchinoAndHamilton1996}\todo{find more academic sources}; since
    \citeauthor{Firesmith2015} does not define these terms, it is hard to
    evaluate \ifnotpaper his \else its \fi distinction.
\fi

\subsection{Recovery Testing}\label{recov-flaw}

``Recovery testing'' is ``testing \dots\ aimed at verifying
software restart capabilities after a system crash or other disaster''
\citep[p.~5\=/9]{SWEBOK2024} including ``recover[ing] the data directly affected
and re-establish[ing] the desired state of the system'' \ifnotpaper
    (\citealp{ISO_IEC2023a}; similar in \citealp[p.~7\=/10]{SWEBOK2024})
\else \cite{ISO_IEC2023a} (similar in \cite[p.~7\=/10]{SWEBOK2024}) \fi
so that the system ``can perform required functions'' \citep[p.~370]{IEEE2017}.
However, the literature also describes similar test approaches with vague or
non-existent distinctions between them. We describe these approaches and their
flaws here and present the relations between them in \Cref{fig:rec-graph-current}.

\NewDocumentCommand\subDRT{s}{given as a subtype of ``disaster/recovery
    testing'' which tests if ``operation of the test item can
    be transferred to a different operating site''
    \IfBooleanTF{#1}{\citetext{p.~37}}{\citeyearpar[p.~37]{IEEE2021c}},
    even though this is \emph{explicitly} excluded from its definition on the
    same page!}

%% again, maybe convert to \paragraph ?
\begin{itemize}
    \item \emph{Recoverability testing} evaluates ``how well a system or
          software can recover data during an interruption or failure''
          \ifnotpaper (\citealp[p.~7\=/10]{SWEBOK2024}; \else
              \cite[p.~7\=/10]{SWEBOK2024} (\fi similar in \citealp{ISO_IEC2023a})
          and ``re-establish the desired state of the system''
          \citeyearpar{ISO_IEC2023a}. \refHelper \citet[p.~47]{Kam2008}
          gives this as a synonym for ``recovery testing''.
    \item \emph{Disaster/recovery testing} evaluates if a system
          can ``return to normal operation after a hardware
          or software failure'' \citep[p.~140]{IEEE2017} or if ``operation of
          the test item can be transferred to a different operating site and
          \dots\ be transferred back again once the failure has been
          resolved'' \citeyearpar[p.~37]{IEEE2021c}.
          \begin{itemize}
              \item \texttt{(OVER, DEFS)}
                    % Flaw count (OVER, DEFS): {IEEE2017} | {IEEE2021c}
                    These two definitions seem to describe different aspects of
                    the system, where the first is intrinsic to the
                    hardware/software and the second might not be, making this
                    term nonatomic.
          \end{itemize}
    \item \emph{Backup and recovery testing} ``measures the
          degree to which system state can be restored from backup within
          specified parameters of time, cost, completeness, and accuracy in
          the event of failure'' \citep[p.~2]{IEEE2013}. This may be what is
          meant by ``recovery testing'' in the context of performance-related
          testing \citeyearpar[Fig.~2]{IEEE2022}.
    \item \emph{Backup/recovery testing} determines the ability of a system
          ``to restor[e] from back-up memory in the event of failure, without
          transfer[ing] to a different operating site or back-up system''
          \citep[p.~37]{IEEE2021c}.
          \begin{itemize}
              %   \item \texttt{(OVER, DEFS)}
              %         % Flaw count (OVER, DEFS): {IEEE2021c} | {IEEE2017}
              %         This definition corresponds to the first definition of
              %         ``disaster/recovery testing'' given by
              %         \citeyearpar[p.~140]{IEEE2017}.
              \item \texttt{(CONTRA, PARS)}
                    % Flaw count (CONTRA, PARS): {IEEE2021c} | {IEEE2021c}
                    This \subDRT{}
              \item \texttt{(OVER, LABELS)}
                    % Flaw count (OVER, LABELS): {IEEE2021c} | {IEEE2013}
                    Its name is also quite similar to ``backup and
                    recovery testing'', adding further confusion.
          \end{itemize}
    \item \emph{Failover/recovery testing} determines the
          ability ``to mov[e] to a back-up system in the event of failure,
          without transfer[ing] to a different operating site''
          \citep[p.~37]{IEEE2021c}.
          \begin{itemize}
              \item \texttt{(CONTRA, PARS)}
                    % Flaw count (CONTRA, PARS): {IEEE2021c} | {IEEE2021c}
                    This is also \subDRT*{}
              \item \texttt{(AMBI, PARS)}
                    % Flaw count (AMBI, PARS): {IEEE2021c}
                    % Assertion: {SWEBOK2024} ISTQB
                    While not explicitly related to recovery, \emph{failover
                        testing} ``validates the SUT's ability to manage heavy
                    loads or unexpected failure to continue typical operations''
                    \citep[p.~5\=/9]{SWEBOK2024} by entering a ``backup
                    operational mode in which [these responsibilities] \dots\
                    are assumed by a secondary system'' \citepISTQB{}. Its name
                    implies that it is a child of ``failover/recovery testing''
                    but its definition makes it more broad (as it includes handling
                    ``heavy loads'' where failover/recovery testing does not)
                    which may reverse the direction of this relation.
              \item \texttt{(AMBI, SYNS)}
                    % Flaw count (AMBI, SYNS): {IEEE2021c} | implied by {Firesmith2015}
                    \refHelper \citet[p.~56]{Firesmith2015} uses the term
                    ``failover and recovery testing'' which may be a synonym of
                    ``failover/recovery testing''.
          \end{itemize}
    \item \emph{Restart \& recovery (testing)} is listed as a test approach by
          \citet[Fig.~5]{Gerrard2000a} but is not defined \texttt{(MISS, DEFS)}
          % Flaw count (MISS, DEFS): {Gerrard2000a}
          and may simply be a synonym to ``recovery testing'' \texttt{(AMBI, SYNS)}.
          % Flaw count (AMBI, SYNS): {Gerrard2000a}
\end{itemize}

\subsection{Scalability Testing}\label{scal-flaw}

\paragraph{\texttt{(CONTRA, SYNS)}}

% Flaw count (CONTRA, SYNS): {IEEE2021c} | {Firesmith2015} {Bas2024}
\citeauthor{IEEE2021c} \citeyearpar[p.~39]{IEEE2021c} give
``scalability testing'' as a synonym of ``capacity testing''
while other sources differentiate between the two \ifnotpaper
    (\citealp[p.~53]{Firesmith2015}; \citealp[pp.~22\==23]{Bas2024})%
\else \cite[p.~53]{Firesmith2015}, \cite[pp.~22\==23]{Bas2024}\fi.

\paragraph{\texttt{(CONTRA, DEFS)}}

% Flaw count (CONTRA, DEFS): {IEEE2021c} | implied by {ISO_IEC2023a}
\citeauthor{IEEE2021c} \citeyearpar[p.~39]{IEEE2021c} also include the external
modification of the system as part of ``scalability'' but
\citet{ISO_IEC2023a} \multiAuthHelper{describe} it as testing the ``capability
of a product to handle growing or shrinking workloads or to adapt its capacity
to handle variability'', implying that this is done by the system itself.

\paragraph{\texttt{(WRONG, LABELS)}}

% Flaw count (WRONG, LABELS): {SWEBOK2024}
\citeauthor{SWEBOK2024} \citeyearpar[p.~5\=/9]{SWEBOK2024} says ``scalability
testing evaluates the capability to use and learn the system and the user
documentation'' and ``focuses on the system's effectiveness in supporting user
tasks and the ability to recover from user errors''. \swebokScalDef{}.

% \subsection{Performance Testing}
% \label{perf-test-ambiguity}

% Similarly, ``performance'' and ``performance efficiency'' are both given as
% software qualities by \ifnotpaper\citeauthor{IEEE2017}\else
%       \cite[p.~319]{IEEE2017}\fi, with the latter defined as the ``performance
% relative to the amount of resources used under stated conditions''
% \ifnotpaper\citeyearpar[p.~319]{IEEE2017} \fi or the ``capability of a product
% to perform its functions within specified time and throughput parameters and be
% efficient in the use of resources under specified conditions'' \citep{ISO_IEC2023a}.
% Initially, there didn't seem to be any meaningful distinction between the two,
% although the term ``performance testing'' is defined
% \ifnotpaper\citeyearpar[p.~320]{IEEE2017}\else\citetext{p.~320}\fi\
% and used by \ifnotpaper\citeauthor{IEEE2017}\else\cite{IEEE2017}\fi\ and the term
% ``performance efficiency testing'' is \emph{also} used by
% \ifnotpaper\citeauthor{IEEE2017}\else\cite{IEEE2017}\fi\ (but not defined
% explicitly). \ifnotpaper Further discussion\thesisissueref{43} brought us to
%       the conclusion \else It can then be concluded \fi that ``performance
% efficiency testing'' is a subset of ``performance testing'', and the
% difference of ``relative to the amount of resources used'' or ``be efficient in
% the use of resources'' between the two is meaningful.

\subsection{Compatibility Testing}\label{compat-flaw}

\paragraph{\texttt{(OVER, DEFS)}}
% Flaw count (OVER, DEFS): {IEEE2022} | {IEEE2017} ISTQB {ISO_IEC2023a}
``Compatibility testing'' is defined as ``testing that measures the
degree to which a test item can function satisfactorily alongside
other independent products in a shared environment (co-existence),
and where necessary, exchanges information with other systems or
components (interoperability)'' \citep[p.~3]{IEEE2022}. This
definition is nonatomic as it combines the ideas of ``co-existence''
and ``interoperability''.

\paragraph{\texttt{(WRONG, SYNS)}}
% Flaw count (WRONG, SYNS): implied by {IEEE2021c} | {IEEE2022}
The ``interoperability'' element of ``compatibility testing'' is explicitly
excluded by \citet[p.~37]{IEEE2021c}, (incorrectly) implying that ``compatibility
testing'' and ``co-existence testing'' are synonyms.

\paragraph{\texttt{(AMBI, SYNS)}}
% Flaw count (AMBI, SYNS): {Kam2008} | {IEEE2022}
Furthermore, the definition of ``compatibility testing'' in
\citet[p.~43]{Kam2008} unhelpfully says ``see \emph{interoperability testing}'',
adding another layer of confusion to the direction of their relationship.

\paragraph{\texttt{(WRONG, LABELS)}}
% Flaw count (WRONG, LABELS): {IEEE2022}
% Assertion: {ISO_IEC2023a} {IEEE2022} {IEEE2021c} {IEEE2017} ISTQB
\refHelper \citet[pp.~22, 43]{IEEE2022} \multiAuthHelper{say}
``interoperability testing helps confirm that applications can work on multiple
operating systems and devices'', but this seems to instead describe
``portability testing'', which evaluates the ``capability of a product to be
adapted to changes in its requirements, contexts of use, or system environment''
\ifnotpaper
    (\citealp{ISO_IEC2023a}; similar in \citealp[p.~7]{IEEE2022};
    \citeyear[pp.~184, 329]{IEEE2017}; \citealpISTQB{})\else
    \cite{ISO_IEC2023a} (similar in \citeyear[pp.~184, 329]{IEEE2017},
    \citealp[p.~7]{IEEE2022}, \cite{ISTQB})\fi, such as being ``transferred
from one hardware \dots{} environment to another'' \citep[p.~39]{IEEE2021c}.
