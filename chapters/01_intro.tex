\section{Introduction}
\label{intro}

As with all fields of science and technology, software development should
be approached systematically and rigorously. \refHelper \ifnotpaper
    \citeauthor{PetersAndPedrycz2000} \else \cite{PetersAndPedrycz2000} \fi
\multiAuthHelper{claim} that ``to be successful, development of software
systems requires an engineering approach'' that is ``characterized by a
practical, orderly, and measured development of software'' \ifnotpaper
    \citeyearpar[p.~3]{PetersAndPedrycz2000}\else \citetext{p.~3}\fi. When a
NATO study group decided to hold a conference to discuss ``the problems of
software'' in 1968, they chose the
phrase ``software engineering'' to ``imply[] the need for
software manufacture to be based on the types of theoretical foundations and
practical disciplines, [sic] that are traditional in the established branches
of engineering'' \citep[p.~13]{NATO1969}. ``The term was not in general use at
that time'', but conferences such as this ``played a major role in gaining
general acceptance \dots{} for the term'' \citep{McClure2001}. While one of the
goals of the conference was to ``discuss possible techniques, methods and
developments which might lead to the[] solution'' to these problems
\citep[p.~14]{NATO1969}, the format of the conference itself was difficult to
document. Two competing classifications of the report emerged: ``one following
from the normal sequence of steps in the development of a software product''
and ``the other related to aspects like communication, documentation,
management, [etc.]'' \citetext{p.~10}. Perhaps more surprisingly, ``to retain
the spirit and liveliness of the conference, \dots{} points of major
disagreement have been left wide open, and \dots{} no attempt \dots{} [was]
made to arrive at a consensus or majority view'' \citetext{p.~11}!

% Testing software is complicated, expensive, and often overlooked. 

% The productivity of testing and testing research would benefit from a standard
% language for communication. 

% These benefits permeate software testing terminology. 

Perhaps unsurprisingly, there are still concepts in software engineering without
consensus, and many of them can be found in the subdomain of software testing.
\refHelper \ifnotpaper \citeauthor{KanerEtAl2011} \else \cite{KanerEtAl2011}
\fi \multiAuthHelper{give} the example of complete testing, which may require
the tester to discover ``every bug in the product'', exhaust the time allocated
to the testing phase, or simply implement every test previously agreed upon
\ifnotpaper \citeyearpar[p.~7]{KanerEtAl2011}\else \citetext{p.~7}\fi.
% They go on to say that
Having a clear definition of ``complete testing'' would reduce the chance for
miscommunication and, ultimately, the tester getting ``blamed for not
doing \dots{} [their] job'' \citetext{p.~7}. Because software testing uses ``a
subtantial percentage of a software development budget (in the range of 30 to
50\%)'', which is increasingly true ``with the growing complexity of software
systems'' \citep[p.~438]{PetersAndPedrycz2000}, this is crucial to the
efficiency of software development. Even more foundationally, if software
engineering holds code to high standards of clarity, consistency, and
robustness, the same should apply to its supporting literature! \ifnotpaper
    \par We noticed this lack of a standard language for software testing while
    working on our own software framework, Drasil \citep{Drasil}, with the
    goal of ``generating all of the software artifacts for (well understood)
    research software''. Currently, these include \acfp{srs}, READMEs,
    Makefiles, and code in up to six languages, depending on the specific case
    study \citep{HuntEtAl2021}. To improve the quality, functionality, and
    maintainability of Drasil, we want to add test cases to this list
    \citep{PotentialProjects,HuntEtAl2021}. This process would be a part of
    our ``continuous integration system, [sic] so that the generated code for
    the case studies is automatically tested with each build'' and would be a
    big step forward, since we currently only ``test that the generated code
    compiles'' \citep{PotentialProjects}. However, before we can include test
    cases as a generated artifact, the underlying domain---software testing---%
    needs to be ``well understood'', which requires a ``stable knowledge
    base'' \citep{HuntEtAl2021}. \fi

Unfortunately, a search for a systematic,
rigorous, and complete taxonomy for software testing revealed that the existing
ones are inadequate\todo{ProWritingAid suggests that including ``and
    incomplete'' is redundant} and mostly focus on the high-level testing
process rather than the testing approaches themselves: % and incomplete

\begin{itemize}
    \item \citeauthor{TebesEtAl2020a} \citeyearpar{TebesEtAl2020a} focus on
          \emph{parts} of the testing process (e.g., test goal, test plan,
          testing role, testable entity) and how they relate to one another,
    \item \citeauthor{SouzaEtAl2017} \citeyearpar{SouzaEtAl2017} prioritize
          organizing test approaches over defining them,
    \item \citeauthor{Firesmith2015} \citeyearpar{Firesmith2015} similarly
          defines relations between test approaches but not the approaches
          themselves, and
    \item \citeauthor{UnterkalmsteinerEtAl2014}
          \citeyearpar{UnterkalmsteinerEtAl2014}
          focus on the ``information linkage or transfer'' \citetext{p.~A:6}
          between requirements engineering and software testing and ``do[]
          not aim at providing a systematic and exhaustive state-of-the-art
          survey of [either domain]'' \citetext{p.~A:2}.
\end{itemize}
In addition to these taxonomies, many standards documents (see \Cref{stds})
and terminology collections (see \Cref{metas}) define testing terminology,
albeit with their own issues.

% Some existing collections of software testing terminology were found, but
% in addition to being incomplete, they also contained many oversights.

% For example, \citet{IEEE2017} \multiAuthHelper{provide} the following
% term-definition pairs:
% \begin{itemize}
%     \item \textbf{Event Sequence Analysis:} ``per'' \citetext{p.~170}
%     \item \textbf{Operable:} ``state of'' \citetext{p.~301}
%           % This may be a bad example, since the cf. provides some more context
%           % \item \textbf{Software Element:} a ``system element that is software''
%           %       \citetext{p.~421}
% \end{itemize}
% These definitions are nonsensical and do not define the terms they claim
% to! To be sure, they \emph{cannot} correspond to the terms given since the
% parts of speech are mismatched: the first defines a noun phrase as a
% preposition, and the second an adjective as a noun phrase fragment.
% \citet{IEEE2017} also \multiAuthHelper{define} ``device'' as a ``mechanism
% or piece of equipment designed to serve a purpose or perform a function''
% \citetext{p.~136}, but do\ifnotpaper\else{es}\fi\ not define ``equipment''
% and only \multiAuthHelper{define} ``mechanism'' in the software sense as
% ``the means used by a function to transform input into output''
% \citetext{p.~270\todo{OG IEEE 1998}}. This is an example of an incomplete
% definition; while the definition of ``device'' seems logical at first
% glance, it actually leaves much undefined.

\phantomsection{}\label{error-fault-failure}
For example, a common point of discussion in the field of software is the
distinction between terms for when software does not work correctly. We find
the following four to be most prevalent:
\begin{itemize}
    \item \textbf{Error:} ``a human action that produces an incorrect result''
          \ifnotpaper (\citealp[p.~128]{IEEE2010};
              \citealp[p.~399]{vanVliet2000})\else
              \cite[p.~128]{IEEE2010}, \cite[p.~399]{vanVliet2000}\fi.
          %   ``the difference between a computed, observed, or measured value or
          %   condition and the true, specified, or theoretically correct value or
          %   condition'' (IEEE, 2010, p. 128;
          %   similar in Washizaki, 2024, pp. 17-18 to 17-19, 18-7 to 18-8)
    \item \textbf{Fault:} ``an incorrect step, process, or data definition in a
          computer program'' \citep[p.~140]{IEEE2010} inserted when a developer
          makes an error \ifnotpaper (pp.~128, 140; \citealp[p.~12\=/3]{SWEBOK2024};
              \citealp[pp.~399--400]{vanVliet2000})\else
              \cite[pp.~128, 140]{IEEE2010}, \cite[pp.~399--400]{vanVliet2000},
              \cite[p.~12\=/3]{SWEBOK2024}\fi.
    \item \textbf{Failure:} the inability of a system ``to perform a required
          function or \dots{} within previously specified limits'' \ifnotpaper
              (\citealp[p.~7]{IEEE2019}; \citeyear[p.~139]{IEEE2010}\todo{OG
                  ISO/IEC, 2005}; similar in \citealp[p.~400]{vanVliet2000})
          \else \cite[p.~139]{IEEE2010}, \cite[p.~7]{IEEE2019} \fi that is
          ``externally visible'' \ifnotpaper(\fi\citealp[p.~7]{IEEE2019}%
          \ifnotpaper; similar in \citealp[p.~400]{vanVliet2000})\fi\
          and caused by a fault \ifnotpaper (\citealp[p.~12\=/3]{SWEBOK2024};
              \citealp[p.~400]{vanVliet2000})\else \cite[p.~400]{vanVliet2000},
              \cite[p.~12\=/3]{SWEBOK2024}\fi.
    \item \textbf{Defect:} ``an imperfection or deficiency in a project
          component where that component does not meet its requirements or
          specifications and needs to be either repaired or replaced''
          \citep[p.~96]{IEEE2010}.
\end{itemize}
This distinction is sometimes important, but not always
\citep[p.~4\=/3]{SWEBOK2014}\todo{OG IEEE, 1996}. The term ``fault'' is
``overloaded with too many meanings, as engineers and others use the word to
refer to all different types of anomalies'' \citep[p.~12\=/3]{SWEBOK2024}, and
``defect'' may be used as a ``generic term that can refer to either a fault
(cause) or a failure (effect)'' \ifnotpaper (\citealp[p.~124]{IEEE2017};
    \citeyear[p.~96]{IEEE2010}\todo{OG 2005})\else \cite[p.~96]{IEEE2010},
    \cite[p.~124]{IEEE2017}\fi. Software testers may even choose to ignore
these nuances completely! \refHelper \bugPattonFlaw{}

\ifnotpaper
    These decisions are not inherently wrong, since they may be useful in
    certain contexts or for certain teams (cf.~\Cref{syn-rels}). Problems start
    to arise when teams need to make these decisions in the first place.
    \citet[p.~14]{Patton2006} notes that ``a well-known computer company spent
    weeks in discussion with its engineers before deciding to rename \acfp{par}
    to \acfp{pir}'', a process that required ``countless dollars'' and updating
    ``all the paperwork, software, forms, and so on''. While consistency and
    clear terminology may have been valuable to the company, ``it's unknown if
        [this decision] made any difference to the programmer's or tester's
    productivity'' \citetext{p.~14}. A potential way to avoid similar resource
    sinks would be to prescribe a standard terminology. Perhaps multiple sets
    of terms could be designed with varying levels of specificity so a company
    would only have to determine which one best suits their needs.
\fi

But why are minor differences between terms like these even important? The
previously defined terms ``error'', ``fault'', ``failure'', and ``defect''
are used to describe many test approaches, including:
\begin{multicols}{2}
    \begin{enumerate}
        \item Defect-based testing
        \item Error forcing
        \item Error guessing
        \item Error tolerance testing
        \item Error-based testing
        \item Error-oriented testing
        \item Failure tolerance \ifnotpaper\else \\ \fi testing
        \item Fault injection testing
        \item Fault seeding
        \item Fault sensitivity \ifnotpaper\else \\ \fi testing
        \item Fault tolerance testing
        \item Fault tree analysis
        \item Fault-based testing
    \end{enumerate}
\end{multicols}
When considering which approaches to use or when actually using them, the
meanings of these four terms inform what their related approaches accomplish
and how to they are performed. For example, the tester needs to know what a
``fault'' is to perform fault injection testing; otherwise, what would they
inject? Information such as this is critical to the testing team, and should
therefore be standardized.

These kinds of inconsistencies can lead to miscommunications---such as that
previously mentioned by \citet[p.~7]{KanerEtAl2011}---and are prominent in the
literature. \expBasedCatMain{} \tourFlaw{} \loadFlaw{} \alphaFlaw{}
It is clear that there is a notable gap in the literature, one which we
attempt to describe and fill. While the creation of a complete taxonomy is
unreasonable, especially considering the pace at which the field of software
changes, we can make progress towards this goal that others can extend and
update as new test approaches emerge.

\phantomsection{}\label{scope}
Based on the observed gap in software testing specifically and our original
motivation for this research, we only consider the component of \acf{vnv} that
tests code itself\thesisissueref{22}. However, some test approaches are only
used to testing \emph{other} artifacts, while others can be used for both! In
these cases, we only consider the subsections that focus on code. For example,
reliability testing and maintainability testing can start \emph{without} code
by ``measur[ing] structural attributes of representations of the software''
\citep[p.~18]{FentonAndPfleeger1997}, but only reliability and
maintainability testing performed on code \emph{itself} is in scope of this
research. \ifnotpaper This is a high-level overview of what is in scope; see
    \Cref{app-scope} for more detailed discussion on what we include and exclude.
    % We provide more detailed discussion on what is and is not in scope later in
    % this document, including practices we exclude in full, such as hardware
    % testing (\Cref{hard-test}), or in part, such as the parts of error seeding,
    % fault injection testing, and mutation testing that do not directly test
    % % ``Fault injection originated out of testing of integrated circuits''
    % % \citep[p.~40]{GhoshAndVoas1999}
    % code (\Cref{other-vnv}). Additionally, static testing is a useful component
    % of software testing and is therefore included at this level of analysis,
    % despite it not being relevant to our original motivation
    % (\Cref{static-test}).
    % % Finally, some
    % % test approaches can be derived from other categories of testing-related
    % % terminology (\Cref{derived-tests}); of these, approaches derived from
    % % programming languages (\Cref{lang-test}) or other orthogonal test
    % % approaches (\Cref{orth-test}) are out of scope.
\fi

This document describes our process, as well as our results, in more detail.
\input{build/methodOverviewIntro}
We follow the procedure laid out in \Cref{procedure} and use these \acfp{rq}
as a guide:
\begin{enumerate}
    \item \rqatext{}
    \item \rqbtext{}
    \item \rqctext{}
\end{enumerate}
\ifnotpaper An excerpt of this recorded information (excluding other notes for
    brevity), is given in \Cref{tab:approachGlossaryExcerpt}. \fi We then
create tools to support our analysis of our findings (\Cref{tools}).
Despite the amount of well understood and organized knowledge, the literature
is still quite flawed (\Cref{flaws}). This reinforces the need for a proper
taxonomy! We then provide some potential solutions covering some of these
flaws (\Cref{recs}).

\ifnotpaper
    \begin{landscape}
        \input{assets/tables/exampleGlossary}
    \end{landscape}
\fi