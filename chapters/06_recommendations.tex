\section{Recommendations}\label{recs}

As we have shown in \Cref{flaws}, ``testing is a mess'' \citetext{Mosser,
    2023, priv.\ comm.}! It will take a lot of time, effort, expertise, and
training to organize these terms (and their relations) logically. However, the
hardest step is often the first one, so we attempt to give some examples of how
this ``rationalization'' can occur. These changes often arise when we notice an
issue with the current state of the terminology and think about what \emph{we}
would do to make it better. We do not claim that these are correct, unbiased,
or exclusive, just that they can be used as an inspiration for those wanting to
pick up where we leave off.

When redefining terms, we seek to make them:
\begin{enumerate}
    \item Atomic (e.g., disaster/recovery testing seems to have two
          disjoint definitions)
    \item Straightforward (e.g., backup and recovery testing's definition
          implies the idea of performance, but its name does not
          \ifnotpaper; failover/recovery testing, failover and recovery
          testing, and failover testing are all given separately\fi)
    \item Consistent (e.g., backup/recovery testing and failover/recovery
          testing explicitly exclude an aspect included in its parent
          disaster/recovery testing)
\end{enumerate}
Likewise, we seek to eliminate classes of flaws that can be detected
automatically, such as test approaches that are given as synonyms to multiple
distinct approaches (\Cref{multiSyns}) or as parents of themselves
(\Cref{selfPars}), or pairs of approaches with both a parent-child \emph{and}
synonym relation (\Cref{parSyns}).\todo{Same label to different phantomsections;
    is that OK?}

We give recommendations for the areas of \ifnotpaper operational (acceptance)
    testing (\Cref{oat-test-rec}), \fi recovery testing (\Cref{rec-test-rec}),
scalability testing (\Cref{scal-test-rec}), and performance-related testing
(\Cref{perf-test-rec}). Graphical representations (described in
\Cref{\ifnotpaper graph-gen\else tools\fi}) of these subsets are given in
\recFigs{}, in which arrows representing relations between approaches are
coloured based on the source tier (see \Cref{sources}) that defines them.
Any added approaches or relations are colored \textcolor{orange}{orange}.
\ifnotpaper Note that
    inferred relations (colored \textcolor{gray}{grey}) are included for
    completeness\todo{Should this be the case?}, despite not coming from the
    literature (see \Cref{infers}).

    \subsection{Operational (Acceptance) Testing}\label{oat-test-rec}
    Since this terminology is not standardized (see \Cref{oat-flaw}), we
    propose that ``\acf{operat}'' and ``\acf{ot}'' are treated as
    synonyms for a type of acceptance testing (\citealp[p.~22]{IEEE2022};
    \citealpISTQB{}) that focuses on ``non-functional'' attributes of the system
    \citep{LambdaTest2024}\todo{find more academic sources}. Indeed, this is how we
    track this approach in \ourApproachGlossary{}! We define it as ``test[ing] to
    determine the correct installation, configuration and operation of a module and
    that it operates securely in the operational environment'' \citep{ISO_IEC2018}
    or to ``evaluate a system or component in its operational environment''
    \citep[p.~303]{IEEE2017}, particularly ``to determine if operations and/or
    systems administration staff can accept [it]'' \citepISTQB{}.
\fi

\subsection{Recovery Testing}\label{rec-test-rec}
The following terms should be used in place of the current terminology to
more clearly distinguish between different recovery-related test approaches.
The result of the proposed terminology, along with their relations, is
demonstrated in \Cref{fig:recovery-graph-proposed}.

\recoveryGraphs{}

\begin{itemize}
    \item \textbf{Recoverability Testing:} ``Testing \dots\ aimed at
          verifying software restart capabilities after a system crash or
          other disaster'' \citep[p.~5-9]{SWEBOK2024} including ``recover[ing]
          the data directly affected and re-establish[ing] the desired state
          of the system''
          \ifnotpaper
              (\citealp{ISO_IEC2023a}; similar in \citealp[p.~7-10]{SWEBOK2024})
          \else
              \cite{ISO_IEC2023a} (similar in \cite[p.~7-10]{SWEBOK2024})
          \fi so that the system ``can perform
          required functions'' \citep[p.~370]{IEEE2017}. ``Recovery testing''
          will be a synonym, as in \citep[p.~47]{Kam2008}, since it is the
          more prevalent term throughout various sources, although
          ``recoverability testing'' is preferred to indicate that this
          explicitly focuses on the \emph{ability} to
          recover, not the \emph{performance} of recovering.
    \item \textbf{Failover Testing:} Testing that ``validates the SUT's
          ability to manage heavy loads or unexpected failure to continue
          typical operations'' \cite[p.~5-9]{SWEBOK2024} by entering a
          ``backup operational mode in which [these responsibilities] \dots\
          are assumed by a secondary system'' \citepISTQB{}. This will
          replace ``failover/recovery testing'', since it is more clear, and
          since this is one way that a system can recover from failure, it
          will be a subset of ``recovery testing''.
    \item \textbf{Transfer Recovery Testing:} Testing to evaluate if,
          in the case of a failure, ``operation of the test item can be
          transferred to a different operating site and \dots\ be transferred
          back again once the failure has been resolved''
          \citeyearpar[p.~37]{IEEE2021}. This replaces the second definition
          of ``disaster/recovery testing'', since the first is just a
          description of ``recovery testing'', and could potentially be
          considered as a kind of failover testing. This may not be
          intrinsic to the hardware/software (e.g., may be the responsibility
          of humans/processes).
    \item \textbf{Backup Recovery Testing:} Testing that determines the
          ability ``to restor[e] from back-up memory in the event of failure''
          \citep[p.~37]{IEEE2021}. The qualification that this occurs
          ``without transfer[ing] to a different operating site or back-up
          system'' \citetext{p.~37} \emph{could} be made explicit, but this is
          implied since it is separate from transfer recovery testing and
          failover testing, respectively.
    \item \textbf{Recovery Performance Testing:} Testing ``how well a system or
          software can recover \dots\ [from] an interruption or failure''
          \ifnotpaper
              (\citealp[p.~7-10]{SWEBOK2024}; similar in \citealp{ISO_IEC2023a})
          \else
              \cite[p.~7-10]{SWEBOK2024} (similar in \cite{ISO_IEC2023a})
          \fi ``within specified parameters of time, cost, completeness, and
          accuracy'' \citep[p.~2]{IEEE2013}. The distinction between the
          performance-related elements of recovery testing seemed to be
          meaningful\thesisissueref{40}, but was not captured consistently
          by the literature. This will be a subset of ``performance-related
          testing'' \ifnotpaper (see \Cref{perf-test-rec}) \fi
          as ``recovery testing'' is in \citep[p.~22]{IEEE2022}. This could
          also be extended into testing the performance of specific elements
          of recovery (e.g., failover performance testing), but this be too
          fine-grained and may better be captured as an
          \hyperref[orth-test]{orthogonally derived test approach}.
\end{itemize}

\subsection{Scalability Testing}\label{scal-test-rec}

We describe the issues with scalability testing terminology in \Cref{scal-flaw}
and show the relations between related approaches (as described by the
literature) in \Cref{fig:scal-graph-current}. These flaws are resolved and/or
explained by other sources! Taking this extra information into account results
in \Cref{fig:scal-graph-proposed} and provides a more accurate description of
scalability testing.

\paragraph{\texttt{(CONTRA, SYNS)}}
\citeauthor{IEEE2021} \citeyearpar[p.~39]{IEEE2021} define ``scalability
testing'' as the testing of a system's ability to ``perform under conditions
that may need to be supported in the future''.
%, which ``may include assessing what level of additional resources (e.g.
% memory, disk capacity, network bandwidth) will be required to support
% anticipated future loads''.
This focus on ``the future''
is supported by \citetISTQB{}, \ifnotpaper who define \else which defines \fi
``scalability'' as ``the degree to which a component or system can be adjusted
for changing capacity''\ifnotpaper; the original source they reference agrees,
defining it as ``the measure of a system's ability to be upgraded to
accommodate increased loads'' \citep[p.~381]{GerrardAndThompson2002}\fi. In
contrast, capacity testing focuses on the system's present state, evaluating
the ``capability of a product to meet requirements for the maximum limits of a
product parameter''
%, such as the number of concurrent users, transaction
% throughput, or database size 
\citep{ISO_IEC2023a}. Therefore, it these terms should \emph{not} be synonyms,
as done by \ifnotpaper
    \citet[p.~53]{Firesmith2015} and \citet[pp.~22\==23]{Bas2024}\else
    \cite[p.~53]{Firesmith2015} and \cite[pp.~22\==23]{Bas2024}\fi.

\paragraph{\texttt{(CONTRA, DEFS)}}

There seems to be an underlying reason that sources disagree on whether
external modification of the system is part of scalability testing.
\citeauthor{SWEBOK2024} \citeyearpar[p.~5\=/9]{SWEBOK2024} claim that one
objective of elasticity testing is ``to evaluate scalability'', implying that
\citep{ISO_IEC2023a}'s notion of ``scalability'' actually refers to
``elasticity''! This makes sense in the context of other definitions
provided by \citet{SWEBOK2024}:
\begin{itemize}
    \item \textbf{Scalability:} ``the software's ability to increase and
          scale up on its nonfunctional requirements, such as load, number of
          transactions, and volume of data'' \citetext{p.~5\=/5}. Based on this
          definition, scalability testing is then a subtype of load testing
          and volume testing, as well as potentially transaction flow testing.
    \item \textbf{Elasticity Testing\footnote{This definition seems correct but
                  its source is unclear; see \flawref{elas-ref}.}:} testing
          that ``assesses
          the ability of the \acs{sut} \dots\ to rapidly expand or shrink
          compute, memory, and storage resources without compromising the
          capacity to meet peak utilization'' \citetext{p.~5\=/9}. Based on this
          definition, elasticity testing is then a subtype of memory
          management testing (with both being a subtype of resource
          utilization testing) and stress testing.
\end{itemize}
This distinction is consistent with how the terms are used in industry:
\citet{Pandey2023}\thesisissueref{35} says that scalability is the ability to
``increase \dots\ performance or efficiency as demand increases over time'',
while elasticity allows a system to ``tackle changes in the workload [that]
occur for a short period''.

\paragraph{\texttt{(WRONG, LABELS)}}

\citeauthor{SWEBOK2024} \citeyearpar[p.~5\=/9]{SWEBOK2024}'s definition of
``scalability testing'' is completely separate from the definitions of
``scalability'', ``capacity'', and ``elasticity'' discussed above! Therefore,
this definition and its related synonym relations should simply be disregarded
since they are inconsistent with the rest of the literature.

\scalGraphs{}

\subsection{Performance(-related) Testing}
\label{perf-test-rec}

``Performance testing'' is defined as testing ``conducted to evaluate the
degree to which a test item accomplishes its designated functions''
\ifnotpaper
    (\citealp[p.~7]{IEEE2022}; \citeyear[p.~320]{IEEE2017}; similar in
    \citeyear[pp.~38-39]{IEEE2021}; \citealp[p.~1187]{Moghadam2019})%
\else
    \cite[p.~320]{IEEE2017}, \cite[p.~7]{IEEE2022} (similar in
    \cite[pp.~38-39]{IEEE2021}, \cite[p.~1187]{Moghadam2019})%
\fi. It does this
by ``measuring the performance metrics''
\ifnotpaper
    (\citealp[p.~1187]{Moghadam2019}; similar in \citealpISTQB{})
\else
    \cite[p.~1187]{Moghadam2019} (similar in \cite{ISTQB})
\fi (such as the ``system's capacity for growth''
\citep[p.~23]{Gerrard2000b}), ``detecting the functional problems appearing
under certain execution conditions'' \citep[p.~1187]{Moghadam2019}, and
``detecting violations of non-functional requirements under expected and
stress conditions'' \ifnotpaper
    (\citealp[p.~1187]{Moghadam2019}; similar in \citealp[p.~5-9]{SWEBOK2024})%
\else
    \cite[p.~1187]{Moghadam2019} (similar in \cite[p.~5-9]{SWEBOK2024})%
\fi. It is performed either \dots\
\begin{enumerate}
    \item ``within given constraints of time and other resources''
          \ifnotpaper
              (\citealp[p.~7]{IEEE2022}; \citeyear[p.~320]{IEEE2017};
              similar in \citealp[p.~1187]{Moghadam2019})%
          \else
              \cite[p.~320]{IEEE2017}, \cite[p.~7]{IEEE2022} (similar
              in \cite[p.~1187]{Moghadam2019})%
          \fi, or
    \item ``under a `typical' load'' \citep[p.~39]{IEEE2021}.
\end{enumerate}

It is listed as a subset of performance-related testing, which is defined as
testing ``to determine whether a test item performs as required when it is
placed under various types and sizes of `load'\,'' \citeyearpar[p.~38]{IEEE2021},
along with other approaches like load and capacity testing
\citep[p.~22]{IEEE2022}. Note that ``performance, load and stress testing might
considerably overlap in many areas'' \citep[p.~1187]{Moghadam2019}.
In contrast, \citet[p.~5-9]{SWEBOK2024}
gives ``capacity and response time'' as examples of ``performance
characteristics'' that performance testing would seek to ``assess'', which
seems to imply that these are subapproaches to performance testing instead.
This is consistent with how some sources treat ``performance testing'' and
``performance-related testing'' as synonyms \ifnotpaper
    (\citealp[p.~5-9]{SWEBOK2024}; \citealp[p.~1187]{Moghadam2019})%
\else \cite[p.~5-9]{SWEBOK2024}, \cite[p.~1187]{Moghadam2019}%
\fi, as noted in \Cref{syns}. This makes sense because of how general the
concept of ``performance'' is; most definitions of ``performance testing'' seem
to treat it as a category of tests.

However, it seems more consistent to infer
that the definition of ``performance-related testing'' is the more general one
often assigned to ``performance testing'' performed ``within given constraints
of time and other resources'' \ifnotpaper (\citealp[p.~7]{IEEE2022};
    \citeyear[p.~320]{IEEE2017}; similar in \citealp[p.~1187]{Moghadam2019})%
\else \cite[p.~320]{IEEE2017}, \cite[p.~7]{IEEE2022}
    (similar in \cite[p.~1187]{Moghadam2019})\fi, and
``performance testing'' is a subapproach of this performed ``under a `typical'
load'' \citep[p.~39]{IEEE2021}. This has other implications for relations
between these types of testing; for example, ``load testing'' usually occurs
``between anticipated conditions of low, typical, and peak usage''
\ifnotpaper (\citealp[p.~5]{IEEE2022}; \citeyear[p.~39]{IEEE2021};
    \citeyear[p.~253]{IEEE2017}\todo{OG IEEE 2013}; \citealpISTQB{})%
\else \cite[p.~253]{IEEE2017}, \cite{ISTQB}, \cite[p.~5]{IEEE2022},
    \cite[p.~39]{IEEE2021}\fi, so it is a child of ``performance-related
testing'' and a parent of ``performance testing''.

After these changes, some finishing touches remain. The ``self-loops''
mentioned in \Cref{selfPars} provide no new information and can be removed.
Similarly, the term ``soak testing'' can be removed. Since it is given as a
synonym to both ``endurance testing'' \emph{and} ``reliability testing'' (see
\Cref{multiSyns}), it makes sense to just use these terms instead of one that
is potentially ambiguous. These changes (along with those from
\Cref{rec-test-rec,scal-test-rec} made implicitly) result in
the relations shown in \Cref{fig:perf-graph}.

\begin{paperFigure}
    \centering
    \performanceGraph{}
    \caption{Proposed relations between rationalized ``performance-related testing'' terms.}
    \label{fig:perf-graph}
\end{paperFigure}