\section{Threats to Validity}\label{threats}

While we do our best to approach the software testing literature with the
rigour it deserves, we make some decisions with which others may disagree. The
benefit of our work being open-source means that others can make their own
decisions, modify our data/code approapriately, and observe how this changes
the results. Nevertheless, some potential threats to validity arise from the
test approach categorization we use (\Cref{cat-threats}).

\subsection{Threats from Approach Categories}\label{cat-threats}

In \Cref{cats-def}, we outline our criteria for categorizing test approaches
based on documents by \citeauthor{IEEE2022} (such as \citeyear[Fig.~2]{IEEE2022})
and summarize it in \Cref{tab:ieeeCats}. However, the terms we use are
sometimes used in a general sense (as opposed to our specific definitions)
which makes it hard to evaluate how consistently the literature uses them
(\Cref{overloaded-cats}). Furthermore, the literature provides other criteria
for categorizing test approaches (\Cref{alt-cats}); using any of these schemas
would affect the category data we collect, if any, and how we use them to
identify the flaws given in \Cref{cats}.

\subsubsection{Overloaded Terms}\label{overloaded-cats}

A side effect of using the terms ``level'', ``type'', ``technique'', and
``practice'' is that they---perhaps excluding ``level''---can be used
interchangably or as synonyms for ``approach''. Because natural language can be
ambiguous, we need to exercise judgement when determining if these terms are
being used in a general or technical sense. For example,
\citet[p.~45\ifnotpaper, emphasis added\fi]{Kam2008}
defines interface testing as ``an integration \emph{test type} that is
concerned with testing \dots{} interfaces'', but since \ifnotpaper he \else it
\fi does not define ``test type'', this may not have special significance.
\ifnotpaper We consider these kinds of ``categorizations'' to be
    inferred (see \Cref{infers}) and mark them with a question mark (?)~during
    data collection. When we evaluate test approaches categorized more than
    once in \Cref{multiCats}, we list well-defined categorizations in
    \Cref{tab:multiCats} and inferred ones in \Cref{tab:infMultiCats}.
\fi

\phantomsection{}\label{use-of-chatgpt}
We are not immune to using these terms sloppily either! For example,
\citet[p.~88]{Patton2006} says that if a specific defect is found, it is wise
to look for other defects in the same location and for similar defects in other
locations, but does not provide a name for this approach. After researching in
vain, we ask ChatGPT\footnote{We do \emph{not} take ChatGPT's output to be
    true at face value; this approach seems to be called ``defect-based
    testing'' based on the principle of ``defect clustering''
    \citep{ChatGPT2024}, which \citet{RusEtAl2008} \multiAuthHelper{support}.}
to name the ``\emph{type} of software testing that focuses on looking for bugs
where others have already been found''
\ifnotpaper \citep[emphasis added]{ChatGPT2024}\else \cite{ChatGPT2024}\fi%
\qtodo{Is this sufficient to explain ChatGPT's role in our methodology?},
using the word ``type'' in a general sense, akin to ``kind'' or ``subset''.
Interestingly, ChatGPT ``corrects'' our imprecise term in its response,
using the more correct term ``approach'' (although it may have been biased by
our previous usage of these terms)!

\subsubsection{Alternate Approach Categorizations}\label{alt-cats}

While our categorization in \Cref{tab:ieeeCats} is used fairly consistently,
other sources \citep[such as][]{SWEBOK2024,BarbosaEtAl2006} propose similar yet
distinct categories that clash or overlap with them. We give these alternate
categorizations in \Cref{tab:otherCats}, which could simply map to their
``IEEE Equivalents'' or could provide new perspectives and be useful in some
contexts, either in place of or in tandem with ours.

\afterpage{\begin{landscape}%
        % Omitted from paper for brevity
        \otherCatsTable{}%
    \end{landscape}}

Similarly, we find many other criteria for categorizing test approaches in
the literature.
% which may be used in tandem with or in place of those given
% in \Cref{tab:ieeeCats,tab:otherCats}.
In general, these are defined less systematically but are more fine-grained,
seeming to ``specialize'' our categories from \Cref{tab:ieeeCats}. The
existence of these categorizations
is not inherently wrong, as they may be useful for specific teams or in
certain contexts. For example, functional testing and structural testing
``use different sources of information and have been shown to highlight
different problems'', and deterministic testing and random testing have
``conditions that make one approach more effective than the other''
\citep[p.~5\=/16]{SWEBOK2024}. Unfortunately, even these alternate
categories are not used consistently (see \flawref{manual-or-keyword})!

We include the most prominent of these alternate categorizations in
\Cref{tab:otherCategorizations} for completeness. In each row, the source
given lists the example approaches and categorizes them as the given
``Parent IEEE Category'' unless stated otherwise (in some cases, the source
gives additional approaches that we omit for brevity). For example, in the
first row, \citet[pp.~22, 35]{IEEE2022}
categorize both manual testing and automated testing as test practices.
Note that since ``approach'' is a catch-all category (see \Cref{approach-def}),
it does not require an explicit categorization, and that these categorizations
may be flawed, as stated in the provided footnotes.

\afterpage{
    \begin{landscape}
        % Omitted from paper for brevity
        \otherCategorizationsTable{}
    \end{landscape}
}

\newpage

\phantomsection{}\label{method-family}
Another way to subdivide the IEEE categories is by grouping related test
approaches into a ``class'' or ``family'' with ``commonalities and
well-identified variabilities that can be
instantiated'', where ``the commonalities are large and the variabilities
smaller'' \citep{classFamilyDisc}. Examples of these are the classes of
combinatorial \citep[p.~15]{IEEE2021} and data flow testing \citetext{p.~3}
and the family of performance-related testing \perfAsFamily{}; we explore
the notion of test approach families in more detail in
\Cref{classFamilyFlaw}. Note that ``there is a lot of overlap between
different classes of testing'' \citep[p.~8]{Firesmith2015}, meaning that ``one
category [of test techniques] might deal with combining two or more techniques''
\citep[p.~5-10]{SWEBOK2024}. For example, ``performance, load and stress
testing might considerably overlap in many areas'' \citep[p.~1187]{Moghadam2019}.
A side effect of this is that it is difficult to ``untangle'' these classes;
for example, take the following sentence: ``whitebox fuzzing extends dynamic
test generation based on symbolic execution and constraint solving from unit
testing to whole-application security testing''
\citep[p.~23]{GodefroidAndLuchaup2011}! This is, in part, why research on
software testing terminology is so vital.\qtodo{Is this sentence needed?}