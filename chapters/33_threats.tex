\section{Threats to Validity}\label{threats}

A case study is valid if its ``results are true and not biased by the
researchers' subjective point of view'' \citep[p.~153]{RunesonAndHöst2009}.
Since our research is a systematic analysis and summary of the software
community's usage of testing terminology, we do our
best to accomplish this. To present and explain our findings,
we make some decisions with which others may disagree. The benefit of our work
being open-source means that others can make their own decisions, modify our
data/code appropriately, and observe how this changes the results. Despite this
and our best efforts, some threats to validity remain; we use
\citepos[pp.~153\==154]{RunesonAndHöst2009} definitions of threats to validity
and list them in their respective subsections as follows:
\begin{enumerate}
    \item \textbf{Reliability:} If another researcher were to conduct the case
          study later on and get the same result, then the case study is
          reliable. (\Cref{rel-valid})
    \item \textbf{Construct Validity:} This refers to the extent to which ``the
          operational measures that are studied really represent what the
          researcher[s] have in mind and what is investigated'', or how well
          the studied data aligns with the data that the researchers
          \emph{intended} to study. (\Cref{cons-valid})
    \item \textbf{Internal Validity:} A study is internally valid if the
          discovered causal relations are trustworthy and cannot be explained
          by other factors. % ``When \dots{} investigating whether one
          %   factor affects an investigated factor[,] there is a risk that the
          %   investigated factor is also affected by a third factor''. This
          %   threatens internal validity ``if the researcher is not aware of the
          %   third factor and/or does not know to what extent it affects the
          %   investigated factor''.
          (\Cref{inter-valid})
    \item \textbf{External Validity:} For results of a study to be externally
          valid, they should be generalizable and ``of interest to other people
          outside the investigated case''. (\Cref{exter-valid})
\end{enumerate}

\subsection{Reliability}\label{rel-valid}
Natural language is nuanced and software testing has a wide scope, so we have
to make judgement calls throughout the research process, usually involving
which information to record. Other researchers may make different decisions,
which would threaten the reliability of our results. We mitigate this at a high
level by outlining what we exclude from our scope in \Cref{app-scope}. This
means that even if other researchers decide on a different scope, the data and
results within the intersection of these scopes should match. By following the
methodology outlined in \Cref{methodology}, we further mitgate the following
threats to reliability:

\begin{itemize}
    \item \textbf{Single Researcher:} Since only one researcher was responsible
          for data collection, it was solely up to them to determine what data
          and relations were important to record and investigate. To reduce the
          negative impact this might have on our research, we follow our
          methodology as rigorously as possible, including heuristics
          for what data is important (see \Cref{ident-terms}). When ambiguity
          arises or more involved judgement is required, we discuss these
          details as a team to include more perspectives, reducing the amount
          of bias and oversight that would propagate throughout our research%
          \ifblind{}{; these discussions can be found \href{
                  https://github.com/samm82/TestingTesting/issues?q=state\%3Aclosed label\%3Adiscussion
              }{on our repo}}.

    \item \textbf{Implicit Information:} While natural language can be
          ambiguous, only recording explicit information would omit a lot of
          data provided by the software testing literature. We therefore
          document implicit information from the
          literature for completeness as described in \Cref{imp-info}. While
          other researchers may disagree on what information ``follows
          logically'' or ``is dubious'', these data are innately subjective,
          so excluding them should result in our \emph{explicit} data (and
          the results based on them) matching those of other researchers.

    \item \textbf{Drift Over Time:} Since we began recording data
          systematically in January 2024\ifblind{}{\ (see the \href{
                  https://github.com/samm82/TestingTesting/commit/d1b2a12
              }{first version of our test approach glossary})}, we have
          iterated on what we consider to be in scope and what methodology to
          use. Some sources we investigate have even been updated (see
          \Cref{swebok-update})! Future researchers following our methodology
          would therefore end up with different results. While we could
          partially mitigate this by re-iterating over all sources with our
          ``finalized'' methodology, this is not possible due to time
          constraints. With respect to sources updating during the course of
          our research, future researchers could replicate our results by
          reviewing a ``snapshot'' of them from when we reviewed\qtodo{Present
              tense? And does this make sense?} them. However, if future
          researchers were to repeat this research, they \emph{should} use the
          most up-to-date sources to update our existing data, methodology, and
          tools. This is one of the benefits of our research being open-source:
          it can exist as a ``living document'' that can (ideally) keep up with
          innovations to software testing!
\end{itemize}

\subsection{Construct Validity}\label{cons-valid}
For clarity and consistency throughout this \docType, we define the terminology
we use in \Cref{terminology}. The following threats to validity come from our
use of \citepos{IEEE2022} categorization scheme; while it is supported by the
testing literature, we make the final decision to use it as described in
\Cref{cats-def}:

\begin{itemize}
    \item \textbf{Similar Approach Categories:}
          % As we outline in \Cref{cats-def},
          % the categorization given in \Cref{tab:ieeeCats} is used fairly
          % consistently by the literature. However, 
          Some sources \citep[such as][]{SWEBOK2025,BarbosaEtAl2006} propose
          similar yet distinct categories that clash or overlap with our
          categories given in \Cref{tab:ieeeCats}. We give these alternate
          categorizations, which seem to simply map to their ``IEEE
          Equivalent''s, in \Cref{tab:otherCats}. While these categories
          could provide new perspectives and be useful in some contexts, either
          in place of or in tandem with ours, their existence suggests that
          \citepos{IEEE2022} categorization scheme is not universal.
    \item \phantomsection{}\label{alt-cats}
          \textbf{Alternate Approach Categories:} Some of these categories can
          be further divided into ``classes'' or ``families'' such as
          %   with ``commonalities and well-identified variabilities that can be
          %   instantiated'', where ``the commonalities are large and the variabilities
          %   smaller'' \citep{classFamilyDisc}. Examples of these are
          the classes of combinatorial \citep[p.~15]{IEEE2021c} and data flow
          testing \citetext{p.~3} and the family of performance-related testing
          \citep[p.~1187]{Moghadam2019}\footnote{The original source describes
              ``performance testing \dots\ as a family of performance-related
              testing techniques'', but it makes more sense to consider
              ``performance-related testing'' as the ``family'' with
              ``performance testing'' being one of the variabilities
              (see \Cref{perf-test-rec}).}.

          Similarly, we find many other
          criteria for categorizing test approaches in the literature.
          % which may be used in tandem with or in place of those given
          % in \Cref{tab:ieeeCats,tab:otherCats}.
          In general, these are defined less systematically but are more
          fine-grained, seeming to ``specialize'' our categories from
          \Cref{tab:ieeeCats}. The existence of these categorizations is not
          inherently wrong, as they may be useful for specific teams or in
          certain contexts. For example, functional testing and structural
          testing ``use different sources of information and have been shown to
          highlight different problems'', and deterministic testing and random
          testing have ``conditions that make one approach more effective than
          the other'' \citep[p.~5\=/16]{SWEBOK2025}. Unfortunately, even these
          alternate categories are not used consistently
          (see \flawref{manual-or-keyword})! While these categories suggest
          that ours are not complete or minimal, they seem to be supplementary
          and generally do not conflict with them.\qtodo{Does this make sense,
              especially in the absence of the examples I omitted for brevity?}
          % Examples omitted for brevity
          % Threat omitted for clarity/brevity
          % \item \textbf{Unclear Category Boundaries:} Note that ``there is a lot of
          %       overlap between different classes of testing'' \citep[p.~8]{Firesmith2015},
          %       meaning that ``one category [of test techniques] might deal with
          %       combining two or more techniques'' \citep[p.~5\=/10]{SWEBOK2025}.
          %       %   For example, ``performance, load and stress
          %       %   testing might considerably overlap in many areas'' \citep[p.~1187]{Moghadam2019}.
          %       A side effect of this is that it is difficult to ``untangle'' these
          %       classes; for example, take the following sentence: ``whitebox fuzzing
          %       extends dynamic test generation based on symbolic execution and
          %       constraint solving from unit testing to whole-application security
          %       testing'' \citep[p.~23]{GodefroidAndLuchaup2011}! As before,
\end{itemize}

% \clearpage
% \afterpage{
\begin{landscape}%
    % Omitted from paper for brevity
    \otherCatsTable{}%
\end{landscape}
% }

Threats can also be found in other terms we define, such as the following:

\begin{itemize}
    \item \phantomsection{}\label{flaw-def-threat}
          \textbf{Definition of Flaw:} We define a ``flaw'' as an instance
          where the software testing literature describes a testing-related
          term (especially a test approach) in a way that is incorrect,
          incomplete, inconsistent, and/or improperly coupled (see
          \Cref{flaw-def}). When picking a
          word to describe one of these instances, we wanted to avoid words
          ``overloaded with too many meanings'' like ``error'' and ``fault''
          \citep[p.~12\=/3; see \Cref{error-fault-failure} for more detailed
              discussion]{SWEBOK2025}. A small literature review revealed that
          established standards (see \Cref{stds}) primarily use the term
          ``flaw'' to refer software artifacts that are \emph{not} code:
          requirements \citep[p.~38]{IEEE2022}, design \citetext{p.~43}, and
          ``system security procedures \dots{} and internal controls''
          % under the term ``vulnerability''
          \citep[p.~194]{IEEE2012}. However, this term sometimes refers to
          problems with software itself \citep[p.~92;][p.~7\=/9]{SWEBOK2025},
          which introduces some confusion \PaigeMtgNote. We attempt to mitigate
          this by being precise with our use of the words ``flaw'', ``error'',
          ``fault'', ``defect'', and ``failure''.
    \item \textbf{Manifestation and Domain:} Similarly, we define the terms
          ``manifestation'' and ``domain'' (see \Cref{mnfst-def,dmn-def},
          respectively) specifically in the context of how
          flaws appear in the literature. However, these terms can also be used
          in the context of software itself: a fault is a manifestation of a
          human error \citep[p.~278]{IEEE2017} and a domain is a ``distinct
          scope'' \citetext{p.~145\todo{OG ISO/IEC, 2012}} or ``problem
          space'' \citeyearpar[p.~114\todo{OG IEEE Std 1517\=/2010}]{IEEE2010}.
          We likewise mitigate these threats by using these terms precisely
          throughout this \docType{}.
    \item \textbf{Notion of Credibility:} We also define a metric for ranking
          the impact a document has on testing literature as a whole. We call
          this metric ``credibility'' and provide some properties that a
          credible source should have in \Cref{cred}. While this influences how
          we sort sources into tiers in \Cref{source-tiers}, the format of a
          given source is a larger factor. Therefore, other researchers may
          have different ideas about what kinds of sources are more credible
          that others or how sources should be grouped to facilitate comparing
          them. We use credibility as a heuristic, describe each source tier
          thoroughly, and justify the use of our sources, mitigating (or at
          least minimizing the impact of) this threat.
\end{itemize}

\subsection{Internal Validity}\label{inter-valid}
Internal threats to our research result from relations that we observe based on
our constructs. The following are the most prominent of these threats:
% This lets us exclude requirements-derived approaches since
% there are only like 3 of them

\begin{itemize}
    \item \textbf{Overloaded Terms:} The ambiguity of natural language means
          that terms are often overloaded. In \Cref{tab:ieeeCats}, we describe
          the categories we use, but these terms may not be used in the same
          way universally; ``test type'', ``test technique'', ``test practice'',
          and ``test approach'' could all reasonably used as synonyms. For
          example, \citet[p.~45\ifnotpaper, emphasis added\fi]{Kam2008}
          defines interface testing as ``an integration \emph{test type} that
          is concerned with testing \dots{} interfaces'', but since \ifnotpaper
              he \else it \fi does not define ``test type'', this may not have
          special significance.
          % Use of ChatGPT omitted for brevity

          Additionally, \citet[p.~23]{Firesmith2015} uses the same acronym
          (``HIL'') for ``hardware-in-the-loop testing'' and
          ``human-in-the-loop testing''. We track this as a flaw (see
          \flawref{hil-acro}), but these terms ``might be disambiguated in
          practice as they often come at very different stages/phases of
          testing; a deeper domain analysis might reveal this''
          \PaigeMtgNote. Dr.~Paige also notes that a deeper domain analysis
          may explain why system testing is given as its own parent (as
          described in \Cref{selfPars}).

    \item \phantomsection{}\label{qual-test}
          \textbf{Qualities Implying Test Types:}
          Since test types are ``focused on specific quality characteristics''
          (\citealp[p.~15]{IEEE2022}; \citeyear[p.~7]{IEEE2021c};
          \citeyear[p.~473]{IEEE2017}), we posit that they can be derived from
          software qualities: ``capabilit[ies] of software product[s] to
          satisfy stated and implied needs when used under specified
          conditions'' \citetext{p.~424}\todo{OG ISO/IEC, 2014}.
          We track \qualityCount{} software qualities\thesisissueref{21,23,27}
          %in addition to test approaches
          following our procedure in \Cref{qual-supp-procedure}, ``upgrading''
          them to test types when they are mentioned (or implied) by a source.
          %   by removing its entry from this quality
          %   glossary and adding an associated test approach to
          %   \ourApproachGlossary{}, also outlined in \Cref{methodology}
          Examples of this include conformance testing
          (\citealp[p.~5\=/7]{SWEBOK2025}; \citealp[p.~25]{JardEtAl1999};
          implied by \citealp[p.~93]{IEEE2017}), efficiency testing
          \citep[p.~44]{Kam2008}, and survivability testing
          \citep[p.~40]{GhoshAndVoas1999}. While other researchers may disagree
          with this relation between software qualities and test types, it
          seems to be supported by the literature (as described above) and is
          further supported by reliability and performance testing: test types
          \citep{IEEE2022,IEEE2021c} that are based on their underlying
          software qualities \citep[p.~18]{FentonAndPfleeger1997}.
          % \ifnotpaper
          %     For quantifying quality-driven testing, measurements should include
          %     an entity to be measured, a specific attribute to measure, and the actual
          %     measure (i.e., units, starting state, ending state, what to include)
          %     \citetext{p.~36} where attributes must be
          %     defined before they can be measured \citetext{p.~38}.
          % \fi

    \item \phantomsection{}\label{cov-test}
          \textbf{Coverage Metrics Implying Test Techniques:}
          Test techniques are able to ``identify test coverage items \dots{} and
          derive corresponding test cases'' (\citealp[p.~11]{IEEE2022};
          \citeyear[p.~5]{IEEE2021a}; similar in \citeyear[p.~467]{IEEE2017})%
          %   in a ``systematic'' way \citeyearpar[p.~464]{IEEE2017}
          , which allows for ``the coverage achieved by a specific test design
          technique'' to be calculated as a percentage % of ``the number of
          %   test coverage items covered by executed test cases''
          \citeyearpar[p.~30]{IEEE2021c}. %   ``Coverage levels can range from
          %   0\% to 100\%'' and may or may not include ``infeasible'' test
          %   coverage items, which are ``not \dots{} executable or [are]
          %   impossible to be covered by a test case'' \citetext{p.~30}.
          %   Perhaps more interestingly, the further implication is that
          Therefore, we posit that a given coverage metric implies a test
          technique with the intent to maximize it. For example, path testing
          ``aims to execute all entry-to-exit control flow paths in a
          \acs{sut}'s control flow graph'' \citep[p.~5\=/13]{SWEBOK2025},
          thus maximizing the path coverage. Again, while other
          researchers may disagree with this relation between coverage
          metrics and test techniques, it seems to be supported by the
          literature as described above and by
          \citet[pp.~183\==185]{DoğanEtAl2014}\thesisissueref{63},
          \citet[Fig.~1]{SharmaEtAl2021}, and \citet[pp.~2\==3]{Reid1996}%
          %   , who elaborates on this relation specifically in regards to
          %   syntax testing and levels of coverage less than 100\%
          .
\end{itemize}

\subsection{External Validity}\label{exter-valid}
While we document issues with
``standardized'' software testing terminology so that they can be addressed in
the future, some dismiss the importance of standardized terminology for various
reasons, including the following:
\begin{itemize}
    \item \textbf{Limitations of Standardized Terminology:} \citet{Schoots2014}
          holds that ``common terminology is dangerous'' and ``to be able to
          truly understand each other, we need to ask questions and discuss in
          depth''. However, these in-depth discussions are \emph{not} mutually
          exclusive with common terminology! Having a shared understanding for
          how terms are defined allows for common ground during these sorts of
          discussions with the chance to adapt them to serve the context of a
          given team or project (which we give examples of in
          \Cref{syn-rels,alt-cats}).
    \item \textbf{Standards Being Mandated:} \citet{Schoots2014} also states
          that while he wishes ``standards would be guidelines, \dots{} reality
          shows standards become mandatory often''. He supports this with
          examples from \citet{Soundararajan2015} where ``contracts and bids
          from large companies'' often ``reference[] ISO linking to industry
          best practices''. This claim, however, overlooks:
          \begin{enumerate}
              \item the possibility of renegotiating contracts and
                    % This doesn't make the point I thought it did initially
                    %   \item ISO's goals of ``clearly demonstrat[ing] the benefits of
                    %         using ISO standards'' and working with stakeholders in
                    %         their development \citep{ISO_Priorities}, and
              \item the notion of ``tailored conformance'' to standards, which
                    is mentioned throughout the family of standards these
                    authors critique (\citealp[pp.~9\=/10]{IEEE2021a};
                    \citeyear[pp.~5, 17, 37]{IEEE2021b};
                    \citeyear[p.~7]{IEEE2021c}) and was perhaps
                    introduced later to address concerns such as these.
          \end{enumerate}
\end{itemize}

% \subsection{Threats from Approach Categories}\label{cat-threats}

% In \Cref{cats-def}, we outline our criteria for categorizing test approaches
% based on documents by \citeauthor{IEEE2022} (such as \citeyear[Fig.~2]{IEEE2022})
% and summarize it in \Cref{tab:ieeeCats}. However, the terms we use are
% sometimes used in a general sense (as opposed to our specific definitions)
% which makes it hard to evaluate how consistently the literature uses them
% (\Cref{overloaded-cats}). Furthermore, the literature provides other criteria
% for categorizing test approaches (\Cref{alt-cats}); using any of these schemas
% would affect the category data we collect, if any, and how we use them to
% identify the flaws given in \Cref{cats}.

% \subsubsection{Overloaded Terms}\label{overloaded-cats}

% A side effect of using the terms ``level'', ``type'', ``technique'', and
% ``practice'' is that they---perhaps excluding ``level''---can be used
% interchangably or as synonyms for ``approach''. Because natural language can be
% ambiguous, we need to exercise judgement when determining if these terms are
% being used in a general or technical sense. For example,
% \citet[p.~45\ifnotpaper, emphasis added\fi]{Kam2008}
% defines interface testing as ``an integration \emph{test type} that is
% concerned with testing \dots{} interfaces'', but since \ifnotpaper he \else it
% \fi does not define ``test type'', this may not have special significance.
% % \ifnotpaper We consider these kinds of ``categorizations'' to be
% %     inferred (see \Cref{infers}) and mark them with a question mark (?)~during
% %     data collection. When we evaluate test approaches categorized more than
% %     once in \Cref{multiCats}, we list well-defined categorizations in
% %     \Cref{tab:multiCats} and inferred ones in \Cref{tab:infMultiCats}.
% % \fi

% \phantomsection{}\label{use-of-chatgpt}
% We are not immune to using these terms sloppily either! For example,
% \citet[p.~88]{Patton2006} says that if a specific defect is found, it is wise
% to look for other defects in the same location and for similar defects in other
% locations, but does not provide a name for this approach. After researching in
% vain, we ask ChatGPT\footnote{We do \emph{not} take ChatGPT's output to be
%     true at face value; this approach seems to be called ``defect-based
%     testing'' based on the principle of ``defect clustering''
%     \citep{ChatGPT2024}, which \citet{RusEtAl2008} \multiAuthHelper{support}.}
% to name the ``\emph{type} of software testing that focuses on looking for bugs
% where others have already been found''
% \ifnotpaper \citep[emphasis added]{ChatGPT2024}\else \cite{ChatGPT2024}\fi%
% \qtodo{Is this sufficient to explain ChatGPT's role in our methodology?},
% using the word ``type'' in a general sense, akin to ``kind'' or ``subset''.
% Interestingly, ChatGPT ``corrects'' our imprecise term in its response,
% using the more correct term ``approach'' (although it may have been biased by
% our previous usage of these terms)!

% \subsubsection{Alternate Approach Categorizations}\label{alt-cats}

% While our categorization in \Cref{tab:ieeeCats} is used fairly consistently,
% other sources \citep[such as][]{SWEBOK2025,BarbosaEtAl2006} propose similar yet
% distinct categories that clash or overlap with them. We give these alternate
% categorizations in \Cref{tab:otherCats}, which could simply map to their
% ``IEEE Equivalents'' or could provide new perspectives and be useful in some
% contexts, either in place of or in tandem with ours.

% \afterpage{\begin{landscape}%
%         % Omitted from paper for brevity
%         \otherCatsTable{}%
%     \end{landscape}}

% Similarly, we find many other criteria for categorizing test approaches in
% the literature.
% % which may be used in tandem with or in place of those given
% % in \Cref{tab:ieeeCats,tab:otherCats}.
% In general, these are defined less systematically but are more fine-grained,
% seeming to ``specialize'' our categories from \Cref{tab:ieeeCats}. The
% existence of these categorizations
% is not inherently wrong, as they may be useful for specific teams or in
% certain contexts. For example, functional testing and structural testing
% ``use different sources of information and have been shown to highlight
% different problems'', and deterministic testing and random testing have
% ``conditions that make one approach more effective than the other''
% \citep[p.~5\=/16]{SWEBOK2025}. Unfortunately, even these alternate
% categories are not used consistently (see \flawref{manual-or-keyword})!

% %% Omitted for brevity
% We include the most prominent of these alternate categorizations in
% \Cref{tab:otherCategorizations} for completeness. In each row, the source
% given lists the example approaches and categorizes them as the given
% ``Parent IEEE Category'' unless stated otherwise (in some cases, the source
% gives additional approaches that we omit for brevity). For example, in the
% first row, \citet[pp.~22, 35]{IEEE2022}
% categorize both manual testing and automated testing as test practices.
% Note that since ``approach'' is a catch-all category (see \Cref{approach-def}),
% it does not require an explicit categorization, and that these categorizations
% may be flawed, as stated in the provided footnotes.

% \afterpage{
%     \begin{landscape}
%         % Omitted from paper for brevity
%         \otherCategorizationsTable{}
%     \end{landscape}
% }

% \newpage

% \phantomsection{}\label{method-family}
% Another way to subdivide the IEEE categories is by grouping related test
% approaches into a ``class'' or ``family'' with ``commonalities and
% well-identified variabilities that can be
% instantiated'', where ``the commonalities are large and the variabilities
% smaller'' \citep{classFamilyDisc}. Examples of these are the classes of
% combinatorial \citep[p.~15]{IEEE2021c} and data flow testing \citetext{p.~3}
% and the family of performance-related testing \citep[p.~1187]{Moghadam2019}%
% \footnote{The original source describes ``performance testing \dots\ as a family
%     of performance-related testing techniques'', but it makes more sense to
%     consider ``performance-related testing'' as the ``family'' with
%     ``performance testing'' being one of the variabilities
%     (see \Cref{perf-test-rec}).}. Note that ``there is a lot of overlap between
% different classes of testing'' \citep[p.~8]{Firesmith2015}, meaning that ``one
% category [of test techniques] might deal with combining two or more techniques''
% \citep[p.~5-10]{SWEBOK2025}. For example, ``performance, load and stress
% testing might considerably overlap in many areas'' \citep[p.~1187]{Moghadam2019}.
% A side effect of this is that it is difficult to ``untangle'' these classes;
% for example, take the following sentence: ``whitebox fuzzing extends dynamic
% test generation based on symbolic execution and constraint solving from unit
% testing to whole-application security testing''
% \citep[p.~23]{GodefroidAndLuchaup2011}! This is, in part, why research on
% software testing terminology is so vital.\qtodo{Is this sentence needed?}
