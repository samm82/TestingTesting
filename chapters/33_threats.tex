\section{Threats to Validity}\label{threats}

A case study is valid if its ``results are true and not biased by the
researchers' subjective point of view'' \citep[p.~153]{RunesonAndHöst2009}.
Since our research is a case study on software testing terminology, we do our
best to accomplish this. However, to present and explain our findings,
we make some decisions with which others may disagree. The benefit of our work
being open-source means that others can make their own decisions, modify our
data/code appropriately, and observe how this changes the results. Despite this
and our best efforts, there remain some threats to validity that we describe in
the following subsections.

\subsection{Reliability}\label{rel-valid}
If another researcher were to conduct the case study later on and get the same
result, then the case study is reliable
\citep[p.~154]{RunesonAndHöst2009}.

\subsection{Internal Validity}\label{inter-valid}
``When \dots{} investigating whether one factor affects an investigated factor[,]
there is a risk that the investigated factor is also affected by a third factor''
\citep[p.~154]{RunesonAndHöst2009}. This threatens internal validity ``if the
researcher is not aware of the third factor and/or does not know to what extent
it affects the investigated factor'' (p.~154).

\begin{itemize}
    \item \textbf{Overloaded Terms:} The ambiguity of natural language means
          that terms are often overloaded. In \Cref{tab:ieeeCats}, we describe
          the categories we use, but these terms may not be used in the same
          way universally; ``test type'', ``test technique'', ``test practice'',
          and ``test approach'' could all reasonably used as synonyms. For
          example, \citet[p.~45\ifnotpaper, emphasis added\fi]{Kam2008}
          defines interface testing as ``an integration \emph{test type} that
          is concerned with testing \dots{} interfaces'', but since \ifnotpaper
              he \else it \fi does not define ``test type'', this may not have
          special significance.
          % Use of ChatGPT omitted for brevity
    \item \textbf{Similar Approach Categories:} While our categorization in
          \Cref{tab:ieeeCats} is used fairly consistently, other sources
          \citep[such as][]{SWEBOK2024,BarbosaEtAl2006} propose similar yet
          distinct categories that clash or overlap with them. We give these
          alternate categorizations in \Cref{tab:otherCats}, which could simply
          map to their ``IEEE Equivalents'' or could provide new perspectives
          and be useful in some contexts, either in place of or in tandem with
          ours.
    \item \phantomsection{}\label{alt-cats}
          \textbf{Alternate Approach Categories:} Similarly, we find many other
          criteria for categorizing test approaches in the literature.
          % which may be used in tandem with or in place of those given
          % in \Cref{tab:ieeeCats,tab:otherCats}.
          In general, these are defined less systematically but are more
          fine-grained, seeming to ``specialize'' our categories from
          \Cref{tab:ieeeCats}. The existence of these categorizations is not
          inherently wrong, as they may be useful for specific teams or in
          certain contexts. For example, functional testing and structural
          testing ``use different sources of information and have been shown to
          highlight different problems'', and deterministic testing and random
          testing have ``conditions that make one approach more effective than
          the other'' \citep[p.~5\=/16]{SWEBOK2024}. Unfortunately, even these
          alternate categories are not used consistently
          (see \flawref{manual-or-keyword})!
          % Examples omitted for brevity
    \item \textbf{Unclear Category Boundaries:} Some of these categories can
          also be grouped into ``classes'' or ``families'' such as
          %   with ``commonalities and well-identified variabilities that can be
          %   instantiated'', where ``the commonalities are large and the variabilities
          %   smaller'' \citep{classFamilyDisc}. Examples of these are
          the classes of combinatorial \citep[p.~15]{IEEE2021c} and data flow
          testing \citetext{p.~3} and the family of performance-related testing
          \citep[p.~1187]{Moghadam2019}\footnote{The original source describes
              ``performance testing \dots\ as a family of performance-related
              testing techniques'', but it makes more sense to consider
              ``performance-related testing'' as the ``family'' with
              ``performance testing'' being one of the variabilities
              (see \Cref{perf-test-rec}).}. Note that ``there is a lot of
          overlap between different classes of testing'' \citep[p.~8]{Firesmith2015},
          meaning that ``one category [of test techniques] might deal with
          combining two or more techniques'' \citep[p.~5\=/10]{SWEBOK2024}.
          %   For example, ``performance, load and stress
          %   testing might considerably overlap in many areas'' \citep[p.~1187]{Moghadam2019}.
          A side effect of this is that it is difficult to ``untangle'' these
          classes; for example, take the following sentence: ``whitebox fuzzing
          extends dynamic test generation based on symbolic execution and
          constraint solving from unit testing to whole-application security
          testing'' \citep[p.~23]{GodefroidAndLuchaup2011}!
\end{itemize}

\clearpage
% \afterpage{
\begin{landscape}%
    % Omitted from paper for brevity
    \otherCatsTable{}%
\end{landscape}
% }

\subsection{External Validity}\label{exter-valid}
For results of a study to be externally valid, they should be generalizable and
``of interest to other people outside the investigated case''
\citep[p.~154]{RunesonAndHöst2009}. While we document issues with
``standardized'' software testing terminology so that they can be addressed in
the future, some dismiss the importance of standardized terminology for various
reasons, including the following:
\begin{itemize}
    \item \textbf{Limitations of Standardized Terminology:} \citet{Schoots2014}
          holds that ``common terminology is dangerous'' and ``to be able to
          truly understand each other, we need to ask questions and discuss in
          depth''. However, these in-depth discussions are \emph{not} mutually
          exclusive with common terminology! Having a shared understanding for
          how terms are defined allows for common ground during these sorts of
          discussions with the chance to adapt them to serve the context of a
          given team or project (which we give examples of in
          \Cref{syn-rels,alt-cats}).
    \item \textbf{Standards Being Mandated:} \citet{Schoots2014} also states
          that while he wishes ``standards would be guidelines, \dots{} reality
          shows standards become mandatory often''. He supports this with
          examples from \citet{Soundararajan2015} where ``contracts and bids
          from large companies'' often ``reference[] ISO linking to industry
          best practices''. This claim, however, overlooks:
          \begin{enumerate}
              \item the possibility of renegotiating contracts and
                    % This doesn't make the point I thought it did initially
                    %   \item ISO's goals of ``clearly demonstrat[ing] the benefits of
                    %         using ISO standards'' and working with stakeholders in
                    %         their development \citep{ISO_Priorities}, and
              \item the notion of ``tailored conformance'' to standards, which
                    is mentioned throughout the family of standards these
                    authors critique (\citealp[pp.~9\=/10]{IEEE2021a};
                    \citeyear[pp.~5, 17, 37]{IEEE2021b};
                    \citeyear[p.~7]{IEEE2021c}) and was perhaps
                    introduced later to address concerns such as these.
          \end{enumerate}
\end{itemize}

\subsection{Construct Validity}\label{cons-valid}
``Construct validity'' refers to the extent to which ``the operational measures
that are studied really represent what the researcher[s] have in mind and what
is investigated'', or how well the studied data aligns with the data that the
researchers \emph{intended} to study \citep[p.~153]{RunesonAndHöst2009}.

% \subsection{Threats from Approach Categories}\label{cat-threats}

% In \Cref{cats-def}, we outline our criteria for categorizing test approaches
% based on documents by \citeauthor{IEEE2022} (such as \citeyear[Fig.~2]{IEEE2022})
% and summarize it in \Cref{tab:ieeeCats}. However, the terms we use are
% sometimes used in a general sense (as opposed to our specific definitions)
% which makes it hard to evaluate how consistently the literature uses them
% (\Cref{overloaded-cats}). Furthermore, the literature provides other criteria
% for categorizing test approaches (\Cref{alt-cats}); using any of these schemas
% would affect the category data we collect, if any, and how we use them to
% identify the flaws given in \Cref{cats}.

% \subsubsection{Overloaded Terms}\label{overloaded-cats}

% A side effect of using the terms ``level'', ``type'', ``technique'', and
% ``practice'' is that they---perhaps excluding ``level''---can be used
% interchangably or as synonyms for ``approach''. Because natural language can be
% ambiguous, we need to exercise judgement when determining if these terms are
% being used in a general or technical sense. For example,
% \citet[p.~45\ifnotpaper, emphasis added\fi]{Kam2008}
% defines interface testing as ``an integration \emph{test type} that is
% concerned with testing \dots{} interfaces'', but since \ifnotpaper he \else it
% \fi does not define ``test type'', this may not have special significance.
% % \ifnotpaper We consider these kinds of ``categorizations'' to be
% %     inferred (see \Cref{infers}) and mark them with a question mark (?)~during
% %     data collection. When we evaluate test approaches categorized more than
% %     once in \Cref{multiCats}, we list well-defined categorizations in
% %     \Cref{tab:multiCats} and inferred ones in \Cref{tab:infMultiCats}.
% % \fi

% \phantomsection{}\label{use-of-chatgpt}
% We are not immune to using these terms sloppily either! For example,
% \citet[p.~88]{Patton2006} says that if a specific defect is found, it is wise
% to look for other defects in the same location and for similar defects in other
% locations, but does not provide a name for this approach. After researching in
% vain, we ask ChatGPT\footnote{We do \emph{not} take ChatGPT's output to be
%     true at face value; this approach seems to be called ``defect-based
%     testing'' based on the principle of ``defect clustering''
%     \citep{ChatGPT2024}, which \citet{RusEtAl2008} \multiAuthHelper{support}.}
% to name the ``\emph{type} of software testing that focuses on looking for bugs
% where others have already been found''
% \ifnotpaper \citep[emphasis added]{ChatGPT2024}\else \cite{ChatGPT2024}\fi%
% \qtodo{Is this sufficient to explain ChatGPT's role in our methodology?},
% using the word ``type'' in a general sense, akin to ``kind'' or ``subset''.
% Interestingly, ChatGPT ``corrects'' our imprecise term in its response,
% using the more correct term ``approach'' (although it may have been biased by
% our previous usage of these terms)!

% \subsubsection{Alternate Approach Categorizations}\label{alt-cats}

% While our categorization in \Cref{tab:ieeeCats} is used fairly consistently,
% other sources \citep[such as][]{SWEBOK2024,BarbosaEtAl2006} propose similar yet
% distinct categories that clash or overlap with them. We give these alternate
% categorizations in \Cref{tab:otherCats}, which could simply map to their
% ``IEEE Equivalents'' or could provide new perspectives and be useful in some
% contexts, either in place of or in tandem with ours.

% \afterpage{\begin{landscape}%
%         % Omitted from paper for brevity
%         \otherCatsTable{}%
%     \end{landscape}}

% Similarly, we find many other criteria for categorizing test approaches in
% the literature.
% % which may be used in tandem with or in place of those given
% % in \Cref{tab:ieeeCats,tab:otherCats}.
% In general, these are defined less systematically but are more fine-grained,
% seeming to ``specialize'' our categories from \Cref{tab:ieeeCats}. The
% existence of these categorizations
% is not inherently wrong, as they may be useful for specific teams or in
% certain contexts. For example, functional testing and structural testing
% ``use different sources of information and have been shown to highlight
% different problems'', and deterministic testing and random testing have
% ``conditions that make one approach more effective than the other''
% \citep[p.~5\=/16]{SWEBOK2024}. Unfortunately, even these alternate
% categories are not used consistently (see \flawref{manual-or-keyword})!

% %% Omitted for brevity
% We include the most prominent of these alternate categorizations in
% \Cref{tab:otherCategorizations} for completeness. In each row, the source
% given lists the example approaches and categorizes them as the given
% ``Parent IEEE Category'' unless stated otherwise (in some cases, the source
% gives additional approaches that we omit for brevity). For example, in the
% first row, \citet[pp.~22, 35]{IEEE2022}
% categorize both manual testing and automated testing as test practices.
% Note that since ``approach'' is a catch-all category (see \Cref{approach-def}),
% it does not require an explicit categorization, and that these categorizations
% may be flawed, as stated in the provided footnotes.

% \afterpage{
%     \begin{landscape}
%         % Omitted from paper for brevity
%         \otherCategorizationsTable{}
%     \end{landscape}
% }

% \newpage

% \phantomsection{}\label{method-family}
% Another way to subdivide the IEEE categories is by grouping related test
% approaches into a ``class'' or ``family'' with ``commonalities and
% well-identified variabilities that can be
% instantiated'', where ``the commonalities are large and the variabilities
% smaller'' \citep{classFamilyDisc}. Examples of these are the classes of
% combinatorial \citep[p.~15]{IEEE2021c} and data flow testing \citetext{p.~3}
% and the family of performance-related testing \citep[p.~1187]{Moghadam2019}%
% \footnote{The original source describes ``performance testing \dots\ as a family
%     of performance-related testing techniques'', but it makes more sense to
%     consider ``performance-related testing'' as the ``family'' with
%     ``performance testing'' being one of the variabilities
%     (see \Cref{perf-test-rec}).}. Note that ``there is a lot of overlap between
% different classes of testing'' \citep[p.~8]{Firesmith2015}, meaning that ``one
% category [of test techniques] might deal with combining two or more techniques''
% \citep[p.~5-10]{SWEBOK2024}. For example, ``performance, load and stress
% testing might considerably overlap in many areas'' \citep[p.~1187]{Moghadam2019}.
% A side effect of this is that it is difficult to ``untangle'' these classes;
% for example, take the following sentence: ``whitebox fuzzing extends dynamic
% test generation based on symbolic execution and constraint solving from unit
% testing to whole-application security testing''
% \citep[p.~23]{GodefroidAndLuchaup2011}! This is, in part, why research on
% software testing terminology is so vital.\qtodo{Is this sentence needed?}
