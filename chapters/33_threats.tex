\section{Threats to Validity}\label{threats}

A case study is valid if its ``results are true and not biased by the
researchers' subjective point of view'' \citep[p.~153]{RunesonAndHöst2009}.
Since our research is a systematic analysis and summary of the software
community's usage of testing terminology, we do our
best to accomplish this. To present and explain our findings,
we make some decisions with which others may disagree. The benefit of our work
being open-source means that others can make their own decisions, modify our
data/code appropriately, and observe how this changes the results. Despite this
and our best efforts, some threats to validity remain; we use
\citepos[pp.~153\==154]{RunesonAndHöst2009} definitions of threats to validity
and list them in their respective subsections as follows:
\begin{enumerate}
    \item \textbf{Reliability:} If another researcher were to conduct the case
          study later on and get the same result, then the case study is
          reliable. (\Cref{rel-valid})
    \item \textbf{Construct Validity:} This refers to the extent to which ``the
          operational measures that are studied really represent what the
          researcher[s] have in mind and what is investigated'', or how well
          the studied data aligns with the data that the researchers
          \emph{intended} to study. (\Cref{cons-valid})
    \item \textbf{Internal Validity:} A study is internally valid if the
          discovered causal relations are trustworthy and cannot be explained
          by other factors. % ``When \dots{} investigating whether one
          %   factor affects an investigated factor[,] there is a risk that the
          %   investigated factor is also affected by a third factor''. This
          %   threatens internal validity ``if the researcher is not aware of the
          %   third factor and/or does not know to what extent it affects the
          %   investigated factor''.
          (\Cref{inter-valid})
    \item \textbf{External Validity:} For results of a study to be externally
          valid, they should be generalizable and ``of interest to other people
          outside the investigated case''. (\Cref{exter-valid})
\end{enumerate}

\subsection{Reliability}\label{rel-valid}
Natural language is nuanced and software testing has a wide scope, so we have
to make judgement calls throughout the research process, usually involving
which information to record. Other researchers may make different decisions,
which would threaten the reliability of our results. We mitigate this at a high
level by outlining what we exclude from our scope in \Cref{app-scope}. This
means that even if other researchers decide on a different scope, the data and
results within the intersection of these scopes should match. By following the
methodology outlined in \Cref{methodology}, we further mitgate the following
threats to reliability:

\begin{itemize}
    \item \textbf{Single Researcher:} Since only one researcher was responsible
          for data collection, it was solely up to them to determine what data
          and relations were important to record and investigate. To reduce the
          negative impact this might have on our research, we follow our
          methodology as rigorously as possible, including heuristics
          for what data is important (see \Cref{ident-terms}). When ambiguity
          arises or more involved judgement is required, we discuss these
          details as a team to include more perspectives, reducing the amount
          of bias and oversight that would propagate throughout our research%
          \ifblind{}{; these discussions can be found \href{
                  https://github.com/samm82/TestingTesting/issues?q=state\%3Aclosed label\%3Adiscussion
              }{on our repo}}.

    \item \textbf{Implicit Information:} While natural language can be
          ambiguous, only recording explicit information would omit a lot of
          data provided by the software testing literature. We therefore
          document implicit information from the
          literature for completeness as described in \Cref{imp-info}. While
          other researchers may disagree on what information ``follows
          logically'' or ``is dubious'', these data are innately subjective,
          so excluding them should result in our \emph{explicit} data (and
          the results based on them) matching those of other researchers.

    \item \textbf{Drift Over Time:} Since we began recording data
          systematically in January 2024\ifblind{}{\ (see the \href{
                  https://github.com/samm82/TestingTesting/commit/d1b2a12
              }{first version of our test approach glossary})}, we have
          iterated on what we consider to be in scope and what methodology to
          use. Some sources we investigate have even been updated (see
          \Cref{swebok-update})! Future researchers following our methodology
          would therefore end up with different results. While we could
          partially mitigate this by re-iterating over all sources with our
          ``finalized'' methodology, this is not possible due to time
          constraints. With respect to sources updating during the course of
          our research, future researchers could replicate our results by
          reviewing a ``snapshot'' of them from when we reviewed\qtodo{Present
              tense? And does this make sense?} them. However, if future
          researchers were to repeat this research, they \emph{should} use the
          most up-to-date sources to update our existing data, methodology, and
          tools. This is one of the benefits of our research being open-source:
          it can exist as a ``living document'' that can (ideally) keep up with
          innovations to software testing!\qtodo{Is this valid to include in
              this chapter?}
\end{itemize}

\subsection{Construct Validity}\label{cons-valid}
\begin{itemize}
    \item \textbf{Similar Approach Categories:} While our categorization in
          \Cref{tab:ieeeCats} is used fairly consistently, other sources
          \citep[such as][]{SWEBOK2024,BarbosaEtAl2006} propose similar yet
          distinct categories that clash or overlap with them. We give these
          alternate categorizations in \Cref{tab:otherCats}, which could simply
          map to their ``IEEE Equivalents'' or could provide new perspectives
          and be useful in some contexts, either in place of or in tandem with
          ours.
    \item \phantomsection{}\label{alt-cats}
          \textbf{Alternate Approach Categories:} Similarly, we find many other
          criteria for categorizing test approaches in the literature.
          % which may be used in tandem with or in place of those given
          % in \Cref{tab:ieeeCats,tab:otherCats}.
          In general, these are defined less systematically but are more
          fine-grained, seeming to ``specialize'' our categories from
          \Cref{tab:ieeeCats}. The existence of these categorizations is not
          inherently wrong, as they may be useful for specific teams or in
          certain contexts. For example, functional testing and structural
          testing ``use different sources of information and have been shown to
          highlight different problems'', and deterministic testing and random
          testing have ``conditions that make one approach more effective than
          the other'' \citep[p.~5\=/16]{SWEBOK2024}. Unfortunately, even these
          alternate categories are not used consistently
          (see \flawref{manual-or-keyword})!
          % Examples omitted for brevity
    \item \textbf{Unclear Category Boundaries:} Some of these categories can be
          further divided into ``classes'' or ``families'' such as
          %   with ``commonalities and well-identified variabilities that can be
          %   instantiated'', where ``the commonalities are large and the variabilities
          %   smaller'' \citep{classFamilyDisc}. Examples of these are
          the classes of combinatorial \citep[p.~15]{IEEE2021c} and data flow
          testing \citetext{p.~3} and the family of performance-related testing
          \citep[p.~1187]{Moghadam2019}\footnote{The original source describes
              ``performance testing \dots\ as a family of performance-related
              testing techniques'', but it makes more sense to consider
              ``performance-related testing'' as the ``family'' with
              ``performance testing'' being one of the variabilities
              (see \Cref{perf-test-rec}).}. Note that ``there is a lot of
          overlap between different classes of testing'' \citep[p.~8]{Firesmith2015},
          meaning that ``one category [of test techniques] might deal with
          combining two or more techniques'' \citep[p.~5\=/10]{SWEBOK2024}.
          %   For example, ``performance, load and stress
          %   testing might considerably overlap in many areas'' \citep[p.~1187]{Moghadam2019}.
          A side effect of this is that it is difficult to ``untangle'' these
          classes; for example, take the following sentence: ``whitebox fuzzing
          extends dynamic test generation based on symbolic execution and
          constraint solving from unit testing to whole-application security
          testing'' \citep[p.~23]{GodefroidAndLuchaup2011}!
\end{itemize}

% \clearpage
% \afterpage{
\begin{landscape}%
    % Omitted from paper for brevity
    \otherCatsTable{}%
\end{landscape}
% }

\subsection{Internal Validity}\label{inter-valid}

\begin{itemize}
    \item \textbf{Overloaded Terms:} The ambiguity of natural language means
          that terms are often overloaded. In \Cref{tab:ieeeCats}, we describe
          the categories we use, but these terms may not be used in the same
          way universally; ``test type'', ``test technique'', ``test practice'',
          and ``test approach'' could all reasonably used as synonyms. For
          example, \citet[p.~45\ifnotpaper, emphasis added\fi]{Kam2008}
          defines interface testing as ``an integration \emph{test type} that
          is concerned with testing \dots{} interfaces'', but since \ifnotpaper
              he \else it \fi does not define ``test type'', this may not have
          special significance.
          % Use of ChatGPT omitted for brevity
\end{itemize}

\subsection{External Validity}\label{exter-valid}
While we document issues with
``standardized'' software testing terminology so that they can be addressed in
the future, some dismiss the importance of standardized terminology for various
reasons, including the following:
\begin{itemize}
    \item \textbf{Limitations of Standardized Terminology:} \citet{Schoots2014}
          holds that ``common terminology is dangerous'' and ``to be able to
          truly understand each other, we need to ask questions and discuss in
          depth''. However, these in-depth discussions are \emph{not} mutually
          exclusive with common terminology! Having a shared understanding for
          how terms are defined allows for common ground during these sorts of
          discussions with the chance to adapt them to serve the context of a
          given team or project (which we give examples of in
          \Cref{syn-rels,alt-cats}).
    \item \textbf{Standards Being Mandated:} \citet{Schoots2014} also states
          that while he wishes ``standards would be guidelines, \dots{} reality
          shows standards become mandatory often''. He supports this with
          examples from \citet{Soundararajan2015} where ``contracts and bids
          from large companies'' often ``reference[] ISO linking to industry
          best practices''. This claim, however, overlooks:
          \begin{enumerate}
              \item the possibility of renegotiating contracts and
                    % This doesn't make the point I thought it did initially
                    %   \item ISO's goals of ``clearly demonstrat[ing] the benefits of
                    %         using ISO standards'' and working with stakeholders in
                    %         their development \citep{ISO_Priorities}, and
              \item the notion of ``tailored conformance'' to standards, which
                    is mentioned throughout the family of standards these
                    authors critique (\citealp[pp.~9\=/10]{IEEE2021a};
                    \citeyear[pp.~5, 17, 37]{IEEE2021b};
                    \citeyear[p.~7]{IEEE2021c}) and was perhaps
                    introduced later to address concerns such as these.
          \end{enumerate}
\end{itemize}

% \subsection{Threats from Approach Categories}\label{cat-threats}

% In \Cref{cats-def}, we outline our criteria for categorizing test approaches
% based on documents by \citeauthor{IEEE2022} (such as \citeyear[Fig.~2]{IEEE2022})
% and summarize it in \Cref{tab:ieeeCats}. However, the terms we use are
% sometimes used in a general sense (as opposed to our specific definitions)
% which makes it hard to evaluate how consistently the literature uses them
% (\Cref{overloaded-cats}). Furthermore, the literature provides other criteria
% for categorizing test approaches (\Cref{alt-cats}); using any of these schemas
% would affect the category data we collect, if any, and how we use them to
% identify the flaws given in \Cref{cats}.

% \subsubsection{Overloaded Terms}\label{overloaded-cats}

% A side effect of using the terms ``level'', ``type'', ``technique'', and
% ``practice'' is that they---perhaps excluding ``level''---can be used
% interchangably or as synonyms for ``approach''. Because natural language can be
% ambiguous, we need to exercise judgement when determining if these terms are
% being used in a general or technical sense. For example,
% \citet[p.~45\ifnotpaper, emphasis added\fi]{Kam2008}
% defines interface testing as ``an integration \emph{test type} that is
% concerned with testing \dots{} interfaces'', but since \ifnotpaper he \else it
% \fi does not define ``test type'', this may not have special significance.
% % \ifnotpaper We consider these kinds of ``categorizations'' to be
% %     inferred (see \Cref{infers}) and mark them with a question mark (?)~during
% %     data collection. When we evaluate test approaches categorized more than
% %     once in \Cref{multiCats}, we list well-defined categorizations in
% %     \Cref{tab:multiCats} and inferred ones in \Cref{tab:infMultiCats}.
% % \fi

% \phantomsection{}\label{use-of-chatgpt}
% We are not immune to using these terms sloppily either! For example,
% \citet[p.~88]{Patton2006} says that if a specific defect is found, it is wise
% to look for other defects in the same location and for similar defects in other
% locations, but does not provide a name for this approach. After researching in
% vain, we ask ChatGPT\footnote{We do \emph{not} take ChatGPT's output to be
%     true at face value; this approach seems to be called ``defect-based
%     testing'' based on the principle of ``defect clustering''
%     \citep{ChatGPT2024}, which \citet{RusEtAl2008} \multiAuthHelper{support}.}
% to name the ``\emph{type} of software testing that focuses on looking for bugs
% where others have already been found''
% \ifnotpaper \citep[emphasis added]{ChatGPT2024}\else \cite{ChatGPT2024}\fi%
% \qtodo{Is this sufficient to explain ChatGPT's role in our methodology?},
% using the word ``type'' in a general sense, akin to ``kind'' or ``subset''.
% Interestingly, ChatGPT ``corrects'' our imprecise term in its response,
% using the more correct term ``approach'' (although it may have been biased by
% our previous usage of these terms)!

% \subsubsection{Alternate Approach Categorizations}\label{alt-cats}

% While our categorization in \Cref{tab:ieeeCats} is used fairly consistently,
% other sources \citep[such as][]{SWEBOK2024,BarbosaEtAl2006} propose similar yet
% distinct categories that clash or overlap with them. We give these alternate
% categorizations in \Cref{tab:otherCats}, which could simply map to their
% ``IEEE Equivalents'' or could provide new perspectives and be useful in some
% contexts, either in place of or in tandem with ours.

% \afterpage{\begin{landscape}%
%         % Omitted from paper for brevity
%         \otherCatsTable{}%
%     \end{landscape}}

% Similarly, we find many other criteria for categorizing test approaches in
% the literature.
% % which may be used in tandem with or in place of those given
% % in \Cref{tab:ieeeCats,tab:otherCats}.
% In general, these are defined less systematically but are more fine-grained,
% seeming to ``specialize'' our categories from \Cref{tab:ieeeCats}. The
% existence of these categorizations
% is not inherently wrong, as they may be useful for specific teams or in
% certain contexts. For example, functional testing and structural testing
% ``use different sources of information and have been shown to highlight
% different problems'', and deterministic testing and random testing have
% ``conditions that make one approach more effective than the other''
% \citep[p.~5\=/16]{SWEBOK2024}. Unfortunately, even these alternate
% categories are not used consistently (see \flawref{manual-or-keyword})!

% %% Omitted for brevity
% We include the most prominent of these alternate categorizations in
% \Cref{tab:otherCategorizations} for completeness. In each row, the source
% given lists the example approaches and categorizes them as the given
% ``Parent IEEE Category'' unless stated otherwise (in some cases, the source
% gives additional approaches that we omit for brevity). For example, in the
% first row, \citet[pp.~22, 35]{IEEE2022}
% categorize both manual testing and automated testing as test practices.
% Note that since ``approach'' is a catch-all category (see \Cref{approach-def}),
% it does not require an explicit categorization, and that these categorizations
% may be flawed, as stated in the provided footnotes.

% \afterpage{
%     \begin{landscape}
%         % Omitted from paper for brevity
%         \otherCategorizationsTable{}
%     \end{landscape}
% }

% \newpage

% \phantomsection{}\label{method-family}
% Another way to subdivide the IEEE categories is by grouping related test
% approaches into a ``class'' or ``family'' with ``commonalities and
% well-identified variabilities that can be
% instantiated'', where ``the commonalities are large and the variabilities
% smaller'' \citep{classFamilyDisc}. Examples of these are the classes of
% combinatorial \citep[p.~15]{IEEE2021c} and data flow testing \citetext{p.~3}
% and the family of performance-related testing \citep[p.~1187]{Moghadam2019}%
% \footnote{The original source describes ``performance testing \dots\ as a family
%     of performance-related testing techniques'', but it makes more sense to
%     consider ``performance-related testing'' as the ``family'' with
%     ``performance testing'' being one of the variabilities
%     (see \Cref{perf-test-rec}).}. Note that ``there is a lot of overlap between
% different classes of testing'' \citep[p.~8]{Firesmith2015}, meaning that ``one
% category [of test techniques] might deal with combining two or more techniques''
% \citep[p.~5-10]{SWEBOK2024}. For example, ``performance, load and stress
% testing might considerably overlap in many areas'' \citep[p.~1187]{Moghadam2019}.
% A side effect of this is that it is difficult to ``untangle'' these classes;
% for example, take the following sentence: ``whitebox fuzzing extends dynamic
% test generation based on symbolic execution and constraint solving from unit
% testing to whole-application security testing''
% \citep[p.~23]{GodefroidAndLuchaup2011}! This is, in part, why research on
% software testing terminology is so vital.\qtodo{Is this sentence needed?}
